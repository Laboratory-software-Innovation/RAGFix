[
    {
        "question_id": "51545026",
        "accepted_answer_id": "51547349",
        "question_title": "Implementing a custom dataset with PyTorch",
        "question_markdown": "I&#39;m attempting to modify this feedforward network taken from https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/feedforward_neural_network/main.py\r\nto utilize my own dataset. \r\n\r\nI define a custom dataset of two 1 dim arrays as input and two scalars the corresponding output : \r\n\r\n    x = torch.tensor([[5.5, 3,3,4] , [1 , 2,3,4], [9 , 2,3,4]])\r\n    print(x)\r\n    \r\n    y = torch.tensor([1,2,3])\r\n    print(y)\r\n    \r\n    import torch.utils.data as data_utils\r\n    \r\n    my_train = data_utils.TensorDataset(x, y)\r\n    my_train_loader = data_utils.DataLoader(my_train, batch_size=50, shuffle=True)\r\n\r\nI&#39;ve updated the hyperparameters to match new input_size (2) &amp; num_classes (3).\r\n\r\nI&#39;ve also changed `images = images.reshape(-1, 28*28).to(device)` to `images = images.reshape(-1, 4).to(device)`\r\n\r\nAs the training set is minimal I&#39;ve changed the batch_size to 1.\r\n\r\nUpon making these modifications I receive error when attempting to train : \r\n\r\n&gt; RuntimeError                              Traceback (most recent call\r\n&gt; last) &lt;ipython-input-52-9cdca58f3ef6&gt; in &lt;module&gt;()\r\n&gt;      51 \r\n&gt;      52         # Forward pass\r\n&gt; ---&gt; 53         outputs = model(images)\r\n&gt;      54         loss = criterion(outputs, labels)\r\n&gt;      55 \r\n&gt; \r\n&gt; /home/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n&gt;     489             result = self._slow_forward(*input, **kwargs)\r\n&gt;     490         else:\r\n&gt; --&gt; 491             result = self.forward(*input, **kwargs)\r\n&gt;     492         for hook in self._forward_hooks.values():\r\n&gt;     493             hook_result = hook(self, input, result)\r\n&gt; \r\n&gt; &lt;ipython-input-52-9cdca58f3ef6&gt; in forward(self, x)\r\n&gt;      31 \r\n&gt;      32     def forward(self, x):\r\n&gt; ---&gt; 33         out = self.fc1(x)\r\n&gt;      34         out = self.relu(out)\r\n&gt;      35         out = self.fc2(out)\r\n&gt; \r\n&gt; /home/.local/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n&gt;     489             result = self._slow_forward(*input, **kwargs)\r\n&gt;     490         else:\r\n&gt; --&gt; 491             result = self.forward(*input, **kwargs)\r\n&gt;     492         for hook in self._forward_hooks.values():\r\n&gt;     493             hook_result = hook(self, input, result)\r\n&gt; \r\n&gt; /home/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py in forward(self, input)\r\n&gt;      53 \r\n&gt;      54     def forward(self, input):\r\n&gt; ---&gt; 55         return F.linear(input, self.weight, self.bias)\r\n&gt;      56 \r\n&gt;      57     def extra_repr(self):\r\n&gt; \r\n&gt; /home/.local/lib/python3.6/site-packages/torch/nn/functional.py\r\n&gt; in linear(input, weight, bias)\r\n&gt;     990     if input.dim() == 2 and bias is not None:\r\n&gt;     991         # fused op is marginally faster\r\n&gt; --&gt; 992         return torch.addmm(bias, input, weight.t())\r\n&gt;     993 \r\n&gt;     994     output = input.matmul(weight.t())\r\n&gt; \r\n&gt; RuntimeError: size mismatch, m1: [3 x 4], m2: [2 x 3] at\r\n&gt; /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249\r\n\r\nHow to amend code to match expected dimensionality ? I&#39;m unsure what code to change as I&#39;ve changed all parameters that require updating ?\r\n\r\nSource prior to changes : \r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import torchvision\r\n    import torchvision.transforms as transforms\r\n    \r\n    \r\n    # Device configuration\r\n    device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\n    \r\n    # Hyper-parameters \r\n    input_size = 784\r\n    hidden_size = 500\r\n    num_classes = 10\r\n    num_epochs = 5\r\n    batch_size = 100\r\n    learning_rate = 0.001\r\n    \r\n    # MNIST dataset \r\n    train_dataset = torchvision.datasets.MNIST(root=&#39;../../data&#39;, \r\n                                               train=True, \r\n                                               transform=transforms.ToTensor(),  \r\n                                               download=True)\r\n    \r\n    test_dataset = torchvision.datasets.MNIST(root=&#39;../../data&#39;, \r\n                                              train=False, \r\n                                              transform=transforms.ToTensor())\r\n    \r\n    # Data loader\r\n    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \r\n                                               batch_size=batch_size, \r\n                                               shuffle=True)\r\n    \r\n    test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \r\n                                              batch_size=batch_size, \r\n                                              shuffle=False)\r\n    \r\n    # Fully connected neural network with one hidden layer\r\n    class NeuralNet(nn.Module):\r\n        def __init__(self, input_size, hidden_size, num_classes):\r\n            super(NeuralNet, self).__init__()\r\n            self.fc1 = nn.Linear(input_size, hidden_size) \r\n            self.relu = nn.ReLU()\r\n            self.fc2 = nn.Linear(hidden_size, num_classes)  \r\n        \r\n        def forward(self, x):\r\n            out = self.fc1(x)\r\n            out = self.relu(out)\r\n            out = self.fc2(out)\r\n            return out\r\n    \r\n    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\r\n    \r\n    # Loss and optimizer\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \r\n    \r\n    # Train the model\r\n    total_step = len(train_loader)\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(train_loader):  \r\n            # Move tensors to the configured device\r\n            images = images.reshape(-1, 28*28).to(device)\r\n            labels = labels.to(device)\r\n            \r\n            # Forward pass\r\n            outputs = model(images)\r\n            loss = criterion(outputs, labels)\r\n            \r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            if (i+1) % 100 == 0:\r\n                print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; \r\n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n    \r\n    # Test the model\r\n    # In test phase, we don&#39;t need to compute gradients (for memory efficiency)\r\n    with torch.no_grad():\r\n        correct = 0\r\n        total = 0\r\n        for images, labels in test_loader:\r\n            images = images.reshape(-1, 28*28).to(device)\r\n            labels = labels.to(device)\r\n            outputs = model(images)\r\n            _, predicted = torch.max(outputs.data, 1)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum().item()\r\n    \r\n        print(&#39;Accuracy of the network on the 10000 test images: {} %&#39;.format(100 * correct / total))\r\n    \r\n    # Save the model checkpoint\r\n    torch.save(model.state_dict(), &#39;model.ckpt&#39;)\r\n\r\nSource post changes : \r\n\r\n    x = torch.tensor([[5.5, 3,3,4] , [1 , 2,3,4], [9 , 2,3,4]])\r\n    print(x)\r\n    \r\n    y = torch.tensor([1,2,3])\r\n    print(y)\r\n    \r\n    import torch.utils.data as data_utils\r\n    \r\n    my_train = data_utils.TensorDataset(x, y)\r\n    my_train_loader = data_utils.DataLoader(my_train, batch_size=50, shuffle=True)\r\n    \r\n    print(my_train)\r\n    \r\n    print(my_train_loader)\r\n    \r\n    import torch\r\n    import torch.nn as nn\r\n    import torchvision\r\n    import torchvision.transforms as transforms\r\n    \r\n    \r\n    # Device configuration\r\n    device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\n    \r\n    # Hyper-parameters \r\n    input_size = 2\r\n    hidden_size = 3\r\n    num_classes = 3\r\n    num_epochs = 5\r\n    batch_size = 1\r\n    learning_rate = 0.001\r\n    \r\n    # MNIST dataset \r\n    train_dataset = my_train\r\n    \r\n    # Data loader\r\n    train_loader = my_train_loader\r\n    \r\n    # Fully connected neural network with one hidden layer\r\n    class NeuralNet(nn.Module):\r\n        def __init__(self, input_size, hidden_size, num_classes):\r\n            super(NeuralNet, self).__init__()\r\n            self.fc1 = nn.Linear(input_size, hidden_size) \r\n            self.relu = nn.ReLU()\r\n            self.fc2 = nn.Linear(hidden_size, num_classes)  \r\n        \r\n        def forward(self, x):\r\n            out = self.fc1(x)\r\n            out = self.relu(out)\r\n            out = self.fc2(out)\r\n            return out\r\n    \r\n    model = NeuralNet(input_size, hidden_size, num_classes).to(device)\r\n    \r\n    # Loss and optimizer\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \r\n    \r\n    # Train the model\r\n    total_step = len(train_loader)\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(train_loader):  \r\n            # Move tensors to the configured device\r\n            images = images.reshape(-1, 4).to(device)\r\n            labels = labels.to(device)\r\n            \r\n            # Forward pass\r\n            outputs = model(images)\r\n            loss = criterion(outputs, labels)\r\n            \r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            if (i+1) % 100 == 0:\r\n                print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; \r\n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n    \r\n    # Test the model\r\n    # In test phase, we don&#39;t need to compute gradients (for memory efficiency)\r\n    with torch.no_grad():\r\n        correct = 0\r\n        total = 0\r\n        for images, labels in test_loader:\r\n            images = images.reshape(-1, 4).to(device)\r\n            labels = labels.to(device)\r\n            outputs = model(images)\r\n            _, predicted = torch.max(outputs.data, 1)\r\n            total += labels.size(0)\r\n            correct += (predicted == labels).sum().item()\r\n    \r\n        print(&#39;Accuracy of the network on the 10000 test images: {} %&#39;.format(100 * correct / total))\r\n    \r\n    # Save the model checkpoint\r\n    torch.save(model.state_dict(), &#39;model.ckpt&#39;)\r\n",
        "accepted_answer_markdown": "You need to change `input_size` to 4 (2*2), and not 2 as your modified code currently shows.  \nIf you compare it to the original MNIST example, you&#39;ll see that `input_size` is set to 784 (28*28) and not just to 28."
    },
    {
        "question_id": "51605893",
        "accepted_answer_id": "51606286",
        "question_title": "Why doesn&#39;t my simple pytorch network work on GPU device?",
        "question_markdown": "I built a simple network from a tutorial and I got this error:\r\n\r\n&gt; RuntimeError: Expected object of type torch.cuda.FloatTensor but found\r\n&gt; type torch.FloatTensor for argument #4 &#39;mat1&#39;\r\n\r\nAny help? Thank you!\r\n\r\n\r\n    import torch\r\n    import torchvision\r\n    \r\n    device = torch.device(&quot;cuda:0&quot;)\r\n    root = &#39;.data/&#39;\r\n    \r\n    dataset = torchvision.datasets.MNIST(root, transform=torchvision.transforms.ToTensor(), download=True)\r\n    \r\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=4)\r\n    \r\n    \r\n    class Net(torch.nn.Module):\r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            self.out = torch.nn.Linear(28*28, 10)\r\n    \r\n        def forward(self, x):\r\n            x = x.view(x.size(0), -1)\r\n            x = self.out(x)\r\n            return x\r\n    \r\n    net = Net()\r\n    net.to(device)\r\n    \r\n    for i, (inputs, labels) in enumerate(dataloader):\r\n        inputs.to(device)\r\n        out = net(inputs)",
        "accepted_answer_markdown": "**TL;DR**  \r\nThis is the fix\r\n\r\n    inputs = inputs.to(device)  \r\n\r\n**Why?!**  \r\nThere is a slight difference between [`torch.nn.Module.to()`][1] and [`torch.Tensor.to()`][2]: while `Module.to()` is an **in-place** operator, `Tensor.to()` is not. Therefore\r\n\r\n    net.to(device)\r\nChanges `net` itself and moves it to `device`. On the other hand\r\n\r\n    inputs.to(device)\r\ndoes not change `inputs`, but rather returns a **copy** of `inputs` that resides on `device`. To use that &quot;on device&quot; copy, you need to assign it into a variable, hence\r\n\r\n    inputs = inputs.to(device)\r\n\r\n [1]: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.to\r\n [2]: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.to"
    },
    {
        "question_id": "51701908",
        "accepted_answer_id": "51702394",
        "question_title": "Issue with running a single prediction with PyTorch",
        "question_markdown": "I have a trained model using PyTorch now I want to simpy run it on one example \r\n\r\n    &gt;&gt;&gt; model\r\n    nn.Sequential {\r\n      [input -&gt; (0) -&gt; (1) -&gt; (2) -&gt; (3) -&gt; (4) -&gt; (5) -&gt; (6) -&gt; (7) -&gt; (8) -&gt; (9) -&gt; (10) -&gt; output]\r\n      (0): nn.SpatialConvolutionMap\r\n      (1): nn.Tanh\r\n      (2): nn.SpatialMaxPooling(2x2, 2, 2)\r\n      (3): nn.SpatialConvolutionMap\r\n      (4): nn.Tanh\r\n      (5): nn.SpatialMaxPooling(2x2, 2, 2)\r\n      (6): nn.Reshape(6400)\r\n      (7): nn.Linear(6400 -&gt; 128)\r\n      (8): nn.Tanh\r\n      (9): nn.Linear(128 -&gt; 5)\r\n      (10): nn.LogSoftMax\r\n    }\r\n\r\nThen I load an image from my test set:\r\n\r\n    image = cv2.imread(&#39;image.png&#39;,cv2.IMREAD_GRAYSCALE)\r\n    transformation = transforms.Compose([transforms.ToTensor()]) \r\n    image_tensor = transformation(image).float()\r\n    inp = Variable(image_tensor)\r\nand finally try to run the network \r\n\r\n    output = model(inp) \r\n\r\nBut I get error **TypeError: &#39;Sequential&#39; object is not callable**\r\n\r\n\r\n\r\n",
        "accepted_answer_markdown": "It seems like your model is not [`nn.Sequential`][2] (**py**torch `Sequential`), but rather [`torch.legacy.nn.Sequential`][3] (a legacy lua torch model).  \r\nTry using this model `forward()` explicitly:\r\n\r\n    output = model.forward(inp[None, ...])  # don&#39;t forget to add &quot;batch&quot; dimension\r\n\r\n [1]: https://discuss.pytorch.org/t/convert-import-torch-model-to-pytorch/37\r\n [2]: https://pytorch.org/docs/stable/nn.html#torch.nn.Sequential\r\n [3]: https://pytorch.org/docs/stable/legacy.html"
    },
    {
        "question_id": "51756581",
        "accepted_answer_id": "51758045",
        "question_title": "How do I turn a Pytorch Dataloader into a numpy array to display image data with matplotlib?",
        "question_markdown": "I am new to Pytorch. I have been trying to learn how to view my input images before I begin training on my CNN. I am having a very hard time changing the images into a form that can be used with matplotlib.\r\n\r\nSo far I have tried this:\r\n\r\n    from multiprocessing import freeze_support\r\n    \r\n    import torch\r\n    from torch import nn\r\n    import torchvision\r\n    from torch.autograd import Variable\r\n    from torch.utils.data import DataLoader, Sampler\r\n    from torchvision import datasets\r\n    from torchvision.transforms import transforms\r\n    from torch.optim import Adam\r\n    \r\n    import matplotlib.pyplot as plt\r\n    import numpy as np\r\n    import PIL\r\n    \r\n    num_classes = 5\r\n    batch_size = 100\r\n    num_of_workers = 5\r\n    \r\n    DATA_PATH_TRAIN = &#39;C:\\\\Users\\Aeryes\\PycharmProjects\\simplecnn\\images\\\\train&#39;\r\n    DATA_PATH_TEST = &#39;C:\\\\Users\\Aeryes\\PycharmProjects\\simplecnn\\images\\\\test&#39;\r\n    \r\n    trans = transforms.Compose([\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.Resize(32),\r\n        transforms.CenterCrop(32),\r\n        transforms.ToPImage(),\r\n        transforms.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\r\n        ])\r\n    \r\n    train_dataset = datasets.ImageFolder(root=DATA_PATH_TRAIN, transform=trans)\r\n    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_of_workers)\r\n    \r\n    def imshow(img):\r\n        img = img / 2 + 0.5     # unnormalize\r\n        npimg = img.numpy()\r\n        print(npimg)\r\n        plt.imshow(np.transpose(npimg, (1, 2, 0, 1)))\r\n    \r\n    def main():\r\n        # get some random training images\r\n        dataiter = iter(train_loader)\r\n        images, labels = dataiter.next()\r\n    \r\n        # show images\r\n        imshow(images)\r\n        # print labels\r\n        print(&#39; &#39;.join(&#39;%5s&#39; % classes[labels[j]] for j in range(4)))\r\n    \r\n    if __name__ == &quot;__main__&quot;:\r\n        main()\r\n\r\nHowever, this throws and error:\r\n\r\n      [[0.27058825 0.18431371 0.31764707 ... 0.18823528 0.3882353\r\n        0.27450982]\r\n       [0.23137254 0.11372548 0.24313724 ... 0.16862744 0.14117646\r\n        0.40784314]\r\n       [0.25490198 0.19607842 0.30588236 ... 0.27450982 0.25882354\r\n        0.34509805]\r\n       ...\r\n       [0.2784314  0.21960783 0.2352941  ... 0.5803922  0.46666667\r\n        0.25882354]\r\n       [0.26666668 0.16862744 0.23137254 ... 0.2901961  0.29803923\r\n        0.2509804 ]\r\n       [0.30980393 0.39607844 0.28627452 ... 0.1490196  0.10588235\r\n        0.19607842]]\r\n    \r\n      [[0.2352941  0.06274509 0.15686274 ... 0.09411764 0.3019608\r\n        0.19215685]\r\n       [0.22745097 0.07843137 0.12549019 ... 0.07843137 0.10588235\r\n        0.3019608 ]\r\n       [0.20392156 0.13333333 0.1607843  ... 0.16862744 0.2117647\r\n        0.22745097]\r\n       ...\r\n       [0.18039215 0.16862744 0.1490196  ... 0.45882353 0.36078432\r\n        0.16470587]\r\n       [0.1607843  0.10588235 0.14117646 ... 0.2117647  0.18039215\r\n        0.10980392]\r\n       [0.18039215 0.3019608  0.2117647  ... 0.11372548 0.06274509\r\n        0.04705882]]]\r\n    \r\n    \r\n     ...\r\n    \r\n    \r\n     [[[0.8980392  0.8784314  0.8509804  ... 0.627451   0.627451\r\n        0.627451  ]\r\n       [0.8509804  0.8235294  0.7921569  ... 0.54901963 0.5568628\r\n        0.56078434]\r\n       [0.7921569  0.7529412  0.7176471  ... 0.47058824 0.48235294\r\n        0.49411765]\r\n       ...\r\n       [0.3764706  0.38431373 0.3764706  ... 0.4509804  0.43137255\r\n        0.39607844]\r\n       [0.38431373 0.39607844 0.3882353  ... 0.4509804  0.43137255\r\n        0.39607844]\r\n       [0.3882353  0.4        0.39607844 ... 0.44313726 0.42352942\r\n        0.39215687]]\r\n    \r\n      [[0.9254902  0.90588236 0.88235295 ... 0.60784316 0.6\r\n        0.5921569 ]\r\n       [0.88235295 0.85490197 0.8235294  ... 0.5411765  0.5372549\r\n        0.53333336]\r\n       [0.8235294  0.7882353  0.75686276 ... 0.47058824 0.47058824\r\n        0.47058824]\r\n       ...\r\n       [0.50980395 0.5176471  0.5137255  ... 0.58431375 0.5647059\r\n        0.53333336]\r\n       [0.5137255  0.53333336 0.5254902  ... 0.58431375 0.5686275\r\n        0.53333336]\r\n       [0.5176471  0.53333336 0.5294118  ... 0.5764706  0.56078434\r\n        0.5294118 ]]\r\n    \r\n      [[0.95686275 0.9372549  0.90588236 ... 0.18823528 0.19999999\r\n        0.20784312]\r\n       [0.9098039  0.8784314  0.8352941  ... 0.1607843  0.17254901\r\n        0.18039215]\r\n       [0.84313726 0.7921569  0.7490196  ... 0.1372549  0.14509803\r\n        0.15294117]\r\n       ...\r\n       [0.03921568 0.05490196 0.05098039 ... 0.11764705 0.09411764\r\n        0.02745098]\r\n       [0.04705882 0.07843137 0.06666666 ... 0.12156862 0.10196078\r\n        0.03529412]\r\n       [0.05098039 0.0745098  0.07843137 ... 0.12549019 0.10196078\r\n        0.04705882]]]\r\n    \r\n    \r\n     [[[0.30588236 0.28627452 0.24313724 ... 0.2901961  0.26666668\r\n        0.21568626]\r\n       [0.8156863  0.6666667  0.5921569  ... 0.18039215 0.23921567\r\n        0.21568626]\r\n       [0.9019608  0.83137256 0.85490197 ... 0.21960783 0.36862746\r\n        0.23921567]\r\n       ...\r\n       [0.7058824  0.83137256 0.85490197 ... 0.2627451  0.24313724\r\n        0.20784312]\r\n       [0.7137255  0.84313726 0.84705883 ... 0.26666668 0.29803923\r\n        0.21568626]\r\n       [0.7254902  0.8235294  0.8392157  ... 0.2509804  0.27058825\r\n        0.2352941 ]]\r\n    \r\n      [[0.24705881 0.22745097 0.19215685 ... 0.2784314  0.25490198\r\n        0.19607842]\r\n       [0.59607846 0.37254903 0.29803923 ... 0.16470587 0.22745097\r\n        0.20392156]\r\n       [0.5921569  0.4509804  0.49803922 ... 0.20784312 0.3764706\r\n        0.2352941 ]\r\n       ...\r\n       [0.42352942 0.4627451  0.42352942 ... 0.23921567 0.23137254\r\n        0.19999999]\r\n       [0.45882353 0.5176471  0.35686275 ... 0.23921567 0.26666668\r\n        0.19607842]\r\n       [0.41568628 0.44313726 0.34901962 ... 0.21960783 0.23921567\r\n        0.21568626]]\r\n    \r\n      [[0.23137254 0.20784312 0.1490196  ... 0.30588236 0.28627452\r\n        0.19607842]\r\n       [0.61960787 0.3764706  0.26666668 ... 0.16470587 0.24313724\r\n        0.21568626]\r\n       [0.57254905 0.43137255 0.48235294 ... 0.2235294  0.40392157\r\n        0.25882354]\r\n       ...\r\n       [0.4        0.42352942 0.37254903 ... 0.25490198 0.24705881\r\n        0.21568626]\r\n       [0.43137255 0.4509804  0.29411766 ... 0.25882354 0.28235295\r\n        0.20392156]\r\n       [0.38431373 0.3529412  0.25490198 ... 0.2352941  0.25490198\r\n        0.23137254]]]\r\n    \r\n    \r\n     [[[0.06274509 0.09019607 0.11372548 ... 0.5803922  0.5176471\r\n        0.59607846]\r\n       [0.09411764 0.14509803 0.1372549  ... 0.5294118  0.49803922\r\n        0.5058824 ]\r\n       [0.04705882 0.09411764 0.10196078 ... 0.45882353 0.42352942\r\n        0.38431373]\r\n       ...\r\n       [0.15294117 0.12941176 0.1607843  ... 0.85882354 0.8509804\r\n        0.80784315]\r\n       [0.14509803 0.10588235 0.1607843  ... 0.8666667  0.85882354\r\n        0.8       ]\r\n       [0.1490196  0.10588235 0.16470587 ... 0.827451   0.8156863\r\n        0.7921569 ]]\r\n    \r\n      [[0.06666666 0.12156862 0.17647058 ... 0.59607846 0.5529412\r\n        0.6039216 ]\r\n       [0.07058823 0.10588235 0.11764705 ... 0.56078434 0.5254902\r\n        0.5372549 ]\r\n       [0.03921568 0.0745098  0.09803921 ... 0.48235294 0.4392157\r\n        0.4117647 ]\r\n       ...\r\n       [0.2117647  0.14509803 0.2784314  ... 0.43137255 0.3529412\r\n        0.34117648]\r\n       [0.2235294  0.11372548 0.2509804  ... 0.4509804  0.39607844\r\n        0.2509804 ]\r\n       [0.25490198 0.12156862 0.24705881 ... 0.38039216 0.36078432\r\n        0.3254902 ]]\r\n    \r\n      [[0.05490196 0.09803921 0.12549019 ... 0.46666667 0.38039216\r\n        0.45490196]\r\n       [0.06274509 0.09803921 0.10196078 ... 0.44705883 0.41568628\r\n        0.3882353 ]\r\n       [0.03921568 0.06666666 0.0862745  ... 0.3764706  0.33333334\r\n        0.28235295]\r\n       ...\r\n       [0.12156862 0.14509803 0.16862744 ... 0.15686274 0.0745098\r\n        0.09411764]\r\n       [0.10588235 0.11372548 0.16862744 ... 0.25882354 0.18431371\r\n        0.05490196]\r\n       [0.12156862 0.11372548 0.17254901 ... 0.2352941  0.17254901\r\n        0.14117646]]]]\r\n    Traceback (most recent call last):\r\n      File &quot;image_loader.py&quot;, line 51, in &lt;module&gt;\r\n        main()\r\n      File &quot;image_loader.py&quot;, line 46, in main\r\n        imshow(images)\r\n      File &quot;image_loader.py&quot;, line 38, in imshow\r\n        plt.imshow(np.transpose(npimg, (1, 2, 0, 1)))\r\n      File &quot;C:\\Users\\Aeryes\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py&quot;, line 598, in transpose\r\n        return _wrapfunc(a, &#39;transpose&#39;, axes)\r\n      File &quot;C:\\Users\\Aeryes\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\numpy\\core\\fromnumeric.py&quot;, line 51, in _wrapfunc\r\n        return getattr(obj, method)(*args, **kwds)\r\n    ValueError: repeated axis in transpose\r\n\r\nI tried to print out the arrays to get the dimensions but I do not know what to make of this. It is very confusing. \r\n\r\nHere is my direct question: How do I view the input images before training using the tensors in my DataLoader object?",
        "accepted_answer_markdown": "First of all, `dataloader` output 4 dimensional tensor - `[batch, channel, height, width]`. Matplotlib and other image processing libraries often requires `[height, width, channel]`. You are right about using the transpose, just not in the right way.\r\n\r\nThere will be a lot of images in your `images` so first you need to pick one (or write a for loop to save all of them). This will be simply `images[i]`, typically I use `i=0`.\r\n\r\nThen, your transpose should convert a now `[channel, height, width]` tensor to a `[height, width, channel]` one. To do this, use `np.transpose(image.numpy(), (1, 2, 0))`, very much like yours.\r\n\r\nPutting them together, you should have\r\n\r\n    plt.imshow(np.transpose(images[0].numpy(), (1, 2, 0)))\r\n\r\nSometimes you need to call `.detach()` (detach this part from the computational graph) and `.cpu()` (transfer data from GPU to CPU) depending on the use case, that will be\r\n\r\n    plt.imshow(np.transpose(images[0].cpu().detach().numpy(), (1, 2, 0)))\r\n\r\n\r\n"
    },
    {
        "question_id": "51818225",
        "accepted_answer_id": "51943091",
        "question_title": "Pytorch RuntimeError: &quot;host_softmax&quot; not implemented for &#39;torch.cuda.LongTensor&#39;",
        "question_markdown": "I am using pytorch for training models. But I got an runtime error when it was computing the cross-entropy loss. \r\n\r\n    Traceback (most recent call last):\r\n      File &quot;deparser.py&quot;, line 402, in &lt;module&gt;\r\n        d.train()\r\n      File &quot;deparser.py&quot;, line 331, in train\r\n        total, correct, avgloss = self.train_util()\r\n      File &quot;deparser.py&quot;, line 362, in train_util\r\n        loss = self.step(X_train, Y_train, correct, total)\r\n      File &quot;deparser.py&quot;, line 214, in step\r\n        loss = nn.CrossEntropyLoss()(out.long(), y)\r\n      File &quot;/home/summer2018/TF/lib/python3.5/site-packages/torch/nn/modules/module.py&quot;, line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File &quot;/home/summer2018/TF/lib/python3.5/site-packages/torch/nn/modules/loss.py&quot;, line 862, in forward\r\n        ignore_index=self.ignore_index, reduction=self.reduction)\r\n      File &quot;/home/summer2018/TF/lib/python3.5/site-packages/torch/nn/functional.py&quot;, line 1550, in cross_entropy\r\n        return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n      File &quot;/home/summer2018/TF/lib/python3.5/site-packages/torch/nn/functional.py&quot;, line 975, in log_softmax\r\n        return input.log_softmax(dim)\r\n    RuntimeError: &quot;host_softmax&quot; not implemented for &#39;torch.cuda.LongTensor&#39;\r\n\r\nI think this is because the `.cuda()` function or conversion between `torch.Float` and `torch.Long`. But I have tried many ways to change the variable by `.cpu()`/`.cuda()` and `.long()`/`.float()`, but it still not work. This error message can&#39;t be found when searching it on google. Can anyone helps me? Thanks!!!\r\n\r\nThis is the code cause error:\r\n\r\n    def step(self, x, y, correct, total):\r\n        self.optimizer.zero_grad()\r\n        out = self.forward(*x)\r\n        loss = nn.CrossEntropyLoss()(out.long(), y)\r\n        loss.backward()\r\n        self.optimizer.step()\r\n        _, predicted = torch.max(out.data, 1)\r\n        total += y.size(0)\r\n        correct += int((predicted == y).sum().data)\r\n        return loss.data\r\n\r\nAnd this function step() is called by: \r\n\r\n    def train_util(self):\r\n        total = 0\r\n        correct = 0\r\n        avgloss = 0\r\n        for i in range(self.step_num_per_epoch):\r\n            X_train, Y_train = self.trainloader()\r\n            self.optimizer.zero_grad()\r\n            if torch.cuda.is_available():\r\n                self.cuda()\r\n                for i in range(len(X_train)):\r\n                    X_train[i] = Variable(torch.from_numpy(X_train[i]))\r\n                    X_train[i].requires_grad = False\r\n                    X_train[i] = X_train[i].cuda()\r\n                Y_train = torch.from_numpy(Y_train)\r\n                Y_train.requires_grad = False\r\n                Y_train = Y_train.cuda()\r\n            loss = self.step(X_train, Y_train, correct, total)\r\n            avgloss+=float(loss)*Y_train.size(0)\r\n            self.optimizer.step()\r\n            if i%100==99:\r\n                print(&#39;STEP %d, Loss: %.4f, Acc: %.4f&#39;%(i+1,loss,correct/total))\r\n            \r\n        return total, correct, avgloss/self.data_len\r\n\r\nThe input data `X_train, Y_train = self.trainloader()` are numpy arrays at begining.\r\n\r\n\r\nThis is a data sample:\r\n\r\n    &gt;&gt;&gt; X_train, Y_train = d.trainloader()\r\n    &gt;&gt;&gt; X_train[0].dtype\r\n    dtype(&#39;int64&#39;)\r\n    &gt;&gt;&gt; X_train[1].dtype\r\n    dtype(&#39;int64&#39;)\r\n    &gt;&gt;&gt; X_train[2].dtype\r\n    dtype(&#39;int64&#39;)\r\n    &gt;&gt;&gt; Y_train.dtype\r\n    dtype(&#39;float32&#39;)\r\n    &gt;&gt;&gt; X_train[0]\r\n    array([[   0,    6,    0, ...,    0,    0,    0],\r\n           [   0, 1944, 8168, ...,    0,    0,    0],\r\n           [   0,  815,  317, ...,    0,    0,    0],\r\n           ...,\r\n           [   0,    0,    0, ...,    0,    0,    0],\r\n           [   0,   23,    6, ...,    0,    0,    0],\r\n           [   0,    0,  297, ...,    0,    0,    0]])\r\n    &gt;&gt;&gt; X_train[1]\r\n    array([ 6,  7,  8, 21,  2, 34,  3,  4, 19, 14, 15,  2, 13,  3, 11, 22,  4,\r\n       13, 34, 10, 13,  3, 48, 18, 16, 19, 16, 17, 48,  3,  3, 13])\r\n    &gt;&gt;&gt; X_train[2]\r\n    array([ 4,  5,  8, 36,  2, 33,  5,  3, 17, 16, 11,  0,  9,  3, 10, 20,  1,\r\n       14, 33, 25, 19,  1, 46, 17, 14, 24, 15, 15, 51,  2,  1, 14])\r\n    &gt;&gt;&gt; Y_train\r\n    array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\r\n           [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\r\n           [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n           ...,\r\n           [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\r\n            0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\r\n          dtype=float32)\r\n\r\nTry all possible combinations:\r\n\r\ncase 1:  \r\n`loss = nn.CrossEntropyLoss()(out, y)`  \r\nI get:  \r\n`RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.cuda.FloatTensor for argument #2 &#39;target&#39;`  \r\n\r\ncase 2:  \r\n`loss = nn.CrossEntropyLoss()(out.long(), y)`  \r\nas description above  \r\n\r\ncase 3:  \r\n`loss = nn.CrossEntropyLoss()(out.float(), y)`  \r\nI get:  \r\n`RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.cuda.FloatTensor for argument #2 &#39;target&#39;`\r\n  \r\ncase 4:  \r\n`loss = nn.CrossEntropyLoss()(out, y.long())`  \r\nI get:   \r\n`RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15`  \r\n\r\ncase 5:  \r\n`loss = nn.CrossEntropyLoss()(out.long(), y.long())`    \r\nI get:   \r\n`RuntimeError: &quot;host_softmax&quot; not implemented for &#39;torch.cuda.LongTensor&#39;`  \r\n\r\ncase 6:  \r\n`loss = nn.CrossEntropyLoss()(out.float(), y.long())`    \r\nI get:   \r\n`RuntimeError: multi-target not supported at /pytorch/aten/src/THCUNN/generic/ClassNLLCriterion.cu:15`  \r\n\r\n\r\ncase 7:  \r\n`loss = nn.CrossEntropyLoss()(out, y.float())`    \r\nI get:   \r\n`RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.cuda.FloatTensor for argument #2 &#39;target&#39;`  \r\n\r\ncase 8:  \r\n`loss = nn.CrossEntropyLoss()(out.long(), y.float())`    \r\nI get:   \r\n`RuntimeError: &quot;host_softmax&quot; not implemented for &#39;torch.cuda.LongTensor&#39;`  \r\n\r\ncase 9:  \r\n`loss = nn.CrossEntropyLoss()(out.float(), y.float())`    \r\nI get:   \r\n`RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.cuda.FloatTensor for argument #2 &#39;target&#39;`  \r\n",
        "accepted_answer_markdown": "I know where the problem is.  \r\n\r\n`y` should be in `torch.int64` dtype without one-hot encoding.\r\nAnd `CrossEntropyLoss()` will auto encoding it with one-hot (while out is the probability distribution of prediction like one-hot format).  \r\n\r\nIt can run now!"
    },
    {
        "question_id": "51885408",
        "accepted_answer_id": "51885920",
        "question_title": "Expected parameters of Conv2d",
        "question_markdown": "Below code : \r\n\r\n&lt;!-- language: lang-python --&gt;\r\n    \r\n    import torch \r\n    import torch.nn as nn\r\n    import torchvision\r\n    import torchvision.transforms as transforms\r\n    import torch.utils.data as data_utils\r\n    import numpy as np\r\n\r\n    train_dataset = []\r\n    mu, sigma = 0, 0.1 # mean and standard deviation\r\n    num_instances = 20\r\n    batch_size_value = 10\r\n    for i in range(num_instances) :\r\n        image = []\r\n        image_x = np.random.normal(mu, sigma, 1000).reshape((1 , 100, 10))\r\n        train_dataset.append(image_x)\r\n    labels = [1 for i in range(num_instances)]\r\n    x2 = torch.tensor(train_dataset).float()\r\n    y2 = torch.tensor(labels).long()\r\n    my_train2 = data_utils.TensorDataset(x2, y2)\r\n    train_loader2 = data_utils.DataLoader(my_train2, batch_size=batch_size_value, shuffle=False)    \r\n    \r\n    # Device configuration\r\n    device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\n    \r\n    # Hyper parameters\r\n    num_epochs = 5\r\n    num_classes = 1\r\n    batch_size = 5\r\n    learning_rate = 0.001\r\n    \r\n    # Convolutional neural network (two convolutional layers)\r\n    class ConvNet(nn.Module):\r\n        def __init__(self, num_classes=1):\r\n            super(ConvNet, self).__init__()\r\n            self.layer1 = nn.Sequential(\r\n                nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\r\n                nn.BatchNorm2d(16),\r\n                nn.ReLU(),\r\n                nn.MaxPool2d(kernel_size=2, stride=2))\r\n            self.layer2 = nn.Sequential(\r\n                nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\r\n                nn.BatchNorm2d(32),\r\n                nn.ReLU(),\r\n                nn.MaxPool2d(kernel_size=2, stride=2))\r\n            self.fc = nn.Linear(7*7*32, num_classes)\r\n            \r\n        def forward(self, x):\r\n            out = self.layer1(x)\r\n            out = self.layer2(out)\r\n            out = out.reshape(out.size(0), -1)\r\n            out = self.fc(out)\r\n            return out\r\n    \r\n    model = ConvNet(num_classes).to(device)\r\n    \r\n    # Loss and optimizer\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n    \r\n    # Train the model\r\n    total_step = len(train_loader2)\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(train_loader2):\r\n            images = images.to(device)\r\n            labels = labels.to(device)\r\n            \r\n            # Forward pass\r\n            outputs = model(images)\r\n            loss = criterion(outputs, labels)\r\n            \r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            if (i+1) % 100 == 0:\r\n                print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; \r\n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n\r\nreturns error : \r\n\r\n    RuntimeError: size mismatch, m1: [10 x 1600], m2: [1568 x 1] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:249\r\n\r\nReading the [documentation for `conv2d`][1],  I tried to change the first parameter to `10X100` to match \r\n\r\n&gt; input \u2013 input tensor of shape (minibatch&#215;in_channels&#215;iH&#215;iW)\r\n\r\nfrom https://pytorch.org/docs/stable/nn.html#torch.nn.functional.conv2d\r\n\r\nbut then received the error : \r\n\r\n    RuntimeError: Given groups=1, weight[16, 1000, 5, 5], so expected input[10, 1, 100, 10] to have 1000 channels, but got 1 channels instead\r\n\r\nSo I&#39;m not sure if I&#39;ve corrected the original error or just caused a new one?\r\n\r\nHow should `Conv2d` be set in order to match image shape of `(10,100)` ?\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/nn.html#torch.nn.functional.conv2d",
        "accepted_answer_markdown": "The error comes from your final fully-connected layer `self.fc = nn.Linear(7*7*32, num_classes)`, not your convolution ones.\r\n\r\nGiven your input dimensions (`(10, 100)`), the shape of `out = self.layer2(out)` is `(batch_size, 32, 25, 2)`, and thus the shape of `out = out.reshape(out.size(0), -1)` is `(batch_size, 32*25*2) = (batch_size, 1600)`.\r\n\r\nOn the other hand, your fully-connected layer is defined for inputs of shape `(batch_size, 32*7*7) = (batch_size, 1568)`.\r\n\r\nThis mismatch between the shape of your 2nd convolution output and the expected shape for your fully-connected layer is causing the error (notice how the shapes mentioned in the trace correspond to the aforementioned ones)."
    },
    {
        "question_id": "52176178",
        "accepted_answer_id": "54508548",
        "question_title": "Pytorch model accuracy test",
        "question_markdown": "I&#39;m using Pytorch to classify a series of images. \r\nThe NN is defined as follows:\r\n\r\n\r\n    model = models.vgg16(pretrained=True)\r\n    model.cuda()\r\n    for param in model.parameters(): param.requires_grad = False\r\n    \r\n    classifier = nn.Sequential(OrderedDict([\r\n                               (&#39;fc1&#39;, nn.Linear(25088, 4096)),\r\n                               (&#39;relu&#39;, nn.ReLU()),\r\n                               (&#39;fc2&#39;, nn.Linear(4096, 102)),\r\n                               (&#39;output&#39;, nn.LogSoftmax(dim=1))\r\n                               ]))\r\n    \r\n    model.classifier = classifier\r\n\r\nThe criterions and optimizers are as follows:\r\n\r\n    criterion = nn.NLLLoss()\r\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\r\n\r\nMy validation function is as follows:\r\n\r\n    def validation(model, testloader, criterion):\r\n        test_loss = 0\r\n        accuracy = 0\r\n        for images, labels in testloader:\r\n    \r\n            images.resize_(images.shape[0], 784)\r\n    \r\n            output = model.forward(images)\r\n            test_loss += criterion(output, labels).item()\r\n    \r\n            ps = torch.exp(output)\r\n            equality = (labels.data == ps.max(dim=1)[1])\r\n            accuracy += equality.type(torch.FloatTensor).mean()\r\n        \r\n        return test_loss, accuracy\r\n\r\nThis is the piece of code that is throwing the following error:\r\n\r\n&gt; RuntimeError: input has less dimensions than expected\r\n\r\n    epochs = 3\r\n    print_every = 40\r\n    steps = 0\r\n    running_loss = 0\r\n    testloader = dataloaders[&#39;test&#39;]\r\n    \r\n    # change to cuda\r\n    model.to(&#39;cuda&#39;)\r\n    \r\n    for e in range(epochs):\r\n        running_loss = 0\r\n        for ii, (inputs, labels) in enumerate(dataloaders[&#39;train&#39;]):\r\n            steps += 1\r\n            \r\n            inputs, labels = inputs.to(&#39;cuda&#39;), labels.to(&#39;cuda&#39;)\r\n            \r\n            optimizer.zero_grad()\r\n            \r\n            # Forward and backward passes\r\n            outputs = model.forward(inputs)\r\n            loss = criterion(outputs, labels)\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            running_loss += loss.item()\r\n            \r\n            if steps % print_every == 0:\r\n                model.eval()\r\n                with torch.no_grad():\r\n                    test_loss, accuracy = validation(model, testloader, criterion)\r\n                    \r\n                print(&quot;Epoch: {}/{}.. &quot;.format(e+1, epochs),\r\n                      &quot;Training Loss: {:.3f}.. &quot;.format(running_loss/print_every),\r\n                      &quot;Test Loss: {:.3f}.. &quot;.format(test_loss/len(testloader)),\r\n                      &quot;Test Accuracy: {:.3f}&quot;.format(accuracy/len(testloader)))\r\n                \r\n                running_loss = 0\r\n\r\nAny help?\r\n\r\n",
        "accepted_answer_markdown": "Just in case it helps someone.\r\n\r\nIf you don&#39;t have a GPU system (say you are developing on a laptop and will eventually test on a server with GPU) you can do the same using:\r\n```    \r\nif torch.cuda.is_available():\r\n        inputs =inputs.to(&#39;cuda&#39;)\r\n    else:\r\n        inputs = inputs.to(&#39;cuda&#39;)\r\n```\r\n\r\nAlso, if you are wondering why there is a `LogSoftmax`, instead of `Softmax` that is because he is using NLLLoss as his loss function. You can read more about softmax [here][1]\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/nn.html"
    },
    {
        "question_id": "52408753",
        "accepted_answer_id": "52503167",
        "question_title": "Expected tensor for argument #1 &#39;input&#39; to have the same dimension",
        "question_markdown": "Using below code I create 10 instances of each training data of which each has 100 dimensions.\r\nEach of the 100 dimension contains 3 dimensions. Therefore it&#39;s shape is : (3, 100, 10). This emulates 10 instances of 100 pixels each with 3 channels to emulate an RGB value\r\n\r\nI&#39;ve set this model to just classify between 1 and 0.\r\n\r\nWhen applying the softmax layer I receive the error : \r\n\r\n&gt; RuntimeError: Expected tensor for argument #1 &#39;input&#39; to have the same\r\n&gt; dimension as tensor for &#39;result&#39;; but 4 does not equal 3 (while\r\n&gt; checking arguments for cudnn_convolution)\r\n\r\nI&#39;m using 0.4.0 (checked using `print(torch.__version__)` )\r\nHow to correctly set the dimension for the softmax layer? As I think my dimensions are correct?\r\n\r\n    %reset -f\r\n    \r\n    import os\r\n    import torch\r\n    from skimage import io, transform\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    from torch.utils.data import Dataset, DataLoader\r\n    from torchvision import transforms, utils\r\n    import torch.utils.data as data_utils\r\n    import torchvision\r\n    import numpy as np\r\n    from sklearn.preprocessing import scale\r\n    import torch.nn.functional as F\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    from random import randint\r\n    \r\n    batch_size_value = 10\r\n    \r\n    train_dataset = []\r\n    mu, sigma = 0, 0.1 # mean and standard deviation\r\n    num_instances = 10\r\n    \r\n    # Create 3000 instance and reshape to (3 , 100, 10) , this emulates 10 instances of 100 pixels \r\n    # each with 3 channels to emulate an RGB value\r\n    \r\n    for i in range(num_instances) :\r\n        image = []\r\n        image_x = np.random.normal(mu, sigma, 3000).reshape((3 , 100, 10))\r\n        train_dataset.append(image_x)\r\n    \r\n    mu, sigma = 100, 0.80 # mean and standard deviation\r\n    for i in range(num_instances) :\r\n        image = []\r\n        image_x = np.random.normal(mu, sigma, 3000).reshape((3 , 100, 10))\r\n        train_dataset.append(image_x)\r\n    \r\n    labels_1 = [1 for i in range(num_instances)]\r\n    labels_0 = [0 for i in range(num_instances)]\r\n    \r\n    labels = labels_1 + labels_0\r\n    \r\n    print(labels)\r\n    \r\n    x2 = torch.tensor(train_dataset).float()\r\n    y2 = torch.tensor(labels).long()\r\n    \r\n    my_train2 = data_utils.TensorDataset(x2, y2)\r\n    train_loader2 = data_utils.DataLoader(my_train2, batch_size=batch_size_value, shuffle=False)\r\n    \r\n    # print(x2)\r\n    \r\n    # Device configuration\r\n    device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\n    print(&#39;device&#39; , device)\r\n    # device = &#39;cpu&#39;\r\n    \r\n    # Hyper parameters\r\n    num_epochs = 10\r\n    num_classes = 2\r\n    batch_size = 5\r\n    learning_rate = 0.001\r\n      \r\n    # Convolutional neural network (two convolutional layers)\r\n    class ConvNet(nn.Module):\r\n        def __init__(self):\r\n            super(ConvNet, self).__init__()\r\n            self.conv1 = nn.Conv2d(3, 6, kernel_size=5) \r\n            self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\r\n            self.fc1   = nn.Linear(864, 120) \r\n            self.fc2   = nn.Linear(120, 84)\r\n            self.fc3   = nn.Linear(84, num_classes)\r\n    \r\n        def forward(self, x):\r\n            out = F.relu(self.conv1(x))\r\n            out = F.max_pool2d(out, 2)\r\n            out = F.relu(self.conv2(out))\r\n            out = F.max_pool2d(out, 2)\r\n            out = out.view(out.size(0), -1)\r\n            out = F.relu(self.fc1(out))\r\n            out = F.relu(self.fc2(out))\r\n            out = self.fc3(out)\r\n    \r\n    model = ConvNet().to(device)\r\n    \r\n    # Loss and optimizer\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\r\n    \r\n    # Train the model\r\n    total_step = len(train_loader2)\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(train_loader2):\r\n            images = images.to(device)\r\n            labels = labels.to(device)\r\n    \r\n            # Forward pass\r\n            outputs = model(images)\r\n    #         print(images)\r\n            \r\n            loss = criterion(outputs, labels)\r\n    \r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n    \r\n            if (i % 10) == 0:\r\n                print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39; \r\n                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n\r\nUpdate : \r\n\r\nRemoving these lines : \r\n\r\n    out = F.relu(self.conv2(out))\r\n    out = F.max_pool2d(out, 2)\r\n\r\nfixes issue of dimensions being smaller than kernel.",
        "accepted_answer_markdown": "There is one structured problem and one bug in your code. Here is the solution.\r\n\r\n\r\n    class ConvNet(nn.Module):\r\n        def __init__(self):\r\n            super(ConvNet, self).__init__()\r\n            self.conv1 = nn.Conv2d(3, 6, kernel_size=5) \r\n            self.conv2 = nn.Conv2d(6, 16, kernel_size=1)  # kernel_size 5----&gt; 1\r\n            self.fc1   = nn.Linear(384, 120)              # FC NodeCount864--&gt; 384\r\n            self.fc2   = nn.Linear(120, 84)\r\n            self.fc3   = nn.Linear(84, num_classes)\r\n    \r\n        def forward(self, x):\r\n            out = F.relu(self.conv1(x))\r\n            out = F.max_pool2d(out, 2)\r\n            out = F.relu(self.conv2(out))\r\n            out = F.max_pool2d(out, 2)\r\n            out = out.view(out.size(0), -1)\r\n            out = F.relu(self.fc1(out))\r\n            out = F.relu(self.fc2(out))\r\n            out = self.fc3(out)\r\n            return out                        # Don&#39;t forget to return the output\r\n\r\n\r\n\r\n\r\n"
    },
    {
        "question_id": "52458508",
        "accepted_answer_id": "56849357",
        "question_title": "Why would Pytorch (CUDA) be running slow on GPU",
        "question_markdown": "I have been playing around with Pytorch on Linux for some time now and recently decided to try get more scripts to run with my GPU on my Windows desktop. Since trying this I have noticed a massive performance difference between my GPU execution time and my CPU execution time, on the same scripts, such that my GPU is significantly slow than CPU. To illustrate this I just a tutorial program found here (https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors)\r\n\r\n    import torch\r\n    import datetime\r\n    print(torch.__version__)\r\n    \r\n    dtype = torch.double\r\n    #device = torch.device(&quot;cpu&quot;)\r\n    device = torch.device(&quot;cuda:0&quot;)\r\n    \r\n    # N is batch size; D_in is input dimension;\r\n    # H is hidden dimension; D_out is output dimension.\r\n    N, D_in, H, D_out = 64, 1000, 100, 10\r\n    \r\n    # Create random input and output data\r\n    x = torch.randn(N, D_in, device=device, dtype=dtype)\r\n    y = torch.randn(N, D_out, device=device, dtype=dtype)\r\n    \r\n    # Randomly initialize weights\r\n    w1 = torch.randn(D_in, H, device=device, dtype=dtype)\r\n    w2 = torch.randn(H, D_out, device=device, dtype=dtype)\r\n    \r\n    \r\n    start = datetime.datetime.now()\r\n    learning_rate = 1e-6\r\n    for t in range(5000):\r\n        # Forward pass: compute predicted y\r\n        h = x.mm(w1)\r\n        h_relu = h.clamp(min=0)\r\n        y_pred = h_relu.mm(w2)\r\n    \r\n        # Compute and print loss\r\n        loss = (y_pred - y).pow(2).sum().item()\r\n        #print(t, loss)\r\n    \r\n        # Backprop to compute gradients of w1 and w2 with respect to loss\r\n        grad_y_pred = 2.0 * (y_pred - y)\r\n        grad_w2 = h_relu.t().mm(grad_y_pred)\r\n        grad_h_relu = grad_y_pred.mm(w2.t())\r\n        grad_h = grad_h_relu.clone()\r\n        grad_h[h &lt; 0] = 0\r\n        grad_w1 = x.t().mm(grad_h)\r\n    \r\n        # Update weights using gradient descent\r\n        w1 -= learning_rate * grad_w1\r\n        w2 -= learning_rate * grad_w2\r\n        \r\n    end = datetime.datetime.now()\r\n    \r\n    print(end-start)\r\n\r\n\r\n\r\n\r\nI increased the number of Epoch&#39;s from 500 to 5000 as I have read that the first CUDA call is very slow due to initialisation. However the performance issue still exists.\r\n\r\n\r\nWith `device = torch.device(&quot;cpu&quot;)` the final time printed out is normal around 3-4 seconds, well `device = torch.device(&quot;cuda:0&quot;)` executes in around 13-15 seconds\r\n\r\n\r\nI have reinstalled Pytorch a number of different ways (uninstalling the previous installation of course) and the problem still persists. I am hoping that someone can help me, if I have perhaps missed a set (didn&#39;t install some other API/program) or am doing something wrong in the code.\r\n\r\n\r\n\r\nPython: v3.6\r\n\r\nPytorch:v0.4.1\r\n\r\nGPU: NVIDIA GeForce GTX 1060 6GB\r\n\r\nAny help would be appreciated :slight_smile:\r\n",
        "accepted_answer_markdown": "Running on gpu could be expensive when you run with smaller batch size. If you put more data to gpu, means increasing the batch size, then you could observe significance amount of increase in data. Yes gpu is running better with float32 than double. \r\nTry this\r\n\r\n**\r\n\r\n    N, D_in, H, D_out = 128, 1000, 500, 10\r\n    dtype = torch.float32\r\n\r\n**"
    },
    {
        "question_id": "52761666",
        "accepted_answer_id": "52771444",
        "question_title": "PyTorch RuntimeError Invalid argument 2 of size",
        "question_markdown": "I am experimenting with a neural network (PyTorch) and I get this error.\r\n\r\n&gt; RuntimeError: invalid argument 2: size &#39;[32 x 9216]&#39; is invalid for input with 8192 elements at /pytorch/aten/src/TH/THStorage.cpp:84\r\n\r\nMy task is about image classification with AlexNet and I have backtracked the error to be the size of the images supplied to the neural network. My question is, given the network architecture with its parameters, how does one determine the correct image size required by the network?\r\n\r\nAs per my code below, I first transform the training images before feeding into the neural network. But I noticed the neural network can only accept the size of `224` and or else it gives the error above. For instance, my instinct was to apply `transforms.RandomResizedCrop` of size 64 but apparently this is wrong. Is there a formula to determine the size required?\r\n\r\n**Code**\r\n\r\n    # transformation to be done on images\r\n    transform_train = transforms.Compose([\r\n        transforms.RandomResizedCrop(64),\r\n        transforms.RandomHorizontalFlip(),\r\n        transforms.ToTensor(),\r\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\r\n    ])\r\n\r\n    class AlexNet(nn.Module):\r\n    \r\n        def __init__(self, num_classes=1000):\r\n            super(AlexNet, self).__init__()\r\n            self.features = nn.Sequential(\r\n                nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n                nn.Conv2d(64, 192, kernel_size=5, padding=2),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n                nn.Conv2d(192, 384, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(384, 256, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.Conv2d(256, 256, kernel_size=3, padding=1),\r\n                nn.ReLU(inplace=True),\r\n                nn.MaxPool2d(kernel_size=3, stride=2),\r\n            )\r\n            self.classifier = nn.Sequential(\r\n                nn.Dropout(),\r\n                nn.Linear(256 * 6 * 6, 4096),\r\n                nn.ReLU(inplace=True),\r\n                nn.Dropout(),\r\n                nn.Linear(4096, 4096),\r\n                nn.ReLU(inplace=True),\r\n                nn.Linear(4096, num_classes),\r\n            )\r\n    \r\n        def forward(self, x):\r\n            x = self.features(x)\r\n            x = x.view(x.size(0), 256 * 6 * 6)\r\n            x = self.classifier(x)\r\n            return x",
        "accepted_answer_markdown": "I have figured out the algorithm of getting the right input size.\r\n\r\n    Out = float(((W\u2212F+2P)/S)+1)\r\n\r\nwhere\r\n\r\n - Out = Output shape\r\n - W = Image volume size (image size)\r\n - F = Receptive field (filter size)\r\n - P = Padding\r\n - S = Stride\r\n\r\nFactoring in the given network hyperparameters,\r\n\r\nThe require Image size I need would be\r\n\r\n    W = (55 - 1) * 4 - 2(2) + 11\r\n      =  223\r\n      \u2a70  224"
    },
    {
        "question_id": "52994435",
        "accepted_answer_id": "53009161",
        "question_title": "PyTorch: create non-fully-connected layer / concatenate output of hidden layers",
        "question_markdown": "In PyTorch, I want to create a hidden layer whose neurons are not fully connected to the output layer. I try to concatenate the output of two linear layers but run into the following error:\r\n\r\n&gt;RuntimeError: size mismatch, m1: [2 x 2], m2: [4 x 4]\r\n\r\nmy current code:\r\n\r\n    class NeuralNet2(nn.Module):\r\n        def __init__(self):\r\n            super(NeuralNet2, self).__init__()\r\n    \r\n            self.input = nn.Linear(2, 40)\r\n            self.hiddenLeft = nn.Linear(40, 2)\r\n            self.hiddenRight = nn.Linear(40, 2)\r\n            self.out = nn.Linear(4, 4)\r\n    \r\n        def forward(self, x):\r\n            x = self.input(x)\r\n            xLeft, xRight = torch.sigmoid(self.hiddenLeft(x)), torch.sigmoid(self.hiddenRight(x))\r\n            x = torch.cat((xLeft, xRight))\r\n            x = self.out(x)\r\n    \r\n            return x\r\n\r\nI don&#39;t get why there is a size mismatch? Is there an alternative way to implement non-fully-connected layers in pytorch?",
        "accepted_answer_markdown": "It turned out to be a simple comprehension problem with the concatenation function.\r\nChanging\r\n    `x = torch.cat((xLeft, xRight))`\r\nto\r\n    `x = torch.cat((xLeft, xRight), dim=1)`\r\ndid the trick.\r\nThanks @dennlinger\r\n\r\n"
    },
    {
        "question_id": "53032586",
        "accepted_answer_id": "53035016",
        "question_title": "AttributeError: &#39;tuple&#39; object has no attribute &#39;dim&#39;, when feeding input to Pytorch LSTM network",
        "question_markdown": "I am trying to run the following code:\r\n\r\n    import matplotlib.pylab as plt\r\n    import numpy as np\r\n    import torch\r\n    import torch.nn as nn\r\n\r\n    class LSTM(nn.Module):\r\n        def __init__(self, input_shape, n_actions):\r\n            super(LSTM, self).__init__()\r\n\r\n            self.lstm = nn.LSTM(input_shape, 12)\r\n            self.hidden2tag = nn.Linear(12, n_actions)\r\n\r\n        def forward(self, x):\r\n            out = self.lstm(x)\r\n            out = self.hidden2tag(out)\r\n            return out\r\n\r\n\r\n    state = [(1,2,3,4,5),(2,3,4,5,6),(3,4,5,6,7),(4,5,6,7,8),(5,6,7,8,9),(6,7,8,9,0)]\r\n\r\n    device = torch.device(&quot;cuda&quot;)\r\n    net = LSTM(5, 3).to(device)\r\n\r\n    state_v = torch.FloatTensor(state).to(device)\r\n\r\n    q_vals_v = net(state_v.view(1, state_v.shape[0], state_v.shape[1]))\r\n    _, action = int(torch.max(q_vals_v, dim=1).item())\r\n\r\nAnd that returns this error:\r\n\r\n&lt;!-- language: none --&gt;\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;/home/dikkerj/Documents/PycharmProjects/LSTMReactor/QuestionStackoverflow.py&quot;, line 26, in &lt;module&gt;\r\n        q_vals_v = net(state_v.view(1, state_v.shape[0], state_v.shape[1]))\r\n      File &quot;/home/dikkerj/.local/lib/python3.5/site-packages/torch/nn/modules/module.py&quot;, line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File &quot;/home/dikkerj/Documents/PycharmProjects/LSTMReactor/QuestionStackoverflow.py&quot;, line 15, in forward\r\n        out = self.hidden2tag(out)\r\n      File &quot;/home/dikkerj/.local/lib/python3.5/site-packages/torch/nn/modules/module.py&quot;, line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File &quot;/home/dikkerj/.local/lib/python3.5/site-packages/torch/nn/modules/linear.py&quot;, line 55, in forward\r\n        return F.linear(input, self.weight, self.bias)\r\n      File &quot;/home/dikkerj/.local/lib/python3.5/site-packages/torch/nn/functional.py&quot;, line 1022, in linear\r\n        if input.dim() == 2 and bias is not None:\r\n    AttributeError: &#39;tuple&#39; object has no attribute &#39;dim&#39;\r\n\r\nAnyone knows how to fix this? (to get rid of the tensor being a tuple so that it can be fed into the LSTM network)\r\n",
        "accepted_answer_markdown": "The pytorch LSTM returns a tuple.&lt;br&gt; So you get this error as your linear layer `self.hidden2tag` can not handle this tuple.\r\n\r\nSo change:\r\n\r\n    out = self.lstm(x)\r\n\r\nto\r\n\r\n    out, states = self.lstm(x)\r\n\r\nThis will fix your error, by splitting up the tuple so that `out` is just your output tensor.\r\n\r\n`out` then stores the hidden states, while `states` is another tuple that contains the last hidden and cell state. \r\n\r\n*You can also take a look here:&lt;br&gt;\r\nhttps://pytorch.org/docs/stable/nn.html#torch.nn.LSTM*\r\n\r\n*You will get another error for the last line as `max()` returns a tuple as well. But this should be easy to fix and is yet different error :)*\r\n\r\n"
    },
    {
        "question_id": "53416833",
        "accepted_answer_id": "53420966",
        "question_title": "RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[3, 1, 224, 224] to have 3 channels, but got 1 channels instead",
        "question_markdown": "In the code below:\r\n\r\n        model_ft.eval()\r\n        test_data, test_target = image_datasets[&#39;train&#39;][idx]\r\n        test_data = test_data.cuda()\r\n        #test_target = test_target.cuda()\r\n        test_target = torch.tensor(test_target)\r\n        test_target = test_target.cuda()\r\n        test_data.unsqueeze_(1)\r\n        test_target.unsqueeze_(0)\r\n        print(test_data.shape)\r\n        output = model_ft(test_data)\r\n\r\n\r\nI get the following error:\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;test_loocv.py&quot;, line 245, in &lt;module&gt;\r\n        output = model_ft(test_data)\r\n      File &quot;/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File &quot;/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/resnet.py&quot;, line 139, in forward\r\n      File &quot;/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py&quot;, line 477, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n      File &quot;/scratch/sjn-p3/anaconda/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py&quot;, line 301, in forward\r\n        self.padding, self.dilation, self.groups)\r\n    RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[3, 1, 224, 224] to have 3 channels, but got 1 channels instead\r\n\r\n\r\nAlso, test_data has the shape: torch.Size([3, 1, 224, 224]).\r\n\r\nHow should I fix this?",
        "accepted_answer_markdown": "Here&#39;s the fix:\r\n\r\n    test_data, test_target = image_datasets[&#39;train&#39;][idx]\r\n    test_data = test_data.cuda()\r\n    test_target = torch.tensor(test_target)\r\n    test_target = test_target.cuda()\r\n    test_data.unsqueeze_(0)\r\n    test_target.unsqueeze_(0)\r\n    output = model_ft(test_data)\r\n\r\nI had to change `test_data.unsqueeze_(1)` to `test_data.unsqueeze_(0)`"
    },
    {
        "question_id": "53421999",
        "accepted_answer_id": "54415604",
        "question_title": "how to save torchtext Dataset?",
        "question_markdown": "I&#39;m working with text and use [`torchtext.data.Dataset`][1].\r\nCreating the dataset takes a considerable amount of time.\r\nFor just running the program this is still acceptable. But I would like to debug the torch code for the neural network. And if python is started in debug mode, the dataset creation takes roughly 20 minutes (!!). That&#39;s just to get a working environment where I can debug-step through the neural network code.\r\n\r\nI would like to save the Dataset, for example with pickle. This sample code is taken from [here][2], but I removed everything that is not necessary for this example:\r\n\r\n    from torchtext import data\r\n    from fastai.nlp import *\r\n    \r\n    PATH = &#39;data/aclImdb/&#39;\r\n    \r\n    TRN_PATH = &#39;train/all/&#39;\r\n    VAL_PATH = &#39;test/all/&#39;\r\n    TRN = f&#39;{PATH}{TRN_PATH}&#39;\r\n    VAL = f&#39;{PATH}{VAL_PATH}&#39;\r\n    \r\n    TEXT = data.Field(lower=True, tokenize=&quot;spacy&quot;)\r\n    \r\n    bs = 64;\r\n    bptt = 70\r\n    \r\n    FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\r\n    md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=10)\r\n    \r\n    with open(&quot;md.pkl&quot;, &quot;wb&quot;) as file:\r\n        pickle.dump(md, file)\r\n\r\nTo run the code, you need the aclImdb dataset, it can be downloaded from [here][3]. Extract it into a `data/` folder next to this code snippet. The code produces an error in the last line, where pickle is used:\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;/home/lhk/programming/fastai_sandbox/lesson4-imdb2.py&quot;, line 27, in &lt;module&gt;\r\n        pickle.dump(md, file)\r\n    TypeError: &#39;generator&#39; object is not callable\r\n\r\nThe samples from fastai often use [dill][4] instead of pickle. But that doesn&#39;t work for me either.\r\n\r\n\r\n  [1]: https://torchtext.readthedocs.io/en/latest/data.html#torchtext.data.Dataset\r\n  [2]: https://github.com/fastai/fastai/blob/master/courses/dl1/lesson4-imdb.ipynb\r\n  [3]: http://files.fast.ai/data/aclImdb.tgz\r\n  [4]: https://pypi.org/project/dill/",
        "accepted_answer_markdown": "You can use dill instead of pickle. It works for me.\r\nYou can save a torchtext Field like\r\n```python\r\nTEXT = data.Field(sequential=True, tokenize=tokenizer, lower=True,fix_length=200,batch_first=True)\r\nwith open(&quot;model/TEXT.Field&quot;,&quot;wb&quot;)as f:\r\n     dill.dump(TEXT,f)\r\n```\r\nAnd load a Field like\r\n```python\r\nwith open(&quot;model/TEXT.Field&quot;,&quot;rb&quot;)as f:\r\n     TEXT=dill.load(f)\r\n```\r\nThe offical code suppport is under development\uff0cyou can follow https://github.com/pytorch/text/issues/451 and https://github.com/pytorch/text/issues/73 ."
    },
    {
        "question_id": "53500511",
        "accepted_answer_id": "53501869",
        "question_title": "ValueError: expected 2D or 3D input (got 1D input) PyTorch",
        "question_markdown": "    class VAE(torch.nn.Module):\r\n    \r\n     def __init__(self, input_size, hidden_sizes, batch_size):\r\n    \r\n        super(VAE, self).__init__()\r\n    \r\n        self.input_size = input_size\r\n        self.hidden_sizes = hidden_sizes\r\n        self.batch_size = batch_size\r\n        self.fc = torch.nn.Linear(input_size, hidden_sizes[0])\r\n        self.BN = torch.nn.BatchNorm1d(hidden_sizes[0])\r\n        self.fc1 = torch.nn.Linear(hidden_sizes[0], hidden_sizes[1])\r\n        self.BN1 = torch.nn.BatchNorm1d(hidden_sizes[1])\r\n        self.fc2 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[2])\r\n        self.BN2 = torch.nn.BatchNorm1d(hidden_sizes[2])\r\n        self.fc3_mu = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\r\n        self.fc3_sig = torch.nn.Linear(hidden_sizes[2], hidden_sizes[3])\r\n    \r\n        self.fc4 = torch.nn.Linear(hidden_sizes[3], hidden_sizes[2])\r\n        self.BN4 = torch.nn.BatchNorm1d(hidden_sizes[2])\r\n        self.fc5 = torch.nn.Linear(hidden_sizes[2], hidden_sizes[1])\r\n        self.BN5 = torch.nn.BatchNorm1d(hidden_sizes[1])\r\n        self.fc6 = torch.nn.Linear(hidden_sizes[1], hidden_sizes[0])\r\n        self.BN6 = torch.nn.BatchNorm1d(hidden_sizes[0])\r\n        self.fc7 = torch.nn.Linear(hidden_sizes[0], input_size)\r\n    \r\n    def sample_z(self, x_size, mu, log_var):\r\n         \r\n         eps = torch.randn(x_size, self.hidden_sizes[-1])\r\n         return(mu + torch.exp(log_var/2) * eps)\r\n    \r\n     def forward(self, x):\r\n    \r\n        ###########\r\n        # Encoder #\r\n        ###########\r\n    \r\n        out1 = self.fc(x)\r\n        out1 = nn.relu(self.BN(out1))\r\n        out2 = self.fc1(out1)\r\n        out2 = nn.relu(self.BN1(out2))\r\n        out3 = self.fc2(out2)\r\n        out3 = nn.relu(self.BN2(out3))\r\n        mu = self.fc3_mu(out3)\r\n        sig = nn.softplus(self.fc3_sig(out3))\r\n    \r\n        ###########\r\n        # Decoder  #\r\n        ###########\r\n    \r\n        # sample from the distro\r\n        sample = self.sample_z(x.size(0), mu, sig)\r\n        out4 = self.fc4(sample)\r\n        out4 = nn.relu(self.BN4(out4))\r\n        out5 = self.fc5(out4)\r\n        out5 = nn.relu(self.BN5(out5))\r\n        out6 = self.fc6(out5)\r\n        out6 = nn.relu(self.BN6(out6))\r\n        out7 = nn.sigmoid(self.fc7(out6))\r\n    \r\n        return(out7, mu, sig)\r\n    \r\n    vae = VAE(input_size, hidden_sizes, batch_size)\r\n    \r\n    vae.eval()\r\n    \r\n    x_sample, z_mu, z_var = vae(X)\r\n    \r\nThe error is:\r\n    \r\n&gt;     File &quot;VAE_LongTensor.py&quot;, line 200, in &lt;module&gt;    \r\n    x_sample, z_mu, z_var = vae(X)      \r\n    ValueError: expected 2D or 3D input (got 1D input)\r\n\r\n",
        "accepted_answer_markdown": "When you build a `nn.Module` in pytorch for processing 1D signals, pytorch actually expects the input to be 2D: first dimension is the &quot;mini batch&quot; dimension.  \r\nThus you need to add a singleton dimesion to your `X`:\r\n\r\n    x_sample, z_mu, z_var = vae(X[None, ...])"
    },
    {
        "question_id": "62061703",
        "accepted_answer_id": "62147652",
        "question_title": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?",
        "question_markdown": "I am using `pytorch-1.5` to do some `gan` test. My code is very simple gan code which just fit the sin(x) function:\r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n   \r\n    \r\n    # Hyper Parameters\r\n    BATCH_SIZE = 64\r\n    LR_G = 0.0001\r\n    LR_D = 0.0001 \r\n    N_IDEAS = 5  \r\n    ART_COMPONENTS = 15 \r\n    PAINT_POINTS = np.vstack([np.linspace(-1, 1, ART_COMPONENTS) for _ in range(BATCH_SIZE)])\r\n    \r\n    \r\n    def artist_works():  # painting from the famous artist (real target)\r\n        r = 0.02 * np.random.randn(1, ART_COMPONENTS)\r\n        paintings = np.sin(PAINT_POINTS * np.pi) + r\r\n        paintings = torch.from_numpy(paintings).float()\r\n        return paintings\r\n    \r\n    \r\n    G = nn.Sequential(  # Generator\r\n        nn.Linear(N_IDEAS, 128),  # random ideas (could from normal distribution)\r\n        nn.ReLU(),\r\n        nn.Linear(128, ART_COMPONENTS),  # making a painting from these random ideas\r\n    )\r\n    \r\n    D = nn.Sequential(  # Discriminator\r\n        nn.Linear(ART_COMPONENTS, 128),  # receive art work either from the famous artist or a newbie like G\r\n        nn.ReLU(),\r\n        nn.Linear(128, 1),\r\n        nn.Sigmoid(),  # tell the probability that the art work is made by artist\r\n    )\r\n    \r\n    opt_D = torch.optim.Adam(D.parameters(), lr=LR_D)\r\n    opt_G = torch.optim.Adam(G.parameters(), lr=LR_G)\r\n    \r\n\r\n    for step in range(10000):\r\n        artist_paintings = artist_works()  # real painting from artist\r\n        G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\r\n        G_paintings = G(G_ideas)  # fake painting from G (random ideas)\r\n\r\n        prob_artist0 = D(artist_paintings)  # D try to increase this prob\r\n        prob_artist1 = D(G_paintings)  # D try to reduce this prob\r\n\r\n        D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\r\n        G_loss = torch.mean(torch.log(1. - prob_artist1))\r\n\r\n        opt_D.zero_grad()\r\n        D_loss.backward(retain_graph=True)  # reusing computational graph\r\n        opt_D.step()\r\n\r\n        opt_G.zero_grad()\r\n        G_loss.backward()\r\n        opt_G.step()\r\n        \r\n   \r\nBut when i runing it got this error:\r\n\r\n    RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\r\n\r\nIs there something wrong with my code?",
        "accepted_answer_markdown": "This happens because the opt_D.step() modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. You can fix this by changing your code to:\r\n\r\n    for step in range(10000):\r\n        artist_paintings = artist_works()  # real painting from artist\r\n        G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\r\n        G_paintings = G(G_ideas)  # fake painting from G (random ideas)\r\n    \r\n        prob_artist1 = D(G_paintings)  # G tries to fool D\r\n    \r\n        G_loss = torch.mean(torch.log(1. - prob_artist1))\r\n        opt_G.zero_grad()\r\n        G_loss.backward()\r\n        opt_G.step()\r\n    \r\n        prob_artist0 = D(artist_paintings)  # D try to increase this prob\r\n        # detach here to make sure we don&#39;t backprop in G that was already changed.\r\n        prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\r\n    \r\n        D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\r\n        opt_D.zero_grad()\r\n        D_loss.backward(retain_graph=True)  # reusing computational graph\r\n        opt_D.step()\r\n\r\nYou can find more about this issue here https://github.com/pytorch/pytorch/issues/39141"
    },
    {
        "question_id": "62157890",
        "accepted_answer_id": "62160781",
        "question_title": "How to solve error: no match between expected input batch size and target batch size in PyTorch?",
        "question_markdown": "I attempting to create a logistic model on CIFAR10 dataset by PyTorch. However I am getting an error:\r\n\r\n&gt; ValueError: Expected input batch_size (900) to match target batch_size (300).\r\n\r\nWhat I think is happening is that 3*100 is 300. So may be the 3 axis of the RGB image is doing that but I cant figure how to solve.\r\n\r\nThese are my hyperparameters.\r\n```\r\nbatch_size = 100\r\nlearning_rate = 0.001\r\n\r\n# Other constants\r\ninput_size = 32*32\r\nnum_classes = 10\r\n```\r\nHere I divide my data into train, validation and test data.\r\n```\r\n\r\ntransform_train = transforms.Compose([transforms.Resize((32,32)),\r\n                                      transforms.RandomHorizontalFlip(),\r\n                                      transforms.RandomRotation(10),\r\n                                      transforms.RandomAffine(0, shear=10, scale=(0.8,1.2)),\r\n                                      transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\r\n                                      transforms.ToTensor(),\r\n                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n                               ])\r\n\r\n\r\ntransform = transforms.Compose([transforms.Resize((32,32)),\r\n                               transforms.ToTensor(),\r\n                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\r\n                               ])\r\ntraining_dataset = CIFAR10(root=&#39;D:\\PyTorch\\cifar-10-python&#39;, train=True, download=True, transform=transform_train)\r\ntrain_ds, val_ds = random_split(training_dataset, [40000, 10000])\r\ntest_ds = CIFAR10(root=&#39;D:\\PyTorch\\cifar-10-python&#39;, train=False, download=True, transform=transform)\r\n\r\ntrain_loader = DataLoader(train_ds, batch_size=100, shuffle=True)\r\nval_loader = DataLoader(val_ds, batch_size = 100, shuffle = False)\r\ntest_loader = DataLoader(test_ds, batch_size = 100, shuffle=False)\r\n```\r\nThis is the model.\r\n```\r\nclass CifarModel(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        self.linear = nn.Linear(input_size,  num_classes)\r\n    def forward(self, xb):\r\n        xb = xb.view(-1, 32*32)\r\n        #xb = xb.reshape(-1, 784)\r\n        print(xb.shape)\r\n        out = self.linear(xb)\r\n        return out\r\n    \r\n    def training_step(self, batch):\r\n        images, labels = batch \r\n        out = self(images)                  # Generate predictions\r\n        loss = F.cross_entropy(out, labels) # Calculate loss\r\n        return loss\r\n    \r\n    def validation_step(self, batch):\r\n        images, labels = batch \r\n        out = self(images)                    # Generate predictions\r\n        loss = F.cross_entropy(out, labels)   # Calculate loss\r\n        acc = accuracy(out, labels)           # Calculate accuracy\r\n        return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc.detach()}\r\n        \r\n    def validation_epoch_end(self, outputs):\r\n        batch_losses = [x[&#39;val_loss&#39;] for x in outputs]\r\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\r\n        batch_accs = [x[&#39;val_acc&#39;] for x in outputs]\r\n        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\r\n        return {&#39;val_loss&#39;: epoch_loss.item(), &#39;val_acc&#39;: epoch_acc.item()}\r\n    \r\n    def epoch_end(self, epoch, result):\r\n        print(&quot;Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}&quot;.format(epoch, result[&#39;val_loss&#39;], result[&#39;val_acc&#39;]))\r\n    \r\nmodel = CifarModel()\r\n```\r\n\r\n```\r\ndef accuracy(outputs, labels):\r\n    _, preds = torch.max(outputs, dim=1)\r\n    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\r\ndef evaluate(model, val_loader):\r\n    outputs = [model.validation_step(batch) for batch in val_loader]\r\n    return model.validation_epoch_end(outputs)\r\n\r\ndef fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\r\n    history = []\r\n    optimizer = opt_func(model.parameters(), lr)\r\n    for epoch in range(epochs):\r\n        # Training Phase \r\n        for batch in train_loader:\r\n            loss = model.training_step(batch)\r\n            loss.backward()\r\n            optimizer.step()\r\n            optimizer.zero_grad()\r\n        # Validation phase\r\n        result = evaluate(model, val_loader)\r\n        model.epoch_end(epoch, result)\r\n        history.append(result)\r\n    return history\r\nevaluate(model, val_loader)\r\n```\r\nHere&#39;s the error I encounter when I run evaluate function:\r\n```\r\ntorch.Size([900, 1024])\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n&lt;ipython-input-23-3621eab8de1a&gt; in &lt;module&gt;\r\n     21         history.append(result)\r\n     22     return history\r\n---&gt; 23 evaluate(model, val_loader)\r\n\r\n&lt;ipython-input-23-3621eab8de1a&gt; in evaluate(model, val_loader)\r\n      3     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\r\n      4 def evaluate(model, val_loader):\r\n----&gt; 5     outputs = [model.validation_step(batch) for batch in val_loader]\r\n      6     return model.validation_epoch_end(outputs)\r\n      7 \r\n\r\n&lt;ipython-input-23-3621eab8de1a&gt; in &lt;listcomp&gt;(.0)\r\n      3     return torch.tensor(torch.sum(preds == labels).item() / len(preds))\r\n      4 def evaluate(model, val_loader):\r\n----&gt; 5     outputs = [model.validation_step(batch) for batch in val_loader]\r\n      6     return model.validation_epoch_end(outputs)\r\n      7 \r\n\r\n&lt;ipython-input-22-c9e17d21eaff&gt; in validation_step(self, batch)\r\n     19         images, labels = batch\r\n     20         out = self(images)                    # Generate predictions\r\n---&gt; 21         loss = F.cross_entropy(out, labels)   # Calculate loss\r\n     22         acc = accuracy(out, labels)           # Calculate accuracy\r\n     23         return {&#39;val_loss&#39;: loss.detach(), &#39;val_acc&#39;: acc.detach()}\r\n\r\n~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)\r\n   2019     if size_average is not None or reduce is not None:\r\n   2020         reduction = _Reduction.legacy_get_string(size_average, reduce)\r\n-&gt; 2021     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n   2022 \r\n   2023 \r\n\r\n~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\r\n   1834     if input.size(0) != target.size(0):\r\n   1835         raise ValueError(&#39;Expected input batch_size ({}) to match target batch_size ({}).&#39;\r\n-&gt; 1836                          .format(input.size(0), target.size(0)))\r\n   1837     if dim == 2:\r\n   1838         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\n\r\nValueError: Expected input batch_size (900) to match target batch_size (300).\r\n```\r\n",
        "accepted_answer_markdown": "One problem that I am seeing is this line: \r\n\r\n    xb = xb.view(-1, 32*32) \r\n\r\nHere you are saying that the input image has only one channels. In other words, grayscale. Change it to reflect the number of channels (RGB):\r\n\r\n    xb = xb.view(-1, 32*32*3) "
    },
    {
        "question_id": "62372938",
        "accepted_answer_id": "62373469",
        "question_title": "Understanding input shape to PyTorch conv1D?",
        "question_markdown": "This seems to be one of the common questions on here ([1][1], [2][2], [3][3]), but I am still struggling to define the right shape for input to [PyTorch conv1D][4].\r\n\r\nI have text sequences of length 512 (number of tokens per sequence) with each token being represented by a vector of length 768 (embedding). The batch size I am using is 6.\r\n\r\nSo my input tensor to conv1D is of shape [6, 512, 768].\r\n\r\n    input = torch.randn(6, 512, 768) \r\n\r\nNow, I want to convolve over the length of my sequence (512) with a kernel size of 2 using the conv1D layer from PyTorch.\r\n\r\nUnderstanding 1:\r\n\r\nI assumed that &quot;in_channels&quot; are the embedding dimension of the conv1D layer. If so, then a conv1D layer will be defined in this way where\r\n\r\n    in_channels = embedding dimension (768)\r\n    out_channels = 100 (arbitrary number)\r\n    kernel = 2\r\n\r\n    convolution_layer = nn.conv1D(768, 100, 2)\r\n    feature_map = convolution_layer(input)\r\n\r\nBut with this assumption, I get the following error:\r\n\r\n    RuntimeError: Given groups=1, weight of size 100 768 2, expected input `[4, 512, 768]` to have 768 channels, but got 512 channels instead\r\n\r\nUnderstanding 2:\r\n\r\nThen I assumed that &quot;in_channels&quot; is the sequence length of the input sequence. If so, then a conv1D layer will be defined in this way where\r\n\r\n    in_channels = sequence length (512)\r\n    out_channels = 100 (arbitrary number)\r\n    kernel = 2\r\n\r\n    convolution_layer = nn.conv1D(512, 100, 2) \r\n    feature_map = convolution_layer(input)\r\n\r\nThis works fine and I get an output feature map of dimension `[batch_size, 100, 767]`. However, I am confused. Shouldn&#39;t the convolutional layer convolve over the sequence length of 512 and output a feature map of dimension `[batch_size, 100, 511]`?\r\n\r\nI will be really grateful for your help.\r\n\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/62162576/calculating-shape-of-conv1d-layer-in-pytorch\r\n  [2]: https://discuss.pytorch.org/t/convolutional-nn-for-text-input/3442\r\n  [3]: https://stackoverflow.com/questions/56845144/dataset-shape-mismatch-conv1d-input-layer\r\n  [4]: https://pytorch.org/docs/master/generated/torch.nn.Conv1d.html",
        "accepted_answer_markdown": "In pytorch your input shape of [6, 512, 768] should actually be [6, 768, 512] where the feature length is represented by the channel dimension and sequence length is the length dimension. Then you can define your conv1d with in/out channels of 768 and 100 respectively to get an output of [6, 100, 511].\r\n\r\nGiven an `input` of shape [6, 512, 768] you can convert it to the correct shape with [`Tensor.transpose`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.transpose).\r\n\r\n    input = input.transpose(1, 2).contiguous()\r\n\r\nThe `.contiguous()` ensures the memory of the tensor is stored contiguously which helps avoid potential issues during processing."
    },
    {
        "question_id": "62416819",
        "accepted_answer_id": "62417121",
        "question_title": "RuntimeError: Given groups=1, weight of size [32, 3, 16, 16, 16], expected input[100, 16, 16, 16, 3] to have 3 channels, but got 16 channels instead",
        "question_markdown": "RuntimeError: Given groups=1, weight of size [32, 3, 16, 16, 16], expected input[100, 16, 16, 16, 3] to have 3 channels, but got 16 channels instead\r\n\r\nThis is the portion of code I think where the problem is. \r\n\r\n```\r\ndef __init__(self):\r\n        super(Lightning_CNNModel, self).__init__()\r\n        \r\n        self.conv_layer1 = self._conv_layer_set(3, 32)\r\n        self.conv_layer2 = self._conv_layer_set(32, 64)\r\n        self.fc1 = nn.Linear(2**3*64, 128)\r\n        self.fc2 = nn.Linear(128, 10)   # num_classes = 10\r\n        self.relu = nn.LeakyReLU()\r\n        self.batch=nn.BatchNorm1d(128)\r\n        self.drop=nn.Dropout(p=0.15)\r\n    \r\n    def _conv_layer_set(self, in_c, out_c):\r\n        conv_layer = nn.Sequential(\r\n            nn.Conv3d(in_c, out_c, kernel_size=(3, 3, 3), padding=0),\r\n            nn.LeakyReLU(),\r\n            nn.MaxPool3d((2, 2, 2)),\r\n        )\r\n        \r\n        return conv_layer\r\n    \r\n    \r\n    \r\n    def forward(self, x):\r\n        out = self.conv_layer1(x)\r\n        out = self.conv_layer2(out)\r\n        out = out.view(out.size(0), -1)\r\n        out = self.fc1(out)\r\n        out = self.relu(out)\r\n        out = self.batch(out)\r\n        out = self.drop(out)\r\n        out = self.fc2(out)\r\n    \r\n        return out\r\n   \r\n```\r\nThis is the code I am working on ",
        "accepted_answer_markdown": "[`nn.Conv3d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv3d) expects the input to have size *[batch_size, channels, depth, height, width]*. The first convolution expects 3 channels, but with your input having size *[100, **16**, 16, 16, 3]*, that would be 16 channels.\r\n\r\nAssuming that your data is given as *[batch_size, depth, height, width, channels]*, you need to swap the dimensions around, which can be done with [`torch.Tensor.permute`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute):\r\n\r\n```py\r\n# From: [batch_size, depth, height, width, channels]\r\n# To: [batch_size, channels, depth, height, width]\r\ninput = input.permute(0, 4, 1, 2, 3)\r\n```"
    },
    {
        "question_id": "62422644",
        "accepted_answer_id": "62422862",
        "question_title": "Pytorch: Dimensions for cross entropy is correct but somehow wrong for MSE?",
        "question_markdown": "I was creating a program that would take in as input the Fashion MNIST set and I was tweaking around with my model to see how different parameters would change the accuracy.\r\n\r\nOne of the tweaks I made to my model was to change my model&#39;s loss function from cross entropy to MSE.\r\n\r\n```\r\n# The code above is miscellaneous training data import code\r\n\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle = True, num_workers=4)\r\ntestloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle = True, num_workers=4)\r\n\r\ndataiter = iter(trainloader)\r\nimages, labels = dataiter.next()\r\nfrom torch import nn, optim\r\nimport torch.nn.functional as F\r\n\r\nmodel = nn.Sequential(nn.Linear(784, 128),\r\n                      nn.ReLU(),\r\n                      nn.Linear(128, 10),\r\n                      nn.LogSoftmax(dim = 1)\r\n                     )\r\nmodel.to(device)\r\n\r\n# Define the loss\r\ncriterion = nn.MSELoss()\r\n\r\n# Define the optimizer\r\noptimizer = optim.Adam(model.parameters(), lr = 0.001)\r\n\r\n# Define the epochs\r\nepochs = 5\r\n\r\ntrain_losses, test_losses = [], []\r\n\r\nfor e in range(epochs):\r\n  running_loss = 0\r\n  for images, labels in trainloader:\r\n    # Flatten Fashion-MNIST images into a 784 long vector\r\n    images = images.to(device)\r\n    labels = labels.to(device)\r\n    images = images.view(images.shape[0], -1)\r\n    \r\n    # Training pass\r\n    optimizer.zero_grad()\r\n    \r\n    output = model.forward(images)\r\n    \r\n    loss = criterion(output, labels)\r\n    loss.backward()\r\n    optimizer.step()\r\n```\r\n\r\nMy model worked without any problems when using cross entropy loss, but when I changed to MSE loss, the interpreter complained and said that my tensors were different sizes and thus could not be computed.\r\n\r\n```\r\n&lt;class &#39;torch.Tensor&#39;&gt;\r\ntorch.Size([64, 1, 28, 28])\r\ntorch.Size([64])\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n&lt;ipython-input-62-ec6942122f02&gt; in &lt;module&gt;\r\n     44     output = model.forward(images)\r\n     45 \r\n---&gt; 46     loss = criterion(output, labels)\r\n     47     loss.backward()\r\n     48     optimizer.step()\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    530             result = self._slow_forward(*input, **kwargs)\r\n    531         else:\r\n--&gt; 532             result = self.forward(*input, **kwargs)\r\n    533         for hook in self._forward_hooks.values():\r\n    534             hook_result = hook(self, input, result)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/loss.py in forward(self, input, target)\r\n    429 \r\n    430     def forward(self, input, target):\r\n--&gt; 431         return F.mse_loss(input, target, reduction=self.reduction)\r\n    432 \r\n    433 \r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction)\r\n   2213             ret = torch.mean(ret) if reduction == &#39;mean&#39; else torch.sum(ret)\r\n   2214     else:\r\n-&gt; 2215         expanded_input, expanded_target = torch.broadcast_tensors(input, target)\r\n   2216         ret = torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))\r\n   2217     return ret\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/functional.py in broadcast_tensors(*tensors)\r\n     50                 [0, 1, 2]])\r\n     51     &quot;&quot;&quot;\r\n---&gt; 52     return torch._C._VariableFunctions.broadcast_tensors(tensors)\r\n     53 \r\n     54 \r\n\r\nRuntimeError: The size of tensor a (10) must match the size of tensor b (64) at non-singleton dimension 1\r\n```\r\n\r\nI tried reshaping my tensors and creating new arrays as placeholders for my output array, yet seem to be getting nowhere.\r\n\r\nWhy cross entropy loss works without any errors yet MSE does not?",
        "accepted_answer_markdown": "[`nn.CrossEntropyLoss`][1] and [`nn.MSELoss`][2] are completely different loss functions with fundamentally different rationale behind them.\r\n\r\n[`nn.CrossEntropyLoss`][1] is a loss function for _discrete labeling_ tasks. Therefore it expects as inputs a prediction of label _probabilities_ and targets as ground-truth _discrete labels_: `x` shape is `n`x`c` (where `c` is the number of labels) and `y` is of shape `n` of type _integer_, each target takes values in the range `{0,...,c-1}`.\r\n\r\n\r\nIn contrast, [`nn.MSELoss`][2] is a loss function for _regression_ tasks. Therefore it expects both predictions and targets to be of the same shape and data type. That is, if your prediction is of shape `n`x`c` the target should also be of shape `n`x`c` (and not just `n` as in the cross-entropy case).\r\n\r\nIf you are insisting on using MSE loss instead of cross entropy, you will need to convert the target integer labels you currently have (of shape `n`) into [1-hot vectors][3] of shape `n`x`c` and only then compute the MSE loss between your predictions and the generated one-hot targets.\r\n\r\n [1]: https://pytorch.org/docs/stable/nn.html#crossentropyloss\r\n [2]: https://pytorch.org/docs/stable/nn.html#mseloss\r\n [3]: https://pytorch.org/docs/stable/nn.functional.html#one-hot"
    },
    {
        "question_id": "62922324",
        "accepted_answer_id": "62949797",
        "question_title": "Is it possible to use non-pytoch augmentation in transform.compose",
        "question_markdown": "I am working on a data classification problem that takes images as an input in Pytorch. I would like the use the imgaug library, but unfortunatly I keep on getting errors. Here is my code.\r\n```\r\n#import necessary libraries\r\nfrom torch import nn\r\nfrom torchvision import models\r\nimport imgaug as ia\r\nimport imgaug.augmenters as iaa\r\nfrom torchvision import datasets\r\nfrom torch.utils.data.dataloader import DataLoader\r\nfrom torchvision import transforms\r\nfrom torch import optim\r\nimport numpy as np\r\nfrom PIL import Image\r\nimport glob\r\nfrom matplotlib import image\r\n```\r\n```\r\n#preprocess images\r\n#create data transformers\r\nseq = iaa.Sequential([iaa.Sometimes(0.5,iaa.GaussianBlur(sigma=(0,3.0))),\r\n                      iaa.Sometimes(0.5,iaa.LinearContrast((0.75,1.5))),\r\n                      iaa.AdditiveGaussianNoise(loc=0,scale=(0.0,0.05*255),per_channel=0.5),\r\n                      iaa.Sometimes(0.5,iaa.Affine(\r\n        scale={&quot;x&quot;: (0.8, 1.2), &quot;y&quot;: (0.8, 1.2)},\r\n        translate_percent={&quot;x&quot;: (-0.2, 0.2), &quot;y&quot;: (-0.2, 0.2)},\r\n        rotate=(-25, 25),\r\n        shear=(-8, 8)))],random_order=True)\r\n        \r\n\r\n \r\n\r\ntrain_transformation = transforms.Compose([transforms.RandomResizedCrop(300),\r\n                                           seq,\r\n                                           transforms.ToTensor()])\r\n\r\ntrain_data = datasets.ImageFolder(root = &#39;train&#39;)\r\ntrain_loader = DataLoader(train_data,shuffle = True,batch_size = 32,num_workers = 0)\r\ntrain_iter = iter(train_loader)\r\ntrain_iter.next()\r\n```\r\n\r\n```\r\nJupyter Server: local\r\nPython 3.8.4 64-bit: Idle\r\nCNN Cancer Detector\r\nMelanoma\r\nIntro\r\nSkin cancer is the most common form of cancer, with 1 in 5 Americans developping it by the time they reach 70 years old. Over 2 people die of skin cancer in the US every hour.[1] Early detection is key in saving peoples lives with skin cancer, with the early detection 5 year survival rate being 99%[1]. Dermatologist have to look at patients one by one, and must assess by eye whether or not a blemish is malignant or benign. Dermatologist&#39;s have around a 66% accuracy rate in assessing 752 different skin diseases, while CNN&#39;s, such as the one detailed in *Dermatologist-level classification of skin cancer with deep neural networks* published in Nature have achieved greater accuracy levels then dermatologist&#39;s, around 72.1%[2].\r\nBy converting cancer detection to easily deployable software, you could allow people to get accurate cancer testing at home, saving resources and time. By making cancer detection more accesible, people would be more likely to get tested, saving lives in the process. Below I will detail my process and results from a melanoma (the most deadly form of skin cancer) detector model using CNN&#39;s.\r\n\r\n[2]\r\n\r\n\r\n\r\nfrom PIL import Image\r\nimport glob\r\nfrom matplotlib import image\r\n\r\n\r\n\r\n[3]\r\n\r\n\r\n\r\n#preprocess images\r\n#create data transformers\r\nseq = iaa.Sequential([iaa.Sometimes(0.5,iaa.GaussianBlur(sigma=(0,3.0))),\r\n                      iaa.Sometimes(0.5,iaa.LinearContrast((0.75,1.5))),\r\n                      iaa.AdditiveGaussianNoise(loc=0,scale=(0.0,0.05*255),per_channel=0.5),\r\n                      iaa.Sometimes(0.5,iaa.Affine(\r\n        scale={&quot;x&quot;: (0.8, 1.2), &quot;y&quot;: (0.8, 1.2)},\r\n        translate_percent={&quot;x&quot;: (-0.2, 0.2), &quot;y&quot;: (-0.2, 0.2)},\r\n        rotate=(-25, 25),\r\n        shear=(-8, 8)))],random_order=True)\r\n\u2026train_iter = iter(train_loader)\r\ntrain_iter.next()\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n in \r\n     20 train_loader = DataLoader(train_data,shuffle = True,batch_size = 32,num_workers = 0)\r\n     21 train_iter = iter(train_loader)\r\n---&gt; 22 train_iter.next()\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in __next__(self)\r\n    343 \r\n    344     def __next__(self):\r\n--&gt; 345         data = self._next_data()\r\n    346         self._num_yielded += 1\r\n    347         if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in _next_data(self)\r\n    383     def _next_data(self):\r\n    384         index = self._next_index()  # may raise StopIteration\r\n--&gt; 385         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    386         if self._pin_memory:\r\n    387             data = _utils.pin_memory.pin_memory(data)\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py in fetch(self, possibly_batched_index)\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n---&gt; 47         return self.collate_fn(data)\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py in default_collate(batch)\r\n     77     elif isinstance(elem, container_abcs.Sequence):\r\n     78         transposed = zip(*batch)\r\n---&gt; 79         return [default_collate(samples) for samples in transposed]\r\n     80 \r\n     81     raise TypeError(default_collate_err_msg_format.format(elem_type))\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py in (.0)\r\n     77     elif isinstance(elem, container_abcs.Sequence):\r\n     78         transposed = zip(*batch)\r\n---&gt; 79         return [default_collate(samples) for samples in transposed]\r\n     80 \r\n     81     raise TypeError(default_collate_err_msg_format.format(elem_type))\r\n\r\nD:\\Python\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py in default_collate(batch)\r\n     79         return [default_collate(samples) for samples in transposed]\r\n     80 \r\n---&gt; 81     raise TypeError(default_collate_err_msg_format.format(elem_type))\r\n\r\nTypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found \r\n```\r\nI am aware that the input to the imgaug transformer must be a numpy array, but I am not sure how to incorporate that into my transform.compose (if I can at all that is.). When the imgaug seq is not in the transform.compose it works properly.\r\n\r\nThank you for the help!",
        "accepted_answer_markdown": "Looking at the documentation of transforms in pytorch gives us a hint of how to do it: https://pytorch.org/docs/stable/torchvision/transforms.html#generic-transforms\r\n\r\nI would try something like:\r\n```\r\ntrain_transformation = transforms.Compose([transforms.RandomResizedCrop(300),\r\n                                           transforms.Lambda(lambda x: seq(x)),\r\n                                           transforms.ToTensor()])\r\n```\r\n"
    },
    {
        "question_id": "62985943",
        "accepted_answer_id": "62989934",
        "question_title": "PyTorch AttributeError: &#39;UNet3D&#39; object has no attribute &#39;size&#39;",
        "question_markdown": "I am making an image segmentation transfer learning project using Pytorch. I am using the weights of this pre-trained model and class UNet3D. \r\nhttps://github.com/MrGiovanni/ModelsGenesis\r\n\r\n When I run the following codes I get this error at the line which MSELoss is called: &quot;AttributeError: &#39;DataParallel&#39; object has no attribute &#39;size&#39; &quot;.  \r\n\r\n \r\nWhen I delete the first line I get a similar error: &quot;AttributeError: &#39;UNet3D&#39; object has no attribute &#39;size&#39;\r\n\r\n&quot;\r\n\r\nHow can I convert DataParallel or UNet3D class to an object which MSELoss can use? I do not need DataParallel for now. I need to run the UNet3D() class for transfer learning.\r\n\r\n\r\n    model = nn.DataParallel(model, device_ids = [i for i in range(torch.cuda.device_count())])\r\n    criterion = nn.MSELoss()\r\n    optimizer = torch.optim.SGD(model.parameters(), conf.lr, momentum=0.9, weight_decay=0.0, nesterov=False)\r\n    scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\r\n    initial_epoch=10\r\n    for epoch in range(initial_epoch, conf.nb_epoch):\r\n        scheduler.step(epoch)\r\n        model.train()\r\n        for batch_ndx, (x,y) in enumerate(train_loader):\r\n            x, y = x.float().to(device), y.float().to(device)\r\n            pred = model\r\n            loss = criterion(pred, y)\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n``` \r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n&lt;ipython-input-46-20d1943b3498&gt; in &lt;module&gt;\r\n     25         x, y = x.float().to(device), y.float().to(device)\r\n     26         pred = model\r\n---&gt; 27         loss = criterion(pred, y)\r\n     28         optimizer.zero_grad()\r\n     29         loss.backward()\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    548             result = self._slow_forward(*input, **kwargs)\r\n    549         else:\r\n--&gt; 550             result = self.forward(*input, **kwargs)\r\n    551         for hook in self._forward_hooks.values():\r\n    552             hook_result = hook(self, input, result)\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/loss.py in forward(self, input, target)\r\n    430 \r\n    431     def forward(self, input, target):\r\n--&gt; 432         return F.mse_loss(input, target, reduction=self.reduction)\r\n    433 \r\n    434 \r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py in mse_loss(input, target, size_average, reduce, reduction)\r\n   2528                 mse_loss, tens_ops, input, target, size_average=size_average, reduce=reduce,\r\n   2529                 reduction=reduction)\r\n-&gt; 2530     if not (target.size() == input.size()):\r\n   2531         warnings.warn(&quot;Using a target size ({}) that is different to the input size ({}). &quot;\r\n   2532                       &quot;This will likely lead to incorrect results due to broadcasting. &quot;\r\n\r\n/opt/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py in __getattr__(self, name)\r\n    592                 return modules[name]\r\n    593         raise AttributeError(&quot;&#39;{}&#39; object has no attribute &#39;{}&#39;&quot;.format(\r\n--&gt; 594             type(self).__name__, name))\r\n    595 \r\n    596     def __setattr__(self, name, value):\r\n\r\nAttributeError: &#39;UNet3D&#39; object has no attribute &#39;size&#39;\r\n\r\n```\r\n\r\n\r\n",
        "accepted_answer_markdown": "You have a typo on this line:\r\n\r\n    pred = model\r\n\r\nshould be\r\n\r\n    pred = model(x)\r\n\r\nmodel is nn.Module object which describes the network.  x, y, pred are (supposed to be) torch tensors.\r\n\r\nAside from this particular case, I think it would be good to think about how to solve this type of problems in general.  \r\n\r\nYou saw an error (exception) on a certain line.  Is the problem there, or *earlier*?  Can you *isolate* the problem?  \r\n\r\nFor example, if you print out the arguments you&#39;re passing to criterion(pred, y) just before the call, do they look right?  (they don&#39;t)  \r\n\r\nWhat happens if you create a couple of tensors of the right shape just before the call and pass them instead?  (works fine)  \r\n\r\nWhat is the error *really* saying?  &quot;AttributeError: &#39;UNet3D&#39; object has no attribute &#39;size&#39;&quot; - well, of course it&#39;s not supposed to have a size, but why is the code trying to access it&#39;s size?  Actually, why is the code even able to access that object on that line?  (since the model is not supposed to be passed to the criterion function - right?)\r\n\r\nMaybe useful further reading: https://ericlippert.com/2014/03/05/how-to-debug-small-programs/\r\n"
    },
    {
        "question_id": "63001490",
        "accepted_answer_id": "63005432",
        "question_title": "PyTorch - RuntimeError: Error(s) in loading state_dict for VGG:",
        "question_markdown": "I&#39;ve trained a model using PyTorch and saved a state dict file. I have loaded the pre-trained model using the code below. I am getting an error message regarding RuntimeError: Error(s) in loading state_dict for VGG:\r\n\r\n```\r\nRuntimeError: Error(s) in loading state_dict for VGG:\r\n\tMissing key(s) in state_dict: &quot;features.0.weight&quot;, &quot;features.0.bias&quot;, &quot;features.2.weight&quot;, &quot;features.2.bias&quot;, &quot;features.5.weight&quot;, &quot;features.5.bias&quot;, &quot;features.7.weight&quot;, &quot;features.7.bias&quot;, &quot;features.10.weight&quot;, &quot;features.10.bias&quot;, &quot;features.12.weight&quot;, &quot;features.12.bias&quot;, &quot;features.14.weight&quot;, &quot;features.14.bias&quot;, &quot;features.17.weight&quot;, &quot;features.17.bias&quot;, &quot;features.19.weight&quot;, &quot;features.19.bias&quot;, &quot;features.21.weight&quot;, &quot;features.21.bias&quot;, &quot;features.24.weight&quot;, &quot;features.24.bias&quot;, &quot;features.26.weight&quot;, &quot;features.26.bias&quot;, &quot;features.28.weight&quot;, &quot;features.28.bias&quot;, &quot;classifier.0.weight&quot;, &quot;classifier.0.bias&quot;, &quot;classifier.3.weight&quot;, &quot;classifier.3.bias&quot;, &quot;classifier.6.weight&quot;, &quot;classifier.6.bias&quot;. \r\n\tUnexpected key(s) in state_dict: &quot;state_dict&quot;, &quot;optimizer_state_dict&quot;, &quot;globalStep&quot;, &quot;train_paths&quot;, &quot;test_paths&quot;. \r\n\r\n```\r\n\r\nI am following instruction available at this site: https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-across-devices\r\n\r\nMany Thanks\r\n\r\n```\r\nimport argparse\r\nimport datetime\r\nimport glob\r\nimport os\r\nimport random\r\nimport shutil\r\nimport time\r\nfrom os.path import join\r\n\r\nimport numpy as np\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nfrom torch.utils.data import DataLoader\r\nfrom torch.utils.tensorboard import SummaryWriter\r\nfrom torchvision.transforms import ToTensor\r\nfrom tqdm import tqdm\r\nimport torch.optim as optim\r\n\r\nfrom convnet3 import Convnet\r\nfrom dataset2 import CellsDataset\r\n\r\nfrom convnet3 import Convnet\r\nfrom VGG import VGG\r\nfrom dataset2 import CellsDataset\r\nfrom torchvision import models\r\nfrom Conv import Conv2d\r\n\r\nparser = argparse.ArgumentParser(&#39;Predicting hits from pixels&#39;)\r\nparser.add_argument(&#39;name&#39;,type=str,help=&#39;Name of experiment&#39;)\r\nparser.add_argument(&#39;data_dir&#39;,type=str,help=&#39;Path to data directory containing images and gt.csv&#39;)\r\nparser.add_argument(&#39;--weight_decay&#39;,type=float,default=0.0,help=&#39;Weight decay coefficient (something like 10^-5)&#39;)\r\nparser.add_argument(&#39;--lr&#39;,type=float,default=0.0001,help=&#39;Learning rate&#39;)\r\nargs = parser.parse_args()\r\n\r\nmetadata = pd.read_csv(join(args.data_dir,&#39;gt.csv&#39;))\r\nmetadata.set_index(&#39;filename&#39;, inplace=True)\r\n\r\n# create datasets:\r\n\r\ndataset = CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\r\ndataset = DataLoader(dataset,num_workers=4,pin_memory=True)\r\nmodel_path = &#39;/Users/nubstech/Documents/GitHub/CellCountingDirectCount/VGG_model_V1/checkpoints/checkpoint.pth&#39;\r\n\r\nclass VGG(nn.Module):\r\n    def __init__(self, pretrained=True):\r\n        super(VGG, self).__init__()\r\n        vgg = models.vgg16(pretrained=pretrained)\r\n        # if pretrained:\r\n        vgg.load_state_dict(torch.load(model_path))\r\n        features = list(vgg.features.children())\r\n        self.features4 = nn.Sequential(*features[0:23])\r\n\r\n\r\n        self.de_pred = nn.Sequential(Conv2d(512, 128, 1, same_padding=True, NL=&#39;relu&#39;),\r\n                                     Conv2d(128, 1, 1, same_padding=True, NL=&#39;relu&#39;))\r\n\r\n\r\n    def forward(self, x):\r\n        x = self.features4(x)       \r\n        x = self.de_pred(x)\r\n\r\n        return x\r\n\r\nmodel=VGG()\r\n#model.load_state_dict(torch.load(model_path),strict=False)\r\nmodel.eval()        \r\n\r\n#optimizer = torch.optim.Adam(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)\r\n\r\nfor images, paths in tqdm(dataset):\r\n\r\n    targets = torch.tensor([metadata[&#39;count&#39;][os.path.split(path)[-1]] for path in paths]) # B\r\n    targets = targets.float()\r\n\r\n    # code to print training data to a csv file\r\n    #filename=CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\r\n    output = model(images) # B x 1 x 9 x 9 (analogous to a heatmap)\r\n    preds = output.sum(dim=[1,2,3]) # predicted cell counts (vector of length B)\r\n    print(preds)\r\n    paths_test = np.array([paths])\r\n    names_preds = np.hstack(paths)\r\n    print(names_preds)                \r\n    df=pd.DataFrame({&#39;Image_Name&#39;:names_preds, &#39;Target&#39;:targets.detach(), &#39;Prediction&#39;:preds.detach()})\r\n    print(df) \r\n    # save image name, targets, and predictions\r\n    df.to_csv(r&#39;model.csv&#39;, index=False, mode=&#39;a&#39;)\r\n\r\n```\r\n\r\nCode for saving the state dict\r\n\r\n```\r\n        torch.save({&#39;state_dict&#39;:model.state_dict(),\r\n                    &#39;optimizer_state_dict&#39;:optimizer.state_dict(),\r\n                    &#39;globalStep&#39;:global_step,\r\n                    &#39;train_paths&#39;:dataset_train.files,\r\n                    &#39;test_paths&#39;:dataset_test.files},checkpoint_path)\r\n```",
        "accepted_answer_markdown": "The problem is that what is being saved is not the same as what is expected to be loaded.  The code is trying to load *only* a state_dict; it is saving quite a bit more than that - looks like a state_dict inside another dict with additional info.  The load method doesn&#39;t have any logic to look inside the dict. \r\n\r\nThis should work:\r\n\r\n    import torch, torchvision.models\r\n    model = torchvision.models.vgg16()\r\n    path = &#39;test.pth&#39;\r\n    torch.save(model.state_dict(), path) # nothing else here\r\n    model.load_state_dict(torch.load(path))"
    },
    {
        "question_id": "63125160",
        "accepted_answer_id": "63125545",
        "question_title": "How do I make custom pytorch datasets structured like the torchvision datasets?",
        "question_markdown": "I&#39;m new to pytorch and I&#39;m trying to reuse a Fashion MNIST CNN ([from deeplizard][1]) to categorize my timeseries data. I&#39;m finding it hard to understand the structure of datasets, because following [this official tutorial][2] and [this SO question][3] as best I can, I&#39;m getting something too simple. I think this is because I don&#39;t understand OOP very well. The dataset I&#39;ve made works fine in my CNN for training but then trying to analyse the results with their code I get stuck.\r\n\r\nSo I create a dataset from two pytorch tensors called features [4050, 1, 150, 6] and targets[4050]:\r\n\r\n```\r\ntrain_dataset = TensorDataset(features,targets) # create your datset\r\ntrain_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=False) # create your dataloader\r\nprint(train_dataset.__dict__.keys()) # list the attributes\r\n```\r\nI get this printed output from inspecting the attributes\r\n&gt; dict_keys([&#39;tensors&#39;])\r\n\r\nBut in the Fashion MNIST tutorial they access the data like this:\r\n```\r\ntrain_set = torchvision.datasets.FashionMNIST(\r\n    root=&#39;./data&#39;\r\n    ,train=True\r\n    ,download=True\r\n    ,transform=transforms.Compose([\r\n        transforms.ToTensor()\r\n    ])\r\n)\r\n\r\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=1000, shuffle=True)\r\nprint(train_set.__dict__.keys()) # list the attributes\r\n```\r\nAnd you get this printed output from inspecting the attributes\r\n\r\n&gt; dict_keys([&#39;root&#39;, &#39;transform&#39;, &#39;target_transform&#39;, &#39;transforms&#39;,\r\n&gt; &#39;train&#39;, &#39;data&#39;, &#39;targets&#39;])\r\n\r\nMy dataset works fine for training but when I get to later analysis parts of the tutorial, they want me to access parts of the dataset and I get an error:\r\n\r\n```\r\n# Analytics\r\nprediction_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\r\ntrain_preds = get_all_preds(network, prediction_loader)\r\npreds_correct = train_preds.argmax(dim=1).eq(train_dataset.targets).sum().item()\r\n\r\nprint(&#39;total correct:&#39;, preds_correct)\r\nprint(&#39;accuracy:&#39;, preds_correct / len(train_set))\r\n```\r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n&lt;ipython-input-73-daa87335a92a&gt; in &lt;module&gt;\r\n      4 prediction_loader = torch.utils.data.DataLoader(train_dataset, batch_size=50)\r\n      5 train_preds = get_all_preds(network, prediction_loader)\r\n----&gt; 6 preds_correct = train_preds.argmax(dim=1).eq(train_dataset.targets).sum().item()\r\n      7 \r\n      8 print(&#39;total correct:&#39;, preds_correct)\r\n\r\nAttributeError: &#39;TensorDataset&#39; object has no attribute &#39;targets&#39;\r\n\r\n```\r\nCan anyone tell me what&#39;s going on here? Is this something I need to change in how I make the datasets, or can I rewrite the analysis code somehow to access the right part of the dataset?\r\n\r\n\r\n  [1]: https://deeplizard.com/learn/video/8n-TGaBZnk4\r\n  [2]: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\r\n  [3]: https://stackoverflow.com/a/42054194/10365396",
        "accepted_answer_markdown": "The equivalent of `.targets` for `TensorDataset`s would be `train_dataset.tensors[1]`.\r\n\r\nThe implementation of [`TensorDataset`](https://github.com/pytorch/pytorch/blob/646042e0fb4b86e6323985962e7f6777c01b705b/torch/utils/data/dataset.py#L155-L173) is very simple:\r\n\r\n```python\r\nclass TensorDataset(Dataset[Tuple[Tensor, ...]]):\r\n    r&quot;&quot;&quot;Dataset wrapping tensors.\r\n    Each sample will be retrieved by indexing tensors along the first dimension.\r\n    Arguments:\r\n        *tensors (Tensor): tensors that have the same size of the first dimension.\r\n    &quot;&quot;&quot;\r\n    tensors: Tuple[Tensor, ...]\r\n\r\n    def __init__(self, *tensors: Tensor) -&gt; None:\r\n        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors)\r\n        self.tensors = tensors\r\n\r\n    def __getitem__(self, index):\r\n        return tuple(tensor[index] for tensor in self.tensors)\r\n\r\n    def __len__(self):\r\n        return self.tensors[0].size(0)\r\n```"
    },
    {
        "question_id": "63441299",
        "accepted_answer_id": "63473731",
        "question_title": "Pytorch CUDA OutOfMemory Error while training",
        "question_markdown": "I&#39;m trying to train a PyTorch FLAIR model in AWS Sagemaker.\r\nWhile doing so getting the following error:\r\n    \r\n    RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 7.31 MiB free; 10.80 GiB reserved in total by PyTorch)\r\n\r\nFor training I used sagemaker.pytorch.estimator.PyTorch class.\r\n\r\nI tried with different variants of instance types from ml.m5, g4dn to p3(even with a 96GB memory one).\r\nIn the ml.m5 getting the error with CPUmemoryIssue, in g4dn with GPUMemoryIssue and in the P3 getting GPUMemoryIssue mostly because Pytorch is using only one of the GPU of 12GB out of 8*12GB.\r\n\r\nNot getting anywhere to complete this training, even in local tried with a CPU machine and got the following error:\r\n\r\n    RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes. Buy new RAM!\r\n\r\nThe model training script:\r\n\r\n        corpus = ClassificationCorpus(data_folder, test_file=&#39;../data/exports/val.csv&#39;, train_file=&#39;../data/exports/train.csv&#39;)\r\n                                              \r\n        print(&quot;finished loading corpus&quot;)\r\n    \r\n        word_embeddings = [WordEmbeddings(&#39;glove&#39;), FlairEmbeddings(&#39;news-forward-fast&#39;), FlairEmbeddings(&#39;news-backward-fast&#39;)]\r\n    \r\n        document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\r\n    \r\n        classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\r\n    \r\n        trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\r\n    \r\n        trainer.train(&#39;../model_files&#39;, max_epochs=12,learning_rate=0.0001, train_with_dev=False, embeddings_storage_mode=&quot;none&quot;)\r\n\r\n\r\nP.S.: I was able to train the same architecture with a smaller dataset in my local GPU machine with a 4GB GTX 1650 DDR5 memory and it was really quick.\r\n\r\n",
        "accepted_answer_markdown": "Okay, so after 2 days of continuous debugging was able to find out the root cause.\r\nWhat I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.\r\nSo there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.\r\nSo there it was breaking.\r\n\r\n**To solve this**: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left/right/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.\r\n\r\nAfter this I hope you will be able to progress with your training, as it happened in my case.\r\n\r\nP.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue."
    },
    {
        "question_id": "63610626",
        "accepted_answer_id": "63684995",
        "question_title": "Pytorch geometric: Having issues with tensor sizes",
        "question_markdown": "This is the first time I&#39;m using Pytorch and Pytorch geometric. I&#39;m trying to create a simple Graph Neural Network with Pytorch Geometric. I&#39;m creating a custom dataset by following the Pytorch Geometric documentations and extending the InMemoryDataset. After that I split the dataset into training, validation and test dataset which sizes (3496, 437, 439) respectively. These are the number of graphs in each dataset. Here is my simple Neural Network\r\n\r\n    class Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = GCNConv(dataset.num_node_features, 10)\r\n        self.conv2 = GCNConv(10, dataset.num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index, batch = data.x, data.edge_index, data.batch\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.conv2(x, edge_index)\r\n\r\n        return F.log_softmax(x, dim=1)\r\n\r\nI get this error while training my model, which suggest that there&#39;s some issue with my input dimensions. Maybe the reason is behind my batch sizes?\r\n\r\n    RuntimeError: The following operation failed in the TorchScript interpreter.\r\n    Traceback of TorchScript (most recent call last):\r\n    File &quot;E:\\Users\\abc\\Anaconda3\\lib\\site-packages\\torch_scatter\\scatter.py&quot;, line 22, in scatter_add\r\n            size[dim] = int(index.max()) + 1\r\n        out = torch.zeros(size, dtype=src.dtype, device=src.device)\r\n        return out.scatter_add_(dim, index, src)\r\n               ~~~~~~~~~~~~~~~~ &lt;--- HERE\r\n    else:\r\n        return out.scatter_add_(dim, index, src)\r\n    RuntimeError: index 13654 is out of bounds for dimension 0 with size 678\r\n\r\nThe error happens specifically on this line of code in the Neural Network,\r\n\r\n    x = self.conv1(x, edge_index)\r\n\r\n**EDIT: Added more information about edge_index and explained in more detail about the data that I&#39;m using.**\r\n\r\nHere are the shapes of the variables that I&#39;m trying to pass\r\n\r\n    x: torch.Size([678, 43])\r\n    edge_index: torch.Size([2, 668])\r\n    torch.max(edge_index): tensor(541690)\r\n    torch.min(edge_index): tensor(1920)\r\n\r\nI&#39;m using a datalist which contains `Data(x=node_features, edge_index=edge_index, y=labels)` objects. When I&#39;m splitting the dataset into training, validation and test datasets, I get `(3496, 437, 439)` graphs in each dataset respectively. Originally I tried to create one single graph from my dataset, but I&#39;m not sure how it would work with `Dataloader` and minibatches.\r\n\r\n    train_loader = DataLoader(train_dataset, batch_size=batch_size)\r\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\r\n    test_loader = DataLoader(test_dataset, batch_size=batch_size)\r\n\r\nHere&#39;s the code that generates the graph from dataframe. I&#39;ve tried to create an simple graph where there are just some amount of vertices with some amount of edges connecting them. I&#39;ve probably overlooked something and that&#39;s why I have this issue. I&#39;ve tried to follow the Pytorch geometric documentation when creating this graph ([Pytorch Geometric: Creating your own dataset][1])\r\n\r\n    def process(self):\r\n            data_list = []\r\n\r\n            grouped = df.groupby(&#39;EntityId&#39;)\r\n            for id, group in grouped:\r\n                node_features = torch.tensor(group.drop([&#39;Labels&#39;], axis=1).values)\r\n                source_nodes = group.index[1:].values\r\n                target_nodes = group.index[:-1].values\r\n                labels = torch.tensor(group.Labels.values)\r\n                edge_index = torch.tensor([source_nodes, target_nodes])\r\n\r\n                data = Data(x=node_features, edge_index=edge_index, y=labels)\r\n                data_list.append(data)\r\n\r\n            if self.pre_filter is not None:\r\n                data_list = [data for data in data_list if self.pre_filter(data)]\r\n\r\n            if self.pre_transform is not None:\r\n                data_list = [self.pre_transform(data) for data in data_list]\r\n\r\n            data, slices = self.collate(data_list)\r\n            torch.save((data, slices), self.processed_paths[0])\r\nIf someone could help me with the process of creating a graph on any kind of data and using it with GCNConv, I would appreciate it.\r\n\r\n  [1]: https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html",
        "accepted_answer_markdown": "I agree with @trialNerror -- it is a data problem. Your `edge_index` should refer to the data nodes and its `max` should not be that high. Since you don&#39;t want to show us the data and ask for &quot;creating a graph on any kind of data &quot;, here it is.\r\n\r\nI mostly left your `Net` unchanged. You can play around with the constants stated to match with your data.\r\n\r\n```python\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\n\r\nfrom torch_geometric.nn import GCNConv\r\nfrom torch_geometric.data import Data\r\n\r\nnum_node_features = 100\r\nnum_classes = 2\r\nnum_nodes = 678\r\nnum_edges = 1500\r\nnum_hidden_nodes = 128\r\n\r\nx = torch.randn((num_nodes, num_node_features), dtype=torch.float32)\r\nedge_index = torch.randint(low=0, high=num_nodes, size=(2, num_edges), dtype=torch.long)\r\ny = torch.randint(low=0, high=num_classes, size=(num_nodes,), dtype=torch.long)\r\n\r\nclass Net(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = GCNConv(num_node_features, num_hidden_nodes)\r\n        self.conv2 = GCNConv(num_hidden_nodes, num_classes)\r\n\r\n    def forward(self, data):\r\n        x, edge_index = data.x, data.edge_index\r\n        x = self.conv1(x, edge_index)\r\n        x = F.relu(x)\r\n        x = F.dropout(x, training=self.training)\r\n        x = self.conv2(x, edge_index)\r\n\r\n        return F.log_softmax(x, dim=1)\r\n\r\ndata = Data(x=x, edge_index=edge_index, y=y)\r\n\r\nnet = Net()\r\n\r\noptimizer = torch.optim.Adam(net.parameters(), lr=1e-2)\r\nfor i in range(1000):\r\n    output = net(data)\r\n    loss = F.cross_entropy(output, data.y)\r\n    optimizer.zero_grad()\r\n    loss.backward()\r\n    optimizer.step()\r\n\r\n    if i % 100 == 0:\r\n        print(&#39;Accuracy: &#39;, (torch.argmax(output, dim=1)==data.y).float().mean())\r\n```\r\n\r\nOutput\r\n```\r\nAccuracy:  tensor(0.5059)\r\nAccuracy:  tensor(0.8702)\r\nAccuracy:  tensor(0.9159)\r\nAccuracy:  tensor(0.9233)\r\nAccuracy:  tensor(0.9336)\r\nAccuracy:  tensor(0.9484)\r\nAccuracy:  tensor(0.9602)\r\nAccuracy:  tensor(0.9676)\r\nAccuracy:  tensor(0.9705)\r\nAccuracy:  tensor(0.9749)\r\n```\r\n(yes we can overfit to random data)"
    },
    {
        "question_id": "63648735",
        "accepted_answer_id": "63650146",
        "question_title": "Pytorch crossentropy loss with 3d input",
        "question_markdown": "I have a network which outputs a 3D tensor of size `(batch_size, max_len, num_classes)`. My groud truth is in the shape `(batch_size, max_len)`. If I do perform one-hot encoding on the labels, it&#39;ll be of shape `(batch_size, max_len, num_classes)` i.e the values in `max_len` are integers in the range `[0, num_classes]`. Since the original code is too long, I have written a simpler version that reproduces the original error.\r\n\r\n```\r\ncriterion = nn.CrossEntropyLoss()\r\nbatch_size = 32\r\nmax_len = 350\r\nnum_classes = 1000\r\npred = torch.randn([batch_size, max_len, num_classes])\r\nlabel = torch.randint(0, num_classes,[batch_size, max_len])\r\npred = nn.Softmax(dim = 2)(pred)\r\ncriterion(pred, label)\r\n```\r\n\r\nthe shape of pred and label are respectively,`torch.Size([32, 350, 1000])` and `torch.Size([32, 350])`\r\n\r\nThe error encountered is \r\n\r\n    ValueError: Expected target size (32, 1000), got torch.Size([32, 350, 1000])\r\n\r\nIf I one-hot encode labels for computing the loss\r\n\r\n```\r\nx = nn.functional.one_hot(label)\r\ncriterion(pred, x)\r\n```\r\nit&#39;ll throw the following error\r\n\r\n    ValueError: Expected target size (32, 1000), got torch.Size([32, 350, 1000])\r\n",
        "accepted_answer_markdown": "From the [Pytorch documentation][1], `CrossEntropyLoss` expects the shape of its input to be `(N, C, ...)`, so the second dimension is always the number of classes. Your code should work if you reshape `preds` to be of size `(batch_size, num_classes, max_len)`. \r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html"
    },
    {
        "question_id": "63781490",
        "accepted_answer_id": "63816225",
        "question_title": "AssertionError: Torch not compiled with CUDA enabled (problem in torch vision)",
        "question_markdown": "so I am trying to run my object detection program and I keep getting the following error message:\r\n\r\nAssertionError: Torch not compiled with CUDA enabled.\r\n\r\nI don&#39;t understand why this happens, as I have a 2017 MacBook Pro with an AMD GPU, so I have no CUDA enabled GPU. \r\n\r\nI added this statement in my code to make sure the device is set to &#39;cpu&#39;, however, it looks as if the program keeps trying to run it through a GPU even though it does not exist.\r\n\r\n``` \r\nif torch.cuda.is_available():\r\n    device = torch.device(&#39;cuda&#39;)\r\n    \r\nelse:\r\n    device = torch.device(&#39;cpu&#39;)\r\n```\r\n\r\nThis is the place where the error happens (4th line):\r\n\r\n\r\n```\r\n    for epoch in range(num_epochs):\r\n        # train for one epoch, printing every 10 iterations\r\n        print(&quot;Hey&quot;)\r\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\r\n        print(&quot;Hey&quot;)\r\n        # update the learning rate\r\n        lr_scheduler.step()\r\n        # evaluate on the test dataset\r\n        evaluate(model, data_loader_test, device=device)\r\n```\r\nIt would be really great, if anyone could help me with this issue!\r\n\r\nThanks everyone in advance!\r\n\r\nPS: I already tried updating the Pytorch version, but still same problem.\r\n\r\nError output: \r\n\r\n[![Error output][1]][1]\r\n\r\n\r\n```\r\nimport os\r\nimport pandas as pd\r\nimport torch\r\nimport torch.utils.data\r\nimport torchvision\r\nfrom PIL import Image\r\nimport utils\r\nfrom engine import train_one_epoch, evaluate\r\nimport transforms as T\r\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\r\n\r\n\r\ndef parse_one_annot(path_to_data_file, filename):\r\n    data = pd.read_csv(path_to_data_file)\r\n    boxes_array = data[data[&quot;filename&quot;] == filename][[&quot;xmin&quot;, &quot;ymin&quot;, &quot;xmax&quot;, &quot;ymax&quot;]].values\r\n\r\n    return boxes_array\r\n\r\n\r\nclass RaccoonDataset(torch.utils.data.Dataset):\r\n\r\n    def __init__(self, root, data_file, transforms=None):\r\n        self.root = root\r\n        self.transforms = transforms\r\n        self.imgs = sorted(os.listdir(os.path.join(root, &quot;images&quot;)))\r\n        self.path_to_data_file = data_file\r\n\r\n    def __getitem__(self, idx):\r\n        # load images and bounding boxes\r\n        img_path = os.path.join(self.root, &quot;images&quot;, self.imgs[idx])\r\n        img = Image.open(img_path).convert(&quot;RGB&quot;)\r\n        box_list = parse_one_annot(self.path_to_data_file,\r\n                                   self.imgs[idx])\r\n        boxes = torch.as_tensor(box_list, dtype=torch.float32)\r\n\r\n        num_objs = len(box_list)\r\n        # there is only one class\r\n        labels = torch.ones((num_objs,), dtype=torch.int64)\r\n        image_id = torch.tensor([idx])\r\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\r\n        # suppose all instances are not crowd\r\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n        target = {}\r\n        target[&quot;boxes&quot;] = boxes\r\n        target[&quot;labels&quot;] = labels\r\n        target[&quot;image_id&quot;] = image_id\r\n        target[&quot;area&quot;] = area\r\n        target[&quot;iscrowd&quot;] = iscrowd\r\n\r\n        if self.transforms is not None:\r\n            img, target = self.transforms(img, target)\r\n            return img, target\r\n\r\n    def __len__(self):\r\n        return len(self.imgs)\r\n\r\n\r\ndataset = RaccoonDataset(root=&quot;./raccoon_dataset&quot;, data_file=&quot;./raccoon_dataset/data/raccoon_labels.csv&quot;)\r\n\r\ndataset.__getitem__(0)\r\n\r\n\r\ndef get_model(num_classes):\r\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n\r\n    in_features = model.roi_heads.box_predictor.cls_score.in_features\r\n    # replace the pre-trained head with a new on\r\n    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\r\n\r\n    return model\r\n\r\n\r\ndef get_transform(train):\r\n    transforms = []\r\n    # converts the image, a PIL image, into a PyTorch Tensor\r\n    transforms.append(T.ToTensor())\r\n\r\n    if train:\r\n        # during training, randomly flip the training images\r\n        # and ground-truth for data augmentation\r\n        transforms.append(T.RandomHorizontalFlip(0.5))\r\n    return T.Compose(transforms)\r\n\r\n\r\ndef main():\r\n\r\n    dataset = RaccoonDataset(root=&quot;./raccoon_dataset&quot;,\r\n                             data_file=&quot;raccoon_dataset/data/raccoon_labels.csv&quot;,\r\n                             transforms=get_transform(train=True))\r\n\r\n    dataset_test = RaccoonDataset(root=&quot;./raccoon_dataset&quot;,\r\n                                  data_file=&quot;raccoon_dataset/data/raccoon_labels.csv&quot;,\r\n                                  transforms=get_transform(train=False))\r\n\r\n    torch.manual_seed(1)\r\n    indices = torch.randperm(len(dataset)).tolist()\r\n    dataset = torch.utils.data.Subset(dataset, indices[:-40])\r\n    dataset_test = torch.utils.data.Subset(dataset_test, indices[-40:])\r\n\r\n    # define training and validation data loaders\r\n    data_loader = torch.utils.data.DataLoader(dataset, batch_size=2, shuffle=True, num_workers=4,\r\n                                              collate_fn=utils.collate_fn)\r\n    data_loader_test = torch.utils.data.DataLoader(dataset_test, batch_size=1, shuffle=False, num_workers=4,\r\n                                                   collate_fn=utils.collate_fn)\r\n\r\n    print(&quot;We have: {} examples, {} are training and {} testing&quot;.format(len(indices), len(dataset), len(dataset_test)))\r\n\r\n    if torch.cuda.is_available():\r\n        device = torch.device(&#39;cuda&#39;)\r\n    else:\r\n        device = torch.device(&#39;cpu&#39;)\r\n\r\n    num_classes = 2\r\n    model = get_model(num_classes)\r\n\r\n    # construct an optimizer\r\n    params = [p for p in model.parameters() if p.requires_grad]\r\n    optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\r\n\r\n    # and a learning rate scheduler which decreases the learning rate by\r\n    # 10x every 3 epochs\r\n\r\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\r\n\r\n    # let&#39;s train it for 10 epochs\r\n\r\n    num_epochs = 10\r\n\r\n    for epoch in range(num_epochs):\r\n        # train for one epoch, printing every 10 iterations\r\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\r\n        # update the learning rate\r\n        lr_scheduler.step()\r\n        # evaluate on the test dataset\r\n        evaluate(model, data_loader_test, device=device)\r\n\r\n\r\n    os.mkdir(&quot;pytorch object detection/raccoon/&quot;)\r\n    torch.save(model.state_dict(), &quot;pytorch object detection/raccoon/model&quot;)\r\n\r\n\r\nif __name__ == &#39;__main__&#39;:\r\n    main()\r\n```\r\n\r\n\r\n  [1]: https://i.sstatic.net/PoJiE.png",
        "accepted_answer_markdown": "Turns out I had to reinstall torch and torch vision to make everything work"
    },
    {
        "question_id": "63842961",
        "accepted_answer_id": "63882976",
        "question_title": "Anaconda reading wrong CUDA version",
        "question_markdown": "I have a conda environment with PyTorch and Tensorflow, which both require CUDA 9.0 (~cudatoolkit 9.0 from conda). After installing pytorch with torchvision and the cudatoolkit (like they provided on their website) I wanted to install Tensorflow, the problem here is that I get this error:\r\n\r\n\r\n    Collecting package metadata (current_repodata.json): done\r\n    Solving environment: failed with initial frozen solve. Retrying with flexible solve.\r\n    Collecting package metadata (repodata.json): done\r\n    Solving environment: failed with initial frozen solve. Retrying with flexible solve.\r\n    Solving environment: / \r\n    Found conflicts! Looking for incompatible packages.\r\n    This can take several minutes.  Press CTRL-C to abort.\r\n    failed                                                                                                   \r\n\r\n    UnsatisfiableError: The following specifications were found\r\n    to be incompatible with the existing python installation in your environment:\r\n\r\n    Specifications:\r\n\r\n      - tensorflow==1.12.0 -&gt; python[version=&#39;2.7.*|3.6.*&#39;]\r\n      - tensorflow==1.12.0 -&gt; python[version=&#39;&gt;=2.7,&lt;2.8.0a0|&gt;=3.6,&lt;3.7.0a0&#39;]\r\n\r\n    Your python: python=3.5\r\n\r\n    If python is on the left-most side of the chain, that&#39;s the version you&#39;ve asked for.\r\n    When python appears to the right, that indicates that the thing on the left is somehow\r\n    not available for the python version you are constrained to. Note that conda will not\r\n    change your python version to a different minor version unless you explicitly specify\r\n    that.\r\n\r\n    The following specifications were found to be incompatible with your system:\r\n\r\n      - feature:/linux-64::__cuda==10.2=0\r\n      - feature:|@/linux-64::__cuda==10.2=0\r\n\r\n    Your installed version is: 10.2\r\n\r\n\r\nIf I run `nvcc` or `nvidia-smi` on my host or the activated conda environment, I get that I have installed CUDA 10.2, even though `conda list` shows me that cudatoolkit 9.0 is installed. Any solution to this?\r\n\r\n\r\nEDIT:\r\n\r\nWhen running this code sample:\r\n\r\n    # setting device on GPU if available, else CPU\r\n    device = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\n    print(&#39;Using device:&#39;, device)\r\n    print()\r\n\r\n    #Additional Info when using cuda\r\n    if device.type == &#39;cuda&#39;:\r\n        print(torch.cuda.get_device_name(0))\r\n        print(&#39;Memory Usage:&#39;)\r\n        print(&#39;Allocated:&#39;, round(torch.cuda.memory_allocated(0)/1024**3,1), &#39;GB&#39;)\r\n        print(&#39;Cached:   &#39;, round(torch.cuda.memory_cached(0)/1024**3,1), &#39;GB&#39;)\r\n\r\n\r\n    print(torch.version.cuda)\r\n\r\n\r\nI get this output:\r\n\r\n    GeForce GTX 1050\r\n    Memory Usage:\r\n    Allocated: 0.0 GB\r\n    Cached:    0.0 GB\r\n    9.0.176\r\n\r\nSo PyTorch does get the correct CUDA version, I just cant get tensorflow-gpu installed.\r\n\r\n",
        "accepted_answer_markdown": "&gt; If I run `nvcc` or `nvidia-smi` on my host or the activated `conda` environment, I get that I have installed `CUDA 10.2`, even though `conda list` shows me that `cudatoolkit 9.0` is installed. Any solution to this?\r\n\r\n`cudatoolkit` doesn&#39;t ship with compiler (`nvcc`), thus when you run `nvcc` you start compiler from system wide installation. That&#39;s why it prints `10.2` istead of `9.0`, and `pytorch` sees the local `cudatoolkit`.\r\n\r\n[anaconda / packages / cudatoolkit ](https://anaconda.org/anaconda/cudatoolkit):\r\n\r\n&gt; This CUDA Toolkit includes GPU-accelerated libraries, and the CUDA runtime for the Conda ecosystem. For the full CUDA Toolkit with a compiler and development tools visit https://developer.nvidia.com/cuda-downloads\r\n\r\nFrom your comment above I understood that you are using `python=3.5.6`. So, first of all you should search for available `tensorflow` `py35` builds using:\r\n\r\n```bash\r\nconda search tensorflow | grep py35\r\n```\r\n\r\nI have the following output:\r\n\r\n```bash\r\ntensorflow                     1.9.0 eigen_py35h8c89287_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35h42d5ad8_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35h60c0932_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35hb39db67_1  pkgs/main           \r\ntensorflow                     1.9.0 mkl_py35h5be851a_1  pkgs/main           \r\ntensorflow                    1.10.0 eigen_py35h5ed898b_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35h566a776_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35ha6119f3_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35hd9c640d_0  pkgs/main           \r\ntensorflow                    1.10.0 mkl_py35heddcb22_0  pkgs/main\r\n```\r\n\r\nAs you can see there is no `tensorflow 1.12.0` builds for `py35`, and that&#39;s why you are getting that error. You can try to inspect other `conda` channels, for example, `conda-forge`:\r\n\r\n```bash\r\nconda search tensorflow -c conda-forge | grep py35\r\n```\r\n\r\nBut that wasn&#39;t helpful:\r\n\r\n```bash\r\ntensorflow                     0.9.0          py35_0  conda-forge         \r\ntensorflow                    0.10.0          py35_0  conda-forge         \r\ntensorflow                 0.11.0rc0          py35_0  conda-forge         \r\ntensorflow                 0.11.0rc2          py35_0  conda-forge         \r\ntensorflow                    0.11.0          py35_0  conda-forge         \r\ntensorflow                    0.12.1          py35_0  conda-forge         \r\ntensorflow                    0.12.1          py35_1  conda-forge         \r\ntensorflow                    0.12.1          py35_2  conda-forge         \r\ntensorflow                     1.0.0          py35_0  conda-forge         \r\ntensorflow                     1.1.0          py35_0  conda-forge         \r\ntensorflow                     1.2.0          py35_0  conda-forge         \r\ntensorflow                     1.2.1          py35_0  conda-forge         \r\ntensorflow                     1.3.0          py35_0  conda-forge         \r\ntensorflow                     1.4.0          py35_0  conda-forge         \r\ntensorflow                     1.5.0          py35_0  conda-forge         \r\ntensorflow                     1.5.1          py35_0  conda-forge         \r\ntensorflow                     1.6.0          py35_0  conda-forge         \r\ntensorflow                     1.8.0          py35_0  conda-forge         \r\ntensorflow                     1.8.0          py35_1  conda-forge         \r\ntensorflow                     1.9.0 eigen_py35h8c89287_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35h42d5ad8_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35h60c0932_1  pkgs/main           \r\ntensorflow                     1.9.0 gpu_py35hb39db67_1  pkgs/main           \r\ntensorflow                     1.9.0 mkl_py35h5be851a_1  pkgs/main           \r\ntensorflow                     1.9.0          py35_0  conda-forge         \r\ntensorflow                    1.10.0 eigen_py35h5ed898b_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35h566a776_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35ha6119f3_0  pkgs/main           \r\ntensorflow                    1.10.0 gpu_py35hd9c640d_0  pkgs/main           \r\ntensorflow                    1.10.0 mkl_py35heddcb22_0  pkgs/main           \r\ntensorflow                    1.10.0          py35_0  conda-forge\r\n```\r\n\r\nSo, the possible solutions are:\r\n1. Install one of the older available `tensorflow 1.10.0 gpu_py35` builds.\r\n2. Switch to `python 3.6`.\r\n\r\n```bash\r\nconda search tensorflow | grep py36\r\n\r\n...\r\ntensorflow                    1.11.0 gpu_py36h4459f94_0  pkgs/main           \r\ntensorflow                    1.11.0 gpu_py36h9c9050a_0  pkgs/main           \r\n...        \r\ntensorflow                    1.12.0 gpu_py36he68c306_0  pkgs/main           \r\ntensorflow                    1.12.0 gpu_py36he74679b_0  pkgs/main\r\n...         \r\n```\r\n\r\n**Note** that versions &gt;=1.13.1 doesn&#39;t support `CUDA 9`.\r\n\r\n3. Use `pip install` inside `conda` env to install missing `tensorflow` build, because `pip` hosts more build combinations: [Tested build configurations](https://www.tensorflow.org/install/source#tested_build_configurations)\r\n\r\nHere is some best practices from Anaconda how to use `pip` w/ `conda`: [Using Pip in a Conda Environment](https://www.anaconda.com/blog/using-pip-in-a-conda-environment)\r\n\r\n4. The last option is to build your own missing `conda` package with [`conda-build`](https://docs.conda.io/projects/conda-build/en/latest/index.html)"
    },
    {
        "question_id": "63927648",
        "accepted_answer_id": "63931158",
        "question_title": "Can&#39;t iterate through PyTorch DataLoader",
        "question_markdown": "I am trying to learn PyTorch and create my first neural network. I am using a custom dataset, here is a sample of the data:\r\n```\r\nID_REF\tcg00001854\tcg00270460\tcg00293191\tcg00585219\tcg00702638\tcg01434611\tcg02370734\tcg02644867\tcg02879967\tcg03036557\tcg03123104\tcg03670302\tcg04146801\tcg04570540\tcg04880546\tcg07044749\tcg07135408\tcg07303143\tcg07475178\tcg07553761\tcg07917901\tcg08016257\tcg08548498\tcg08715791\tcg09334636\tcg11153071\tcg11441796\tcg11642652\tcg12256803\tcg12352902\tcg12541127\tcg13313833\tcg13500819\tcg13975075\tcg14061946\tcg14086922\tcg14224196\tcg14530143\tcg15456742\tcg16230982\tcg16734549\tcg17166941\tcg17290213\tcg17292667\tcg18266594\tcg18335535\tcg18584803\tcg19273773\tcg19378199\tcg19523692\tcg20115827\tcg20558024\tcg20608895\tcg20899581\tcg21186299\tcg22115892\tcg22454769\tcg22549547\tcg23098693\tcg23193759\tcg23500537\tcg23606718\tcg24079702\tcg24888989\tcg25090514\tcg25344401\tcg25635000\tcg25726357\tcg25743481\tcg26019498\tcg26647566\tcg26792755\tcg26928195\tcg26940620\tAge\r\n0\t0.252486\t0.284724\t0.243242\t0.200685\t0.904132\t0.102795\t0.473919\t0.264084\t0.367480\t0.671434\t0.075955\t0.329343\t0.217375\t0.210861\t1.000000\t0.356048\t0.577945\t0.557148\t0.249014\t0.847134\t0.254539\t0.319858\t0.220589\t0.796789\t0.361994\t0.296101\t0.105965\t0.239796\t0.169738\t0.357586\t0.365674\t0.132575\t0.250932\t0.283227\t1.000000\t0.262259\t0.208146\t0.290623\t0.113049\t0.255710\t0.555382\t0.281046\t0.168826\t0.492007\t0.442871\t0.509569\t0.219183\t0.641244\t0.339088\t0.164062\t0.227678\t0.340220\t0.541491\t0.423010\t0.621303\t0.243750\t0.869947\t0.124120\t0.317660\t0.985243\t0.645869\t0.590888\t0.841485\t0.825372\t0.904037\t0.407343\t0.223722\t0.352113\t0.855653\t0.289593\t0.428849\t0.719758\t0.800240\t0.473586\t68\r\n1\t0.867671\t0.606590\t0.803673\t0.845942\t0.086222\t0.996915\t0.871998\t0.791823\t0.877639\t0.095326\t0.857108\t0.959701\t0.688322\t0.650640\t0.062329\t0.920434\t0.687537\t0.193038\t0.891809\t0.273775\t0.583457\t0.793486\t0.798427\t0.102910\t0.773496\t0.658568\t0.759050\t0.754877\t0.787817\t0.585895\t0.792240\t0.734543\t0.854528\t0.735642\t0.389495\t0.736709\t0.600386\t0.775989\t0.819579\t0.696350\t0.110374\t0.878199\t0.659849\t0.716714\t0.771206\t0.870711\t0.919629\t0.359592\t0.677752\t0.693433\t0.683448\t0.792423\t0.933971\t0.170669\t0.249908\t0.879879\t0.111498\t0.623053\t0.626821\t0.000000\t0.157429\t0.197567\t0.160809\t0.183031\t0.202754\t0.597896\t0.826429\t0.886736\t0.086038\t0.844088\t0.761793\t0.056548\t0.270670\t0.940083\t21\r\n2\t0.789439\t0.594060\t0.857086\t0.633195\t0.000000\t0.953293\t0.832107\t0.692119\t0.641294\t0.169303\t0.935807\t0.674698\t0.789146\t0.796555\t0.208590\t0.791318\t0.777537\t0.221895\t0.804405\t0.138006\t0.738616\t0.758083\t0.749127\t0.180998\t0.769312\t0.592938\t0.578885\t0.896125\t0.553588\t0.781393\t0.898768\t0.705339\t0.861029\t0.966552\t0.274496\t0.575738\t0.490313\t0.951172\t0.833724\t0.901890\t0.115235\t0.651489\t0.619196\t0.760758\t0.902768\t0.835082\t0.610065\t0.294962\t0.907979\t0.703284\t0.775867\t0.910324\t0.858090\t0.190595\t0.041909\t0.792941\t0.146005\t0.615639\t0.761822\t0.254161\t0.101765\t0.343289\t0.356166\t0.088915\t0.114347\t0.628616\t0.697758\t0.910687\t0.133282\t0.775737\t0.809420\t0.129848\t0.126485\t0.875580\t20\r\n3\t0.615803\t0.710968\t0.874037\t0.771136\t0.199428\t0.861378\t0.861346\t0.695713\t0.638599\t0.158479\t0.903668\t0.758718\t0.581146\t0.857357\t0.307756\t0.977337\t0.805049\t0.188333\t0.788991\t0.312119\t0.706578\t0.782006\t0.793232\t0.288111\t0.691131\t0.758102\t0.829221\t1.000000\t0.742666\t0.897607\t0.797869\t0.803221\t0.912101\t0.736800\t0.315636\t0.760577\t0.609101\t0.733923\t0.578598\t0.796944\t0.096960\t0.924135\t0.612601\t0.727117\t0.905177\t0.776481\t0.727865\t0.429820\t0.666803\t0.924595\t0.567474\t0.752196\t0.742709\t0.303662\t0.168286\t0.720899\t0.099313\t0.595328\t0.734024\t0.268583\t0.293437\t0.244840\t0.311726\t0.213415\t0.418673\t0.819981\t0.816660\t0.684730\t0.146797\t0.686270\t0.777680\t0.087826\t0.335125\t1.000000\t23\r\n4\t0.847329\t0.735766\t0.858018\t0.896453\t0.186994\t0.831964\t0.762522\t0.840186\t0.830930\t0.199264\t0.788487\t0.912629\t0.702284\t0.838771\t0.065271\t0.959230\t0.912387\t0.377203\t0.794480\t0.207909\t0.766246\t0.582117\t0.902944\t0.301144\t0.765401\t0.715115\t0.646735\t0.812084\t0.697886\t0.714310\t0.890658\t0.826644\t0.944022\t0.729517\t0.530379\t0.756268\t0.764899\t0.914573\t0.825766\t0.673394\t0.017316\t0.949335\t0.614375\t0.650553\t0.898788\t0.685396\t0.823348\t0.210175\t0.831852\t0.829067\t0.858212\t0.916433\t0.778864\t0.241186\t0.144072\t0.889536\t0.058360\t0.703567\t0.852496\t0.094223\t0.341236\t0.284903\t0.231957\t0.125196\t0.333207\t0.752592\t0.899356\t0.839006\t0.174601\t0.937948\t0.716135\t0.000000\t0.114062\t0.969760\t22\r\n```\r\n\r\nI split the data into train/test/val data like this:\r\n```\r\ntrain_df, rest_df = train_test_split(df, test_size=0.4)\r\ntest_df, val_df = train_test_split(rest_df, test_size=0.5)\r\n\r\nx_train_tensor = torch.tensor(train_df.drop(&#39;Age&#39;, axis=1).to_numpy(), requires_grad=True)\r\ny_train_tensor = torch.tensor(train_df[&#39;Age&#39;].to_numpy())\r\n\r\nx_test_tensor = torch.tensor(test_df.drop(&#39;Age&#39;, axis=1).to_numpy(), requires_grad=True)\r\ny_test_tensor = torch.tensor(test_df[&#39;Age&#39;].to_numpy())\r\n\r\nx_val_tensor = torch.tensor(val_df.drop(&#39;Age&#39;, axis=1).to_numpy(), requires_grad=True)\r\ny_val_tensor = torch.tensor(val_df[&#39;Age&#39;].to_numpy())\r\n\r\nbs = len(train_df.index)//10\r\ntrain_dl = DataLoader(train_df, bs, shuffle=True)\r\ntest_dl = DataLoader(test_df, len(test_df), shuffle=False)\r\nval_dl = DataLoader(val_df, bs, shuffle=False)\r\n```\r\n\r\nAnd here is the Network so far (very basic, just to test if it works):\r\n```\r\nclass Net(nn.Module):\r\n    def __init__(self):\r\n        super().__init__()\r\n        input_size = len(df.columns)-1\r\n        self.fc1 = nn.Linear(input_size, input_size//2)\r\n        self.fc2 = nn.Linear(input_size//2, input_size//4)\r\n        self.fc3 = nn.Linear(input_size//4, 1)\r\n\r\n    def forward(self, x):\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = F.relu(self.fc3(x))\r\n\r\n        return x\r\n\r\nnet = Net()\r\nprint(net)\r\n```\r\n\r\nHere is where I get the error, on the last line:\r\n```\r\nloss = torch.nn.MSELoss()\r\noptimizer = optim.Adam(net.parameters(), lr=0.001)\r\n\r\nEPOCHS = 3\r\nSTEPS_PER_EPOCH = len(train_dl.dataset)//bs\r\niterator = iter(train_dl)\r\nprint(train_dl.dataset)\r\nfor epoch in range(EPOCHS):\r\n    for s in range(STEPS_PER_EPOCH):\r\n        print(iterator)\r\n        iterator.next()\r\n```\r\n```\r\nID_REF  cg00001854  cg00270460  cg00293191  ...  cg26928195  cg26940620  Age\r\n29        0.781979    0.744825    0.744579  ...    0.242138    0.854054   19\r\n44        0.185400    0.299145    0.160084  ...    0.638449    0.413286   69\r\n21        0.085470    0.217421    0.277675  ...    0.863455    0.512334   75\r\n4         0.847329    0.735766    0.858018  ...    0.114062    0.969760   22\r\n20        0.457293    0.462984    0.323835  ...    0.584259    0.481060   68\r\n33        0.784562    0.845031    0.958335  ...    0.122210    0.854005   19\r\n25        0.258434    0.354822    0.405620  ...    0.677245    0.540463   70\r\n27        0.737131    0.768188    0.897724  ...    0.203228    0.831175   20\r\n37        0.002051    0.202403    0.134198  ...    0.753844    0.302229   70\r\n10        0.737427    0.537413    0.614343  ...    0.464244    0.723953   23\r\n0         0.252486    0.284724    0.243242  ...    0.800240    0.473586   68\r\n32        0.927260    1.000000    0.853864  ...    0.261990    0.892503   18\r\n7         0.035825    0.271602    0.236109  ...    1.000000    0.471256   69\r\n17        0.000000    0.202986    0.132144  ...    0.874550    0.342981   79\r\n18        0.282112    0.479775    0.218852  ...    0.908217    0.426143   79\r\n11        0.708797    0.536074    0.721171  ...    0.048768    0.699540   27\r\n15        0.686921    0.639198    0.858981  ...    0.305142    0.978350   24\r\n38        0.246031    0.186011    0.235928  ...    0.754013    0.342380   70\r\n30        0.814767    0.771483    0.437789  ...    0.000000    0.658354   18\r\n43        0.247471    0.399231    0.271619  ...    0.895016    0.468336   72\r\n46        0.000428    0.263164    0.163303  ...    0.567005    0.252806   76\r\n3         0.615803    0.710968    0.874037  ...    0.335125    1.000000   23\r\n5         0.777925    0.821814    0.636676  ...    0.233359    0.753266   20\r\n34        0.316262    0.307535    0.203090  ...    0.570755    0.351226   73\r\n23        0.133038    0.000000    0.208442  ...    0.631202    0.459593   76\r\n6         0.746102    0.585211    0.626580  ...    0.311914    0.753994   25\r\n1         0.867671    0.606590    0.803673  ...    0.270670    0.940083   21\r\n47        0.444606    0.502357    0.207560  ...    0.987106    0.446959   71\r\n\r\n[28 rows x 75 columns]\r\n&lt;torch.utils.data.dataloader._SingleProcessDataLoaderIter object at 0x7f166241c048&gt;\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\r\n   2645             try:\r\n-&gt; 2646                 return self._engine.get_loc(key)\r\n   2647             except KeyError:\r\n\r\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\r\n\r\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\r\n\r\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\nKeyError: 13\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nKeyError                                  Traceback (most recent call last)\r\n6 frames\r\n/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)\r\n   2646                 return self._engine.get_loc(key)\r\n   2647             except KeyError:\r\n-&gt; 2648                 return self._engine.get_loc(self._maybe_cast_indexer(key))\r\n   2649         indexer = self.get_indexer([key], method=method, tolerance=tolerance)\r\n   2650         if indexer.ndim &gt; 1 or indexer.size &gt; 1:\r\n\r\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\r\n\r\npandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()\r\n\r\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\npandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()\r\n\r\nKeyError: 13\r\n```\r\n\r\nI really have no idea what the error means or where to look.\r\nI&#39;d greatly appreciate some guidance, thank you!\r\n",
        "accepted_answer_markdown": "Use `Numpy` array instead of `dataframe`. You can use `to_numpy()` to convert dataframe to numpy array.\r\n\r\n```python\r\ntrain_dl = DataLoader(train_df.to_numpy(), bs, shuffle=True)\r\ntest_dl = DataLoader(test_df.to_numpy(), len(test_df), shuffle=False)\r\nval_dl = DataLoader(val_df.to_numpy(), bs, shuffle=False)\r\n```"
    },
    {
        "question_id": "43779500",
        "accepted_answer_id": "43779600",
        "question_title": "pytorch Network.parameters() missing 1 required positional argument: &#39;self&#39;",
        "question_markdown": "There&#39;s a problem when I call Network.parameters() in pytorch in this line in my main function:\r\noptimizer = optim.SGD(Network.parameters(), lr=0.001, momentum=0.9)\r\n\r\nI get the error code:\r\n\r\n`TypeError: parameters() missing 1 required positional argument: &#39;self&#39;`\r\n\r\n\r\n\r\nMy network is defined in this class\r\n\r\n    class Network(nn.Module):\r\n    def __init__(self):\r\n        super(Network, self).__init__()\r\n        self.conv1 = nn.Conv2d(1, 32, 5)\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(32, 64, 5)\r\n        self.pool2 = nn.MaxPool2d(2, 2)\r\n        self.conv3 = nn.Conv2d(64, 64, 5)\r\n        self.pool2 = nn.MaxPool2d(2, 2)\r\n        self.fc1 = nn.Linear(64 * 5 * 5, 512)\r\n        self.fc2 = nn.Linear(512, 640)\r\n        self.fc3 = nn.Linear(640, 3756)\r\n\r\n    def forward(self, x):\r\n        x = self.pool(F.relu(self.conv(x)))\r\n        x = self.pool(F.relu(self.conv2(x)))\r\n        x = self.pool(F.relu(self.conv3(x)))\r\n        x = x.view(-1, 64 * 5 * 5)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\n        return x\r\n\r\nPretty sure that I imported all torch modules correctly.\r\nAny ideas of what I&#39;m doing wrong here?\r\n\r\nThank you!\r\n\r\n",
        "accepted_answer_markdown": "When doing `Network.parameters()` you are calling the **static method** `parameters`.\r\n\r\nBut, `parameters` is an **instance method**.\r\n\r\nSo you have to instansiate `Network` before calling `parameters`.\r\n\r\n    network = Network()\r\n    optimizer = optim.SGD(network.parameters(), lr=0.001, momentum=0.9)\r\n\r\nOr, if you only needs Network first this particular line:\r\n\r\n    optimizer = optim.SGD(Network().parameters(), lr=0.001, momentum=0.9)"
    },
    {
        "question_id": "44406819",
        "accepted_answer_id": "44409799",
        "question_title": "pytorch custom layer &quot;is not a Module subclass&quot;",
        "question_markdown": "I am new to PyTorch, trying it out after using a different toolkit for a while.\r\n\r\nI would like understand how to program custom layers and functions. And as a simple test, I wrote this:\r\n\r\n    class Testme(nn.Module):         ## it _is_ a sublcass of module ##\r\n        def __init__(self):\r\n            super(Testme, self).__init__()\r\n    \r\n        def forward(self, x):\r\n            return x / t_.max(x)\r\n\r\nwhich is intended to cause the data passing through it to sum to 1. Not actually useful, just at test.\r\n\r\nThen I plug it to the example code from the PyTorch Playground:  \r\n\r\n    def make_layers(cfg, batch_norm=False):\r\n        layers = []\r\n        in_channels = 3\r\n        for i, v in enumerate(cfg):\r\n            if v == &#39;M&#39;:\r\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\r\n            else:\r\n                padding = v[1] if isinstance(v, tuple) else 1\r\n                out_channels = v[0] if isinstance(v, tuple) else v\r\n                conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=padding)\r\n                if batch_norm:\r\n                    layers += [conv2d, nn.BatchNorm2d(out_channels, affine=False), nn.ReLU()]\r\n                else:\r\n                    layers += [conv2d, nn.ReLU()]\r\n                layers += [Testme]                           # here &lt;------------------\r\n                in_channels = out_channels\r\n        return nn.Sequential(*layers)\r\n\r\n\r\nThe result is an error!  \r\n\r\n    TypeError: model.Testme is not a Module subclass\r\n\r\nMaybe this needs to be a Function rather than a Module? Also not clear what the difference is between Function, Module.\r\n\r\nFor example, why does a Function need a `backward()`, even if it is constructed entirely from standard pytorch primitive, whereas a Module does not need this?\r\n",
        "accepted_answer_markdown": "That&#39;s a simple one. You almost got it, but you forgot to actually create an instance of your new class Testme. You need to do this, even if the creation of an instance of a particular class doesn&#39;t take any parameters (as for Testme). But it&#39;s easier to forget than for a convolutional layer, to which you typically pass a lot of arguments.\r\n\r\nChange the line you have indicated to the following and your problem is resolved.\r\n\r\n    layers += [Testme()]\r\n\r\n"
    },
    {
        "question_id": "44631520",
        "accepted_answer_id": "44635614",
        "question_title": "subtraction of scalar from tensor yields &#39;inconsistent tensor size&#39; in pytorch",
        "question_markdown": "I&#39;m using PyTorch and my variables are\r\n\r\n```py\r\n    x = [torch.FloatTensor of size 1x3x32x32]\r\n    mean = Variable containing:\r\n    1.00000e-02 *\r\n      2.0518\r\n    [torch.FloatTensor of size 1]\r\n```\r\n\r\nwhat I want to do is subtract the scalar mean from x by doing\r\n\r\n```py \r\n    x = x - mean\r\n```\r\n\r\nHowever, I&#39;m getting this error:\r\n\r\n```py\r\n    RuntimeError: inconsistent tensor size at /py/conda-\r\n    bld/pytorch_1493670682084/work/torch/lib/TH/generic/THTensorMath.c:831\r\n```\r\n\r\nWhat am I doing wrong?\r\nThanks a lot\r\n",
        "accepted_answer_markdown": "what you are trying only works if mean is truly a scalar, i.e. a `float()` (in this case) and not a `torch.FloatTensor of size 1`. You can either extract a true scalar from `mean` or expand `mean` to the size of `x` in order to perform the subtraction. \r\n\r\nTo extract the `float` from `mean`, do: \r\n\r\n    x = x - mean[0]\r\n\r\nTo expand `mean` to the size of `x`, do: \r\n\r\n    x = x - mean.expand_as(x)\r\n\r\nNote that both of these methods subtract the mean from each element in your tensor. "
    },
    {
        "question_id": "44717100",
        "accepted_answer_id": "44719369",
        "question_title": "Pytorch: Convert FloatTensor into DoubleTensor",
        "question_markdown": "I have 2 numpy arrays, which I convert into tensors to use the TensorDataset object.  \r\n\r\n    import torch.utils.data as data_utils\r\n\r\n    X = np.zeros((100,30))\r\n    Y = np.zeros((100,30))\r\n\r\n    train = data_utils.TensorDataset(torch.from_numpy(X).double(), torch.from_numpy(Y))\r\n    train_loader = data_utils.DataLoader(train, batch_size=50, shuffle=True)\r\n\r\nwhen I do:\r\n\r\n    for batch_idx, (data, target) in enumerate(train_loader):\r\n        data, target = Variable(data), Variable(target)\r\n        optimizer.zero_grad()\r\n        output = model(data)               # error occurs here\r\n\r\nI get the fallowing error:\r\n\r\n&gt; TypeError: addmm_ received an invalid combination of arguments - got (int, int, torch.DoubleTensor, torch.FloatTensor), but expected one of:\r\n [...]  \r\n * (float beta, float alpha, torch.DoubleTensor mat1, torch.DoubleTensor mat2)\r\n      didn&#39;t match because some of the arguments have invalid types: (int, int, torch.DoubleTensor, torch.FloatTensor)  \r\n * (float beta, float alpha, torch.SparseDoubleTensor mat1, torch.DoubleTensor mat2)\r\n      didn&#39;t match because some of the arguments have invalid types: (int, int, torch.DoubleTensor, torch.FloatTensor)\r\n\r\nThe last error comes from:\r\n\r\n&gt; output.addmm_(0, 1, input, weight.t())\r\n\r\nAs you see in my code I tried converting the tensor by using .double() - but this did not work. Why is he casting one array into a FloatTensor object and the other into a DoubleTensor?\r\nAny ideas?\r\n\r\n",
        "accepted_answer_markdown": "Your `numpy` arrays are `64-bit floating point`\tand will be converted to `torch.DoubleTensor` standardly. Now, if you use them with your model, you&#39;ll need to make sure that your model parameters are also `Double`. Or you need to make sure, that your `numpy` arrays are cast as `Float`, because model parameters are standardly cast as `float`.\r\n\r\nHence, do either of the following: \r\n\r\n    data_utils.TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(Y).float())\r\n\r\nor do: \r\n\r\n    model.double()\r\n\r\nDepeding, if you want to cast your model parameters, inputs and targets as `Float` or as `Double`. \r\n\r\n"
    },
    {
        "question_id": "44857373",
        "accepted_answer_id": "44932889",
        "question_title": "Find number of non-zero elements in a tensor along an aixs",
        "question_markdown": "I want to find the number of non-zero elements in a tensor along a particular axis. Is there any PyTorch function which can do this?\r\n\r\nI tried to use the [nonzero()][1] method in PyTorch.\r\n\r\n    torch.nonzero(losses).size(0)\r\n\r\nHere, `lossess` is a tensor of shape `64 x 1`. When I run the above statement, it gives me the following error.\r\n\r\n    TypeError: Type Variable doesn&#39;t implement stateless method nonzero\r\n\r\nBut if I run, `torch.nonzero(losses.data).size(0)`, then it works fine. Any clue, why this is happening or what the error means?\r\n\r\n  [1]: http://pytorch.org/docs/master/torch.html?highlight=nonzero#torch.nonzero",
        "accepted_answer_markdown": "Meaning of the error message - `TypeError: Type Variable doesn&#39;t implement stateless method nonzero` is, we cannot use `torch.nonzero()` on `autograd.Variable` but only on simple tensors. Also it should be noted that, tensors are stateless while the Variables are stateful."
    },
    {
        "question_id": "45022734",
        "accepted_answer_id": "45023288",
        "question_title": "Understanding a simple LSTM pytorch",
        "question_markdown": "    import torch,ipdb\r\n    import torch.autograd as autograd\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.optim as optim\r\n    from torch.autograd import Variable\r\n    \r\n    rnn = nn.LSTM(input_size=10, hidden_size=20, num_layers=2)\r\n    input = Variable(torch.randn(5, 3, 10))\r\n    h0 = Variable(torch.randn(2, 3, 20))\r\n    c0 = Variable(torch.randn(2, 3, 20))\r\n    output, hn = rnn(input, (h0, c0))\r\n\r\nThis is the LSTM example from the docs. I don&#39;t know understand the following things:\r\n\r\n 1. What is output-size and why is it not specified anywhere?\r\n 2. Why does the input have 3 dimensions. What does 5 and 3 represent?\r\n 3. What are 2 and 3 in h0 and c0, what do those represent?\r\n\r\nEdit:\r\n\r\n    import torch,ipdb\r\n    import torch.autograd as autograd\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.optim as optim\r\n    from torch.autograd import Variable\r\n    import torch.nn.functional as F\r\n    \r\n    num_layers=3\r\n    num_hyperparams=4\r\n    batch = 1\r\n    hidden_size = 20\r\n    rnn = nn.LSTM(input_size=num_hyperparams, hidden_size=hidden_size, num_layers=num_layers)\r\n    \r\n    input = Variable(torch.randn(1, batch, num_hyperparams)) # (seq_len, batch, input_size)\r\n    h0 = Variable(torch.randn(num_layers, batch, hidden_size)) # (num_layers, batch, hidden_size)\r\n    c0 = Variable(torch.randn(num_layers, batch, hidden_size))\r\n    output, hn = rnn(input, (h0, c0))\r\n    affine1 = nn.Linear(hidden_size, num_hyperparams)\r\n    \r\n    ipdb.set_trace()\r\n    print output.size()\r\n    print h0.size()\r\n\r\n\r\n&gt; *** RuntimeError: matrices expected, got 3D, 2D tensors at",
        "accepted_answer_markdown": "The output for the LSTM is the output for all the hidden nodes on the final layer.  \r\n`hidden_size` - the number of LSTM blocks per layer.  \r\n`input_size` - the number of input features per time-step.  \r\n`num_layers` - the number of hidden layers.  \r\nIn total there are `hidden_size * num_layers` LSTM blocks.\r\n\r\nThe input dimensions are `(seq_len, batch, input_size)`.  \r\n`seq_len` - the number of time steps in each input stream.  \r\n`batch` - the size of each batch of input sequences.\r\n\r\nThe hidden and cell dimensions are: `(num_layers, batch, hidden_size)`\r\n\r\n&gt; **output** (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t.\r\n\r\nSo there will be `hidden_size * num_directions` outputs. You didn&#39;t initialise the RNN to be bidirectional so `num_directions` is 1. So `output_size = hidden_size`.\r\n\r\n**Edit**: You can change the number of outputs by using a linear layer:\r\n\r\n    out_rnn, hn = rnn(input, (h0, c0))\r\n    lin = nn.Linear(hidden_size, output_size)\r\n    v1 = nn.View(seq_len*batch, hidden_size)\r\n    v2 = nn.View(seq_len, batch, output_size)\r\n    output = v2(lin(v1(out_rnn)))\r\n\r\n**Note**: for this answer I assumed that we&#39;re only talking about non-bidirectional LSTMs.\r\n\r\nSource: [PyTorch docs][1].\r\n\r\n  [1]: http://pytorch.org/docs/master/nn.html#torch.nn.LSTM"
    },
    {
        "question_id": "45446983",
        "accepted_answer_id": "45461669",
        "question_title": "Tensor type mismatch when moving to GPU",
        "question_markdown": "I&#39;m getting the following error when trying to move my network and tensors to GPU. I&#39;ve checked that the network parameters are moved to the GPU and check each batch&#39;s tensor and move them if they&#39;re not already on the GPU. But I&#39;m still getting this issue say that there&#39;s a mismatch in the tensor types - one is a `torch.cuda.FloatTensor` and the other is a `torch.FloatTensor`? Could someone tell me what I&#39;m doing wrong? Thanks.\r\n\r\nMy code:\r\n\r\n    class Train():\r\n      def __init__(self, network, training, address):\r\n\t\tself.network    = network\r\n\t\tself.address    = address\r\n\t\tself.batch_size = training[&#39;batch_size&#39;]\r\n\t\tself.iterations = training[&#39;iterations&#39;]\r\n\t\tself.samples \t= training[&#39;samples&#39;]\r\n\t\tself.data       = training[&#39;data&#39;]\r\n\t\tself.lr \t    = training[&#39;lr&#39;]\r\n\t\tself.noisy_lr \t= training[&#39;nlr&#39;]\r\n\t\tself.cuda       = training[&#39;cuda&#39;]\r\n\t\tself.save       = training[&#39;save&#39;]\r\n\t\tself.scale\t    = training[&#39;scale&#39;]\r\n\t\tself.limit      = training[&#39;limit&#39;]\r\n\t\tself.replace    = training[&#39;strategy&#39;]\r\n\t\tself.optimizer  = torch.optim.Adam(self.network.parameters(), lr=self.lr)\r\n\r\n      def tensor_to_Variable(self, t):\r\n    \tif next(self.network.parameters()).is_cuda and not t.is_cuda:\r\n    \t\tt = t.cuda()\r\n    \r\n    \treturn Variable(t)\r\n    \r\n      def train(self):\r\n        if self.cuda:\r\n\t\t\tself.network.cuda()\r\n        dh = DataHandler(self.data)\r\n\t\tloss_fn = torch.nn.MSELoss()\r\n\t\tlosses    = []\r\n\t\tvalidate  = []\r\n\t\tval_size  = 100\r\n\t\tval_diff  = 1\r\n\t\ttotal_val = float(val_size * self.batch_size)\r\n\t\thypos     = []\r\n\t\tlabels    = []\r\n\r\n    \t# training loop\r\n    \tfor i in range(self.iterations):\r\n    \t\tx, y = dh.get_batch(self.batch_size)\r\n    \t\tx = self.tensor_to_Variable(x)\r\n    \t\ty = self.tensor_to_Variable(y)\r\n    \t\t\r\n    \t\tself.optimizer.zero_grad()\r\n    \t\thypo = self.network(x)\r\n    \t\tloss = loss_fn(hypo, y)\r\n    \t\tloss.backward()\r\n    \t\tself.optimizer.step()\r\n    \t\t\r\n    \r\n    class Feedforward(nn.Module):\r\n       def __init__(self, topology):\r\n\t\tsuper(Feedforward, self).__init__()\r\n\t\tself.input_dim     = topology[&#39;features&#39;]\r\n\t\tself.num_hidden    = topology[&#39;hidden_layers&#39;]\r\n\t\tself.hidden_dim    = topology[&#39;hidden_dim&#39;]\r\n\t\tself.output_dim    = topology[&#39;output_dim&#39;]\r\n\t\tself.input_layer   = nn.Linear(self.input_dim, self.hidden_dim)\r\n\t\tself.hidden_layer  = nn.Linear(self.hidden_dim, self.hidden_dim)\r\n\t\tself.output_layer  = nn.Linear(self.hidden_dim, self.output_dim)\r\n\t\tself.dropout_layer = nn.Dropout(p=0.2)\r\n\r\n    \r\n    def forward(self, x):\r\n    \tbatch_size = x.size()[0]\r\n    \tfeat_size  = x.size()[1]\r\n    \tinput_size = batch_size * feat_size\r\n    \r\n    \tself.input_layer = nn.Linear(input_size, self.hidden_dim)\r\n    \thidden = self.input_layer(x.view(1, input_size)).clamp(min=0)\r\n    \r\n    \tfor _ in range(self.num_hidden):\r\n    \t\thidden = self.dropout_layer(F.relu(self.hidden_layer(hidden)))\r\n    \t\t\r\n    \toutput_size = batch_size * self.output_dim\r\n    \tself.output_layer = nn.Linear(self.hidden_dim, output_size)\r\n    \treturn self.output_layer(hidden).view(output_size)\r\n\r\nThe error:\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;/media/project/train.py&quot;, line 78, in train\r\n        hypo = self.network(x)\r\n\r\n     * (torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (float beta, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (float beta, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n     * (float beta, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n          didn&#39;t match because some of the arguments have invalid types: (int, int, torch.cuda.FloatTensor, torch.FloatTensor)\r\n     * (float beta, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n          didn&#39;t match because some of the arguments have invalid types: (int, int, torch.cuda.FloatTensor, torch.FloatTensor)\r\n    \r\n\r\nStacktrace:\r\n\r\n    Traceback (most recent call last):\r\n    File &quot;smpl.py&quot;, line 90, in &lt;module&gt;\r\n    main()\r\n    File &quot;smpl.py&quot;, line 80, in main\r\n    trainer.train()\r\n    File &quot;/media/mpl/temp/train.py&quot;, line 82, in train\r\n    hypo = self.network(x)\r\n    File &quot;/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py&quot;, line 206, in call\r\n    result = self.forward(input, **kwargs)\r\n    File &quot;model/network.py&quot;, line 35, in forward\r\n    hidden = self.input_layer(x.view(1, input_size)).clamp(min=0)\r\n    File &quot;/usr/local/lib/python2.7/dist-packages/torch/nn/modules/module.py&quot;, line 206, in call\r\n    result = self.forward(input, *kwargs)\r\n    File &quot;/usr/local/lib/python2.7/dist-packages/torch/nn/modules/linear.py&quot;, line 54, in forward\r\n    return self.backend.Linear()(input, self.weight, self.bias)\r\n    File &quot;/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/linear.py&quot;, line 10, in forward\r\n    output.addmm(0, 1, input, weight.t())\r\n    TypeError: addmm_ received an invalid combination of arguments - got (int, int, torch.cuda.FloatTensor, torch.FloatTensor), but expected one of: (torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n    (torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2) (float beta, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n    (float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2) (float beta, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n    (float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2) (float beta, float alpha, torch.cuda.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n    didn&#39;t match because some of the arguments have invalid types: (int, int, torch.cuda.FloatTensor, torch.FloatTensor)\r\n    * (float beta, float alpha, torch.cuda.sparse.FloatTensor mat1, torch.cuda.FloatTensor mat2)\r\n    didn&#39;t match because some of the arguments have invalid types: (int, int, torch.cuda.FloatTensor, torch.FloatTensor\r\n\r\n",
        "accepted_answer_markdown": "This is happening because you are **re-initializing** `self.input_layer` in your `forward()` function. \r\n\r\nThe call `self.network.cuda()` moves all of the model parameters into cuda. Which means any and all the layers you initialize at the creation of your `FeedForward` object will be moved to cuda memory. But when you **reinitialize** `self.input_layer` in your `forward()` function, you initialize that layer&#39;s parameters in cpu and not gpu. Same goes for `self.output_layer`.\r\n"
    },
    {
        "question_id": "53586245",
        "accepted_answer_id": "53593936",
        "question_title": "How to connect the input to the output directly using single fully connected layer in PyTorch?",
        "question_markdown": "I am new to deep learning and cnn and trying to get familiar with that field using CIFAR10 tutorial code from PyTorch website. So, in that code I was playing with removing/adding layers to better understand the effect of them and I tried to connect the input(which is the initial data with the batch of 4 images) to the output directly with using only single fully connected layer. I know that does not make much sense, but I do it only for the sake of experiment. So, when I tried to do it, I faced with some errors, which are as follows:\r\n\r\nFirst, here is the code snippet:\r\n\r\n    ########################################################################\r\n    # 2. Define a Convolution Neural Network\r\n    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    # Copy the neural network from the Neural Networks section before and modify it to\r\n    # take 3-channel images (instead of 1-channel images as it was defined).\r\n    \r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    \r\n    \r\n    class Net(nn.Module):\r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            #self.conv1 = nn.Conv2d(3, 6, 5)\r\n            #self.pool = nn.MaxPool2d(2, 2)\r\n            #self.conv2 = nn.Conv2d(6, 16, 5)\r\n            #self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n            #self.fc2 = nn.Linear(120, 84)\r\n            self.fc3 = nn.Linear(768 * 4 * 4, 10)\r\n    \r\n        def forward(self, x):\r\n            #x = self.pool(F.relu(self.conv1(x)))\r\n            #x = self.pool(F.relu(self.conv2(x)))\r\n            x = x.view(-1, 768 * 4 * 4)\r\n            #x = F.relu(self.fc1(x))\r\n            #x = F.relu(self.fc2(x))\r\n            x = self.fc3(x)\r\n            return x\r\n    \r\n    \r\n    net = Net()\r\n    \r\n    #######################################################################\r\n    # 3. Define a Loss function and optimizer\r\n    # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    # Let&#39;s use a Classification Cross-Entropy loss and SGD with momentum.\r\n    \r\n    import torch.optim as optim\r\n    \r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\r\n    \r\n    ########################################################################\r\n    # 4. Train the network\r\n    # ^^^^^^^^^^^^^^^^^^^^\r\n    #\r\n    # This is when things start to get interesting.\r\n    # We simply have to loop over our data iterator, and feed the inputs to the\r\n    # network and optimize.\r\n    \r\n    for epoch in range(4):  # loop over the dataset multiple times\r\n    \r\n        running_loss = 0.0\r\n        for i, data in enumerate(trainloader, 0):\r\n            # get the inputs\r\n            inputs, labels = data\r\n    \r\n            # zero the parameter gradients\r\n            optimizer.zero_grad()\r\n    \r\n            # forward + backward + optimize\r\n            outputs = net(inputs)\r\n            print(len(outputs))\r\n            print(len(labels))\r\n            loss = criterion(outputs, labels)\r\n            loss.backward()\r\n            optimizer.step()\r\n    \r\n            # print statistics\r\n            running_loss += loss.item()\r\n            if i % 2000 == 1999:    # print every 2000 mini-batches\r\n                print(&#39;[%d, %5d] loss: %.3f&#39; %\r\n                      (epoch + 1, i + 1, running_loss / 2000))\r\n                running_loss = 0.0\r\n    \r\n    print(&#39;Finished Training&#39;)\r\n\r\nSo, when I run the code, I get the following error:\r\n\r\n    Traceback (most recent call last):\r\n          File &quot;C:\\Users\\Andrey\\Desktop\\Machine_learning_Danila\\Homework 3\\cifar10_tutorial1.py&quot;, line 180, in &lt;module&gt;\r\n            loss = criterion(outputs, labels)\r\n          File &quot;C:\\Program Files\\Python36\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 477, in __call__\r\n            result = self.forward(*input, **kwargs)\r\n          File &quot;C:\\Program Files\\Python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py&quot;, line 862, in forward\r\n            ignore_index=self.ignore_index, reduction=self.reduction)\r\n          File &quot;C:\\Program Files\\Python36\\lib\\site-packages\\torch\\nn\\functional.py&quot;, line 1550, in cross_entropy\r\n            return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n          File &quot;C:\\Program Files\\Python36\\lib\\site-packages\\torch\\nn\\functional.py&quot;, line 1405, in nll_loss\r\n            .format(input.size(0), target.size(0)))\r\n        ValueError: Expected input batch_size (1) to match target batch_size (4).\r\n\r\nI was trying to check the length of x and it turns out, that it is 4 initially but it becomes 1 after the line  \r\n\r\n `x = x.view(-1, 768 * 4 * 4)`\r\n\r\nI think my numbers are correct, but it seems like I am having only 1 tensor instead of 4 as I supposed to have, and I feel like that is what causes that error.\r\nI am wondering, why is that and what is the best way to fix that?\r\nAlso, what would be the best optimal number for output dimension output in  *nn.Linear*(Fully connected Layer) in this case?",
        "accepted_answer_markdown": "There are two obvious errors in your modified code (from the official ones from PyTorch webpage). First, \r\n\r\n`torch.nn.Linear(in_features, out_features)`\r\n\r\nis the correct syntax. But, you&#39;re passing `768 * 4 * 4` as `in_features`. This is 4 times the actual number of neurons (pixels) in one CIFAR10 image (32*32*3 = 3072).\r\n\r\nThe second bug is related to the first one. When you prepare your `inputs` tensor,\r\n\r\n\r\n    # forward + backward + optimize;\r\n    # `inputs` should be a tensor of shape [batch_size, input_shape]\r\n            outputs = net(inputs)\r\n\r\nyou should pass it as a tensor of shape `[batch_size, input_size]`, which according to your requirement is `[4, 3072]` since you want to use a batch size of 4. This is where you should provide the batch dimension; Not in `nn.Linear` which is what you&#39;re currently doing and that is causing the error.\r\n\r\nFinally, you should also fix the line in `forward` method. Change the below line\r\n\r\n    x = x.view(-1, 768 * 4 * 4)   \r\n\r\nto\r\n\r\n    x = x.view(-1, 32*32*3)\r\n\r\n\r\nFixing these bugs should fix your errors.\r\n\r\n-----------------\r\n\r\nHaving said that I&#39;m unsure whether this would actually work, in a conceptual sense. Because this is a simple linear transformation (i.e. an affine transformation without any non-linearity). The data points (which correspond to images in CIFAR10) would most probably be not linearly separable, in this 3072 dimensional space (manifold). Hence, the accuracy would be drastically poor. Thus it is advisable to add at least a hidden layer with non-linearity such as ReLU.\r\n"
    },
    {
        "question_id": "53639839",
        "accepted_answer_id": "53640315",
        "question_title": "Why does this error pop up while working with Deep Q learning?",
        "question_markdown": "I have been working with Deep Q Learning on Windows 10 Machine. I have version 0.4.1 of pytorch with NVIDA graphics card.\r\n\r\n    def select_action(self, state):\r\n        probs = F.softmax(self.model(Variable(state, volatile = True))*7)\r\n        action = probs.multinomial()\r\n        return action.data[0,0]\r\n\r\n   From this section of the code, I keep getting this error:\r\n\r\n    TypeError: multinomial() missing 1 required positional arguments: &quot;num_samples&quot;\r\n\r\nIf any other information is needed, It will be very quickly provided. \r\n",
        "accepted_answer_markdown": "Based on the [documentation][1] you didn&#39;t specify the `num_samples` of `multinomial` function to draw your multinomial distribution.\r\n\r\n&gt; **torch.multinomial(input, num_samples, replacement=False, out=None)**&lt;br/&gt;\r\n&gt;&gt;Returns a tensor where each row contains **num_samples**\r\n&gt; indices sampled from the multinomial probability distribution located\r\n&gt; in the corresponding row of tensor **input**.\r\n\r\nChange the code as below:\r\n\r\n    def select_action(self, state):\r\n        probs = F.softmax(self.model(Variable(state, volatile = True))*7)\r\n        action = probs.multinomial(1) # 1 is the number of samples to draw\r\n        return action.data[0,0]\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/torch.html"
    },
    {
        "question_id": "53691156",
        "accepted_answer_id": "53692802",
        "question_title": "can&#39;t find the inplace operation: one of the variables needed for gradient computation has been modified by an inplace operation",
        "question_markdown": "I am trying to compute a loss on the jacobian of the network (i.e. to perform double backprop), and I get the following error:\r\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\r\n\r\nI can&#39;t find the inplace operation in my code, so I don&#39;t know which line to fix.\r\n\r\n*The error occurs in the last line:\r\n\r\n    loss3.backward()\r\n\r\n\r\n---\r\n\r\n\r\n    inputs_reg = Variable(data, requires_grad=True)\r\n    output_reg = self.model.forward(inputs_reg)\r\n\r\n    num_classes = output.size()[1]\r\n    jacobian_list = []\r\n    grad_output = torch.zeros(*output_reg.size())\r\n\r\n    if inputs_reg.is_cuda:\r\n        grad_output = grad_output.cuda()\r\n        jacobian_list = jacobian.cuda()\r\n\r\n    for i in range(10):\r\n\r\n        zero_gradients(inputs_reg)\r\n        grad_output.zero_()\r\n        grad_output[:, i] = 1\r\n        jacobian_list.append(torch.autograd.grad(outputs=output_reg,\r\n                                          inputs=inputs_reg,\r\n                                          grad_outputs=grad_output,\r\n                                          only_inputs=True,\r\n                                          retain_graph=True,\r\n                                          create_graph=True)[0])\r\n\r\n\r\n    jacobian = torch.stack(jacobian_list, dim=0)\r\n    loss3 = jacobian.norm()\r\n    loss3.backward()\r\n\r\n",
        "accepted_answer_markdown": "`grad_output.zero_()` is in-place and so is `grad_output[:, i-1] = 0`. In-place means &quot;modify a tensor instead of returning a new one, which has the modifications applied&quot;. An example solution which is not in-place is [`torch.where`](https://pytorch.org/docs/master/torch.html#torch.where). An example use to zero out the 1st column\r\n\r\n    import torch\r\n    t = torch.randn(3, 3)\r\n    ixs = torch.arange(3, dtype=torch.int64)\r\n    zeroed = torch.where(ixs[None, :] == 1, torch.tensor(0.), t)\r\n\r\n    zeroed\r\n    tensor([[-0.6616,  0.0000,  0.7329],\r\n            [ 0.8961,  0.0000, -0.1978],\r\n            [ 0.0798,  0.0000, -1.2041]])\r\n\r\n    t\r\n    tensor([[-0.6616, -1.6422,  0.7329],\r\n            [ 0.8961, -0.9623, -0.1978],\r\n            [ 0.0798, -0.7733, -1.2041]])\r\n\r\nNotice how `t` retains the values it had before and `zeroed` has the values you want."
    },
    {
        "question_id": "53810497",
        "accepted_answer_id": "53813456",
        "question_title": "IndexError when iterating my dataset using Dataloader in PyTorch",
        "question_markdown": "I iterated my dataset using Dataloader in PyTorch 0.2 like these:\r\n \r\n\r\n    dataloader = torch.utils.data.DataLoader(...)\r\n    data_iter = iter(dataloader)\r\n    data = data_iter.next()\r\n\r\nbut IndexError was raised.\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;main.py&quot;, line 193, in &lt;module&gt;\r\n        data_target = data_target_iter.next()\r\n      File &quot;/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py&quot;, line 201, in __next__\r\n        return self._process_next_batch(batch)\r\n      File &quot;/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py&quot;, line 221, in _process_next_batch\r\n        raise batch.exc_type(batch.exc_msg)\r\n    IndexError: Traceback (most recent call last):\r\n      File &quot;/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py&quot;, line 40, in _worker_loop\r\n        samples = collate_fn([dataset[i] for i in batch_indices])\r\n      File &quot;/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py&quot;, line 40, in &lt;listcomp&gt;\r\n        samples = collate_fn([dataset[i] for i in batch_indices])\r\n      File &quot;/home/asr4/zhuminxian/adversarial/code/dataset/data_loader.py&quot;, line 33, in __getitem__\r\n        return self.X_train[idx], self.y_train[idx]\r\n    IndexError: index 4196 is out of bounds for axis 0 with size 4135\r\n\r\nI am wondering why the index was out of bounds. Is it the bug of Pytorch?\r\n\r\nI tried to run my code again, the same error raised, but at different iteration and with different out-of-bound index.",
        "accepted_answer_markdown": "My guess is that your [`data.Dataset.__len__`][1] was not overloaded properly and in-fact `len(dataloader.dataset)` returns a number larger than `len(self.X_train)`.  \r\nCheck your implementation of the underlying dataset in `&#39;/home/asr4/zhuminxian/adversarial/code/dataset/data_loader.py&#39;`.\r\n\r\n\r\n [1]: https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset"
    },
    {
        "question_id": "53843711",
        "accepted_answer_id": "53848701",
        "question_title": "Pytorch - inference all images and back-propagate batch by batch",
        "question_markdown": "I have a **special** use case that I have to separate inference and back-propagation: I have to inference **all** images and **slice** outputs into batches followed by back-propagating batches by batches. I **don&#39;t** need to update my network&#39;s weights.\r\n\r\nI modified snippets of [cifar10_tutorial][1] into the following to simulate my problem: `j` is a variable to represent the index which returns by my own logic and I want the gradient of some variables.\r\n\r\n&lt;!-- language: python --&gt;\r\n\r\n    for epoch in range(2):  # loop over the dataset multiple times\r\n    \r\n        for i, data in enumerate(trainloader, 0):\r\n            # get the inputs\r\n            inputs, labels = data\r\n            inputs.requires_grad = True\r\n\r\n            # zero the parameter gradients\r\n            optimizer.zero_grad()\r\n    \r\n            # forward + backward + optimize\r\n            outputs = net(inputs)\r\n    \r\n            for j in range(4): # j is given by external logic in my own case\r\n    \r\n                loss = criterion(outputs[j, :].unsqueeze(0), labels[j].unsqueeze(0))\r\n    \r\n                loss.backward()\r\n    \r\n                print(inputs.grad.data[j, :]) # what I really want\r\n\r\nI got the following errors:&lt;br&gt;\r\n\r\n\r\n&gt;RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\r\n\r\nMy questions are:\r\n\r\n 1. According to my understanding, the problem arises because the first back-propagate backwards the whole `outputs` and `outputs[1,:].unsqueeze(0)` was released so second back-propagate failed. Am I right?\r\n\r\n 2. In my case, if I set `retain_graph=True`, will the code run slower and slower according to this [post][2]? \r\n\r\n 3. Is there better way to achieve my goal?\r\n\r\n  [1]: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\r\n  [2]: https://stackoverflow.com/questions/48274929/pytorch-runtimeerror-trying-to-backward-through-the-graph-a-second-time-but",
        "accepted_answer_markdown": "1. Yes you are correct. When you already back-propagated through `outputs` the first time (first iteration), the buffers will be freed and it will fail the following time *(next iteration of your loop)*, because then necessary data for this computation have already been removed.\r\n\r\n2. Yes, the graph grows bigger and bigger, so it could be slower depending on GPU *(or CPU)* usage and your network. I had used this once and it was much slower, however this depends much on your network architecture. But certainly you will need more memory with `retain_graph=True` than without.\r\n\r\n3. Depending on your `outputs` and `labels` shape you should be able to calculate the loss for all your `outputs` and `labels` at once:\r\n\r\n        criterion(outputs, labels)\r\n\r\n   You have to skip the `j`-*loop* then, this would also make your code faster. Maybe you need to reshape (resp. `view`) your data, but this should work fine.\r\n\r\n If you for some reason cannot do that you can manually sum up the loss on a tensor and call `backward` after the loop. This should work fine too, but is slower than the solution above. \r\n\r\n So than your code would look like this:\r\n\r\n        # init loss tensor\r\n        loss = torch.tensor(0.0) # move to GPU if you&#39;re using one\r\n\r\n        for j in range(4):\r\n            # summing up your loss for every j\r\n            loss += criterion(outputs[j, :].unsqueeze(0), labels[j].unsqueeze(0))\r\n            # ...\r\n        # calling backward on the summed loss - getting gradients\r\n        loss.backward()\r\n        # as you call backward now only once on the outputs\r\n        # you shouldn&#39;t get any error and you don&#39;t have to use retain_graph=True\r\n\r\n***Edit:***\r\n\r\nThe accumulation of the losses and calling later backward is completely equivalent, here is a small example with and without accumulating the losses:\r\n\r\nFirst creating some data `data`:\r\n\r\n    # w in this case will represent a very simple model\r\n    # I leave out the CE and just use w to map the output to a scalar value\r\n    w = torch.nn.Linear(4, 1)\r\n    data = [torch.rand(1, 4) for j in range(4)]\r\n\r\n`data` looks like:\r\n\r\n    [tensor([[0.4593, 0.3410, 0.1009, 0.9787]]),\r\n     tensor([[0.1128, 0.0678, 0.9341, 0.3584]]),\r\n     tensor([[0.7076, 0.9282, 0.0573, 0.6657]]),\r\n     tensor([[0.0960, 0.1055, 0.6877, 0.0406]])]\r\n\r\nLet&#39;s first do like you&#39;re doing it, calling backward for every iteration `j` separately:\r\n\r\n    # code for directly applying backward\r\n    # zero the weights layer w\r\n    w.zero_grad()\r\n    for j, inp in enumerate(data):\r\n        # activate grad flag\r\n        inp.requires_grad = True\r\n        # remove / zero previous gradients for inputs\r\n        inp.grad = None\r\n        # apply model (only consists of one layer in our case)\r\n        loss = w(inp)\r\n        # calling backward on every output separately\r\n        loss.backward()\r\n        # print out grad\r\n        print(&#39;Input:&#39;, inp)\r\n        print(&#39;Grad:&#39;, inp.grad)\r\n        print()\r\n    print(&#39;w.weight.grad:&#39;, w.weight.grad)\r\n\r\nHere is the print-out with every input and the respective gradient and gradients for the model resp. layer `w` in our simplified case:\r\n\r\n    Input: tensor([[0.4593, 0.3410, 0.1009, 0.9787]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.1128, 0.0678, 0.9341, 0.3584]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.7076, 0.9282, 0.0573, 0.6657]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.0960, 0.1055, 0.6877, 0.0406]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    w.weight.grad: tensor([[1.3757, 1.4424, 1.7801, 2.0434]])\r\n\r\nNow instead of calling backward once for every iteration *j* we accumulate the values and call `backward` on the sum and compare the results:\r\n\r\n    # init tensor for accumulation\r\n    loss = torch.tensor(0.0)\r\n    # zero layer gradients\r\n    w.zero_grad()\r\n    for j, inp in enumerate(data):\r\n        # activate grad flag\r\n        inp.requires_grad = True\r\n        # remove / zero previous gradients for inputs\r\n        inp.grad = None\r\n        # apply model (only consists of one layer in our case)\r\n        # accumulating values instead of calling backward\r\n        loss += w(inp).squeeze()\r\n    # calling backward on the sum\r\n    loss.backward()\r\n    \r\n    # printing out gradients \r\n    for j, inp in enumerate(data):\r\n        print(&#39;Input:&#39;, inp)\r\n        print(&#39;Grad:&#39;, inp.grad)\r\n        print()\r\n    print(&#39;w.grad:&#39;, w.weight.grad)\r\n\r\nLets take a look at the results:\r\n\r\n    Input: tensor([[0.4593, 0.3410, 0.1009, 0.9787]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.1128, 0.0678, 0.9341, 0.3584]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.7076, 0.9282, 0.0573, 0.6657]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    Input: tensor([[0.0960, 0.1055, 0.6877, 0.0406]], requires_grad=True)\r\n    Grad: tensor([[-0.0999,  0.2665, -0.1506,  0.4214]])\r\n    \r\n    w.grad: tensor([[1.3757, 1.4424, 1.7801, 2.0434]])\r\n\r\nWhen comparing the results we can see that both are the same. &lt;br&gt;This is a very simple example, but nevertheless we can see that calling `backward()` on every single tensor and summing up tensors and then calling `backward()` is equivalent in terms of the resulting gradients for both inputs and weights. \r\n\r\nWhen you use CE for all *j* &#39;s at once as described in ***3.*** you can use the flag `reduction=&#39;sum&#39;` to archive the same behaviour like above with summing up the CE values, default is \u2018mean\u2019, which probably leads to slightly different results. \r\n\r\n"
    },
    {
        "question_id": "53879727",
        "accepted_answer_id": "53881868",
        "question_title": "PyTorch - How to deactivate dropout in evaluation mode",
        "question_markdown": "This is the model I defined it is a simple lstm with 2 fully connect layers.\r\n\r\n    import copy\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.optim as optim\r\n\r\n    class mylstm(nn.Module):\r\n        def __init__(self,input_dim, output_dim, hidden_dim,linear_dim):\r\n            super(mylstm, self).__init__()\r\n            self.hidden_dim=hidden_dim\r\n            self.lstm=nn.LSTMCell(input_dim,self.hidden_dim)\r\n            self.linear1=nn.Linear(hidden_dim,linear_dim)\r\n            self.linear2=nn.Linear(linear_dim,output_dim)\r\n        def forward(self, input):\r\n            out,_=self.lstm(input)\r\n            out=nn.Dropout(p=0.3)(out)\r\n            out=self.linear1(out)\r\n            out=nn.Dropout(p=0.3)(out)\r\n            out=self.linear2(out)\r\n            return out\r\n`x_train` and `x_val` are float dataframe with shape `(4478,30)`, while `y_train` and `y_val` are float df with shape `(4478,10)`  \r\n\r\n        x_train.head()\r\n    Out[271]: \r\n           0       1       2       3    ...        26      27      28      29\r\n    0  1.6110  1.6100  1.6293  1.6370   ...    1.6870  1.6925  1.6950  1.6905\r\n    1  1.6100  1.6293  1.6370  1.6530   ...    1.6925  1.6950  1.6905  1.6960\r\n    2  1.6293  1.6370  1.6530  1.6537   ...    1.6950  1.6905  1.6960  1.6930\r\n    3  1.6370  1.6530  1.6537  1.6620   ...    1.6905  1.6960  1.6930  1.6955\r\n    4  1.6530  1.6537  1.6620  1.6568   ...    1.6960  1.6930  1.6955  1.7040\r\n    \r\n    [5 rows x 30 columns]\r\n    \r\n    x_train.shape\r\n    Out[272]: (4478, 30)\r\nDefine the varible and do one time bp, I can find out the vaildation loss is 1.4941\r\n\r\n\r\n    model=mylstm(30,10,200,100).double()\r\n    from torch import optim\r\n    optimizer=optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\r\n    criterion=nn.L1Loss()\r\n    input_=torch.autograd.Variable(torch.from_numpy(np.array(x_train)))\r\n    target=torch.autograd.Variable(torch.from_numpy(np.array(y_train)))\r\n    input2_=torch.autograd.Variable(torch.from_numpy(np.array(x_val)))\r\n    target2=torch.autograd.Variable(torch.from_numpy(np.array(y_val)))\r\n    optimizer.zero_grad()\r\n    output=model(input_)\r\n    loss=criterion(output,target)\r\n    loss.backward()\r\n    optimizer.step()\r\n    moniter=criterion(model(input2_),target2)\r\n    \r\n    moniter\r\n    Out[274]: tensor(1.4941, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)\r\n\r\nBut I called forward function again I get a different number due to randomness of dropout\r\n\r\n    moniter=criterion(model(input2_),target2)\r\n    moniter\r\n    Out[275]: tensor(1.4943, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)\r\n\r\nwhat should I do that I can eliminate all the dropout in predicting phrase?\r\n\r\nI tried `eval()`: \r\n\r\n    moniter=criterion(model.eval()(input2_),target2)\r\n    moniter\r\n    Out[282]: tensor(1.4942, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)\r\n    \r\n    moniter=criterion(model.eval()(input2_),target2)\r\n    moniter\r\n    Out[283]: tensor(1.4945, dtype=torch.float64, grad_fn=&lt;L1LossBackward&gt;)\r\n\r\nAnd pass an addtional parameter p to control dropout:\r\n\r\n    import copy\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.optim as optim\r\n    class mylstm(nn.Module):\r\n        def __init__(self,input_dim, output_dim, hidden_dim,linear_dim,p):\r\n            super(mylstm, self).__init__()\r\n            self.hidden_dim=hidden_dim\r\n            self.lstm=nn.LSTMCell(input_dim,self.hidden_dim)\r\n            self.linear1=nn.Linear(hidden_dim,linear_dim)\r\n            self.linear2=nn.Linear(linear_dim,output_dim)\r\n        def forward(self, input,p):\r\n            out,_=self.lstm(input)\r\n            out=nn.Dropout(p=p)(out)\r\n            out=self.linear1(out)\r\n            out=nn.Dropout(p=p)(out)\r\n            out=self.linear2(out)\r\n            return out\r\n\r\n    model=mylstm(30,10,200,100,0.3).double()\r\n    \r\n    output=model(input_)\r\n    loss=criterion(output,target)\r\n    loss.backward()\r\n    optimizer.step()\r\n    moniter=criterion(model(input2_,0),target2)\r\n    Traceback (most recent call last):\r\n    \r\n      File &quot;&lt;ipython-input-286-e49b6fac918b&gt;&quot;, line 1, in &lt;module&gt;\r\n        output=model(input_)\r\n    \r\n      File &quot;D:\\Users\\shan xu\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 489, in __call__\r\n        result = self.forward(*input, **kwargs)\r\n    \r\n    TypeError: forward() missing 1 required positional argument: &#39;p&#39;\r\n\r\nBut neither of them worked.",
        "accepted_answer_markdown": "You have to define your `nn.Dropout` layer in your `__init__` and assign it to your model to be responsive for calling `eval()`.\r\n\r\nSo changing your model like this should work for you:\r\n\r\n    class mylstm(nn.Module):\r\n        def __init__(self,input_dim, output_dim, hidden_dim,linear_dim,p):\r\n            super(mylstm, self).__init__()\r\n            self.hidden_dim=hidden_dim\r\n            self.lstm=nn.LSTMCell(input_dim,self.hidden_dim)\r\n            self.linear1=nn.Linear(hidden_dim,linear_dim)\r\n            self.linear2=nn.Linear(linear_dim,output_dim)\r\n\r\n            # define dropout layer in __init__\r\n            self.drop_layer = nn.Dropout(p=p)\r\n        def forward(self, input):\r\n            out,_= self.lstm(input)\r\n\r\n            # apply model dropout, responsive to eval()\r\n            out= self.drop_layer(out)\r\n            out= self.linear1(out)\r\n\r\n            # apply model dropout, responsive to eval()\r\n            out= self.drop_layer(out)\r\n            out= self.linear2(out)\r\n            return out\r\n\r\nIf you change it like this dropout will be inactive as soon as you call `eval()`.\r\n\r\n**NOTE: If you want to continue training afterwards you need to call `train()` on your model to leave evaluation mode.**\r\n\r\n&lt;hr&gt;\r\n\r\n*You can also find a small working example for dropout with `eval()` for evaluation mode here:\r\nhttps://stackoverflow.com/questions/53419474/nn-dropout-vs-f-dropout-pytorch/53452827#53452827*"
    },
    {
        "question_id": "54053256",
        "accepted_answer_id": "54056603",
        "question_title": "Pytorch: AttributeError: cannot assign module before Module.__init__() call even if initialized",
        "question_markdown": "I&#39;m getting the following error:\r\n\r\n&gt; AttributeError: cannot assign module before Module.__init__() call\r\n\r\nI&#39;m trying to create an instance of my class :\r\n\r\n    class ResNetGenerator(nn.Module):\r\n        def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=9, use_dropout=False):\r\n            # super(ResNetGenerator, self).__init__()\r\n            super().__init__()\r\n\r\nI&#39;m calling `super().__init__()` but in vain.\r\n\r\nWhat I am doing wrong here?\r\n-----------------------------\r\nComplete Traceback:\r\n\r\n\r\n    File &quot;train.py&quot;, line 40, in &lt;module&gt;\r\n        model = ColorizationCycleGAN(args)\r\n    File &quot;/path/cycle_gan.py&quot;, line 27, in __init__\r\n        self.G_A2B = ResNetGenerator(input_nc=self.input_nc, output_nc=self.output_nc, n_residual_blocks=9, use_dropout=False)\r\n    File &quot;/path/.local/lib/python3.6/site packages/torch/nn/modules/module.py&quot;, line 544, in __setattr__\r\n        &quot;cannot assign module before Module.__init__() call&quot;)\r\n    AttributeError: cannot assign module before Module.__init__() call",
        "accepted_answer_markdown": "In fact, I realized that i wasn&#39;t calling `super().__init__()` in the main class `ColorizationCycleGAN`. Adding this solved the problem.\r\n\r\nI hope that this answer will have the effect of reminding you to check to call the `super().__init__()` function in all classes that inherits from `nn.Module`."
    },
    {
        "question_id": "54058256",
        "accepted_answer_id": "54058284",
        "question_title": "RuntimeError: Error(s) in loading state_dict for ResNet:",
        "question_markdown": "I am loading my model using the following code.\r\n\r\n    def load_model(checkpoint_path):\r\n      &#39;&#39;&#39;\r\n      Function that loads a checkpoint and rebuilds the model\r\n      &#39;&#39;&#39;\r\n  \r\n      checkpoint = torch.load(checkpoint_path, map_location = &#39;cpu&#39;)\r\n  \r\n      if checkpoint[&#39;architecture&#39;] == &#39;resnet18&#39;:\r\n        model = models.resnet18(pretrained=True)\r\n  \r\n      # Freezing the parameters\r\n        for param in model.parameters():\r\n            param.requires_grad = False\r\n      \r\n      \r\n      else:\r\n        print(&#39;Wrong Architecture!&#39;)\r\n        return None\r\n    \r\n      model.class_to_idx = checkpoint[&#39;class_to_idx&#39;]\r\n  \r\n      classifier = nn.Sequential(OrderedDict([\r\n                                (&#39;fc1&#39;, nn.Linear(512, 1024)),\r\n                                (&#39;relu1&#39;, nn.ReLU()),\r\n                                (&#39;dropout&#39;, nn.Dropout(0.2)),\r\n                                (&#39;fc2&#39;, nn.Linear(1024, 102))\r\n                                ]))\r\n\r\n  \r\n      model.fc = classifier\r\n  \r\n      model.load_state_dict(checkpoint[&#39;state_dict&#39;])\r\n  \r\n      return model\r\n\r\nAnd while running\r\n\r\n    # Load your model to this variable\r\n    model = load_model(&#39;checkpoint.pt&#39;)\r\n\r\nI get the following error,\r\n\r\n&gt; RuntimeError                              Traceback (most recent call\r\n&gt; last)\r\n&gt;     &lt;ipython-input-12-50c11e1820e9&gt; in &lt;module&gt;()\r\n&gt;           1 # Load your model to this variable\r\n&gt;     ----&gt; 2 model = load_model(&#39;checkpoint.pt&#39;)\r\n&gt;           3 \r\n&gt;           4 # If you used something other than 224x224 cropped images, set the \r\n&gt;     correct size here\r\n&gt;           5 image_size = 224\r\n&gt; \r\n&gt;     &lt;ipython-input-11-81aef50793cb&gt; in load_model(checkpoint_path)\r\n&gt;          30   model.fc = classifier\r\n&gt;          31 \r\n&gt;     ---&gt; 32   model.load_state_dict(checkpoint[&#39;state_dict&#39;])\r\n&gt;          33 \r\n&gt;          34   return model\r\n&gt; \r\n&gt;     /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py in \r\n&gt;     load_state_dict(self, state_dict, strict)\r\n&gt;         719         if len(error_msgs) &gt; 0:\r\n&gt;         720             raise RuntimeError(&#39;Error(s) in loading state_dict for \r\n&gt;     {}:\\n\\t{}&#39;.format(\r\n&gt;     --&gt; 721                                self.__class__.__name__, \r\n&gt;     &quot;\\n\\t&quot;.join(error_msgs)))\r\n&gt;         722 \r\n&gt;         723     def parameters(self):\r\n&gt; \r\n&gt;     RuntimeError: Error(s) in loading state_dict for ResNet:\r\n&gt; \t    Unexpected key(s) in state_dict: &quot;bn1.num_batches_tracked&quot;, \r\n&gt;     &quot;layer1.0.bn1.num_batches_tracked&quot;, &quot;layer1.0.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer1.1.bn1.num_batches_tracked&quot;, &quot;layer1.1.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer2.0.bn1.num_batches_tracked&quot;, &quot;layer2.0.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer2.0.downsample.1.num_batches_tracked&quot;, \r\n&gt;     &quot;layer2.1.bn1.num_batches_tracked&quot;, &quot;layer2.1.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer3.0.bn1.num_batches_tracked&quot;, &quot;layer3.0.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer3.0.downsample.1.num_batches_tracked&quot;, \r\n&gt;     &quot;layer3.1.bn1.num_batches_tracked&quot;, &quot;layer3.1.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer4.0.bn1.num_batches_tracked&quot;, &quot;layer4.0.bn2.num_batches_tracked&quot;, \r\n&gt;     &quot;layer4.0.downsample.1.num_batches_tracked&quot;, \r\n&gt;     &quot;layer4.1.bn1.num_batches_tracked&quot;, &quot;layer4.1.bn2.num_batches_tracked&quot;.\r\n\r\n",
        "accepted_answer_markdown": "I was using Pytorch 0.4.1 but Jupyter Notebook which I loaded uses 0.4.0. So I added **strict=False** attribute to **load_state_dict()**.\r\n\r\n    model.load_state_dict(checkpoint[&#39;state_dict&#39;], strict=False)"
    },
    {
        "question_id": "54085357",
        "accepted_answer_id": "54086058",
        "question_title": "Expect FloatTensors but got LongTensors in MNIST-like task",
        "question_markdown": "I am performing a MNIST-like task, the input is 10-class images, and the expected output is the predicted class of the images.\r\n\r\nBut now the `output` is like [-2.3274, -2.2723, ...], which the length is the batch_size. And the `target` is [4., 2., 2., 8., ...] \r\n\r\n`Error message`: RuntimeError: expected object for scalar type Long but got scalar type float for argument #2 &#39;target&#39;\r\n\r\n    class Net(nn.Module):\r\n        ...\r\n        ...\r\n        def forward(self, x):\r\n            ...\r\n            ...\r\n            return F.log_softmax(x, dim = 1)\r\n\r\n\r\n    criterion = torch.nn.NLLLoss()\r\n\r\nCan anyone give me some advice? Thanks.\r\n\r\n",
        "accepted_answer_markdown": "The error you got refers to the *second* (#2) argument of the loss: the `target`.  \r\n[`NLLLoss`][1] expects (for each element) to have a *float* vector of probabilities, and a single *long* (i.e., integer) target per element.  \r\nIn your case, your &quot;target&quot; values are ` [4., 2., 2., 8., ...]` which are of type *float*. you need to convert your target to long:\r\n\r\n    target = target.to(dtype=torch.long)\r\n\r\n [1]: https://pytorch.org/docs/0.4.1/nn.html#torch.nn.NLLLoss"
    },
    {
        "question_id": "54096158",
        "accepted_answer_id": "54238357",
        "question_title": "How to fix &#39;cannot initialize type TensorProto DataType&#39; error while importing torch?",
        "question_markdown": "I installed pytorch using pip3 command for my windows pc without GPU support.\r\nBut when I tried to import torch it is giving an error.\r\n\r\n\r\nAt first, there was a different error saying numpy version not matching and I updated the numpy to the latest version.\r\n\r\n    import torch\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-10-c031d3dd82fc&gt; in &lt;module&gt;()\r\n    ----&gt; 1 import torch\r\n    \r\n    C:\\Users\\iamuraptha\\Anaconda3\\lib\\site-packages\\torch\\__init__.py in &lt;module&gt;()\r\n         82     pass\r\n         83 \r\n    ---&gt; 84 from torch._C import *\r\n         85 \r\n         86 __all__ += [name for name in dir(_C)\r\n    \r\n    RuntimeError: generic_type: cannot initialize type &quot;TensorProtoDataType&quot;: an object with that name is already defined\r\n\r\n\r\n\r\n",
        "accepted_answer_markdown": "I reinstalled anaconda and then created a virtual environment for pytorch.Now everything works fine"
    },
    {
        "question_id": "54109617",
        "accepted_answer_id": "54170758",
        "question_title": "Implementing dropout from scratch",
        "question_markdown": "\r\nThis code attempts to utilize a custom implementation of dropout : \r\n\r\n    %reset -f\r\n    \r\n    import torch\r\n    import torch.nn as nn\r\n    # import torchvision\r\n    # import torchvision.transforms as transforms\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.utils.data as data_utils\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    import torch.nn.functional as F\r\n    \r\n    num_epochs = 1000\r\n    \r\n    number_samples = 10\r\n    \r\n    from sklearn.datasets import make_moons\r\n    from matplotlib import pyplot\r\n    from pandas import DataFrame\r\n    # generate 2d classification dataset\r\n    X, y = make_moons(n_samples=number_samples, noise=0.1)\r\n    # scatter plot, dots colored by class value\r\n    \r\n    x_data = [a for a in enumerate(X)]\r\n    x_data_train = x_data[:int(len(x_data) * .5)]\r\n    x_data_train = [i[1] for i in x_data_train]\r\n    x_data_train\r\n    \r\n    y_data = [y[i[0]] for i in x_data]\r\n    y_data_train = y_data[:int(len(y_data) * .5)]\r\n    y_data_train\r\n    \r\n    x_test = [a[1] for a in x_data[::-1][:int(len(x_data) * .5)]]\r\n    y_test = [a for a in y_data[::-1][:int(len(y_data) * .5)]]\r\n    \r\n    x = torch.tensor(x_data_train).float() # &lt;2&gt;\r\n    print(x)\r\n    \r\n    y = torch.tensor(y_data_train).long()\r\n    print(y)\r\n    \r\n    x_test = torch.tensor(x_test).float()\r\n    print(x_test)\r\n    \r\n    y_test = torch.tensor(y_test).long()\r\n    print(y_test)\r\n    \r\n    class Dropout(nn.Module):\r\n        def __init__(self, p=0.5, inplace=False):\r\n    #         print(p)\r\n            super(Dropout, self).__init__()\r\n            if p &lt; 0 or p &gt; 1:\r\n                raise ValueError(&quot;dropout probability has to be between 0 and 1, &quot;\r\n                                 &quot;but got {}&quot;.format(p))\r\n            self.p = p\r\n            self.inplace = inplace\r\n    \r\n        def forward(self, input):\r\n            print(list(input.shape))\r\n            return np.random.binomial([np.ones((len(input),np.array(list(input.shape))))],1-dropout_percent)[0] * (1.0/(1-self.p))\r\n    \r\n        def __repr__(self):\r\n            inplace_str = &#39;, inplace&#39; if self.inplace else &#39;&#39;\r\n            return self.__class__.__name__ + &#39;(&#39; \\\r\n                + &#39;p=&#39; + str(self.p) \\\r\n                + inplace_str + &#39;)&#39;\r\n        \r\n    class MyLinear(nn.Linear):\r\n        def __init__(self, in_feats, out_feats, drop_p, bias=True):\r\n            super(MyLinear, self).__init__(in_feats, out_feats, bias=bias)\r\n            self.custom_dropout = Dropout(p=drop_p)\r\n    \r\n        def forward(self, input):\r\n            dropout_value = self.custom_dropout(self.weight)\r\n            return F.linear(input, dropout_value, self.bias)\r\n    \r\n    \r\n    \r\n    my_train = data_utils.TensorDataset(x, y)\r\n    train_loader = data_utils.DataLoader(my_train, batch_size=2, shuffle=True)\r\n    \r\n    my_test = data_utils.TensorDataset(x_test, y_test)\r\n    test_loader = data_utils.DataLoader(my_train, batch_size=2, shuffle=True)\r\n    \r\n    # Device configuration\r\n    device = &#39;cpu&#39;\r\n    print(device)\r\n    \r\n    # Hyper-parameters \r\n    input_size = 2\r\n    hidden_size = 100\r\n    num_classes = 2\r\n    \r\n    learning_rate = 0.0001\r\n    \r\n    pred = []\r\n    \r\n    # Fully connected neural network with one hidden layer\r\n    class NeuralNet(nn.Module):\r\n        def __init__(self, input_size, hidden_size, num_classes, p):\r\n            super(NeuralNet, self).__init__()\r\n    #         self.drop_layer = nn.Dropout(p=p)\r\n    #         self.drop_layer = MyLinear()\r\n    #         self.fc1 = MyLinear(input_size, hidden_size, p)\r\n            self.fc1 = MyLinear(input_size, hidden_size , p) \r\n            self.relu = nn.ReLU()\r\n            self.fc2 = nn.Linear(hidden_size, num_classes)  \r\n    \r\n        def forward(self, x):\r\n    #         out = self.drop_layer(x)\r\n            out = self.fc1(x)\r\n            out = self.relu(out)\r\n            out = self.fc2(out)\r\n            return out\r\n    \r\n    model = NeuralNet(input_size, hidden_size, num_classes, p=0.9).to(device)\r\n    \r\n    # Loss and optimizer\r\n    criterion = nn.CrossEntropyLoss()\r\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \r\n    \r\n    # Train the model\r\n    total_step = len(train_loader)\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(train_loader):  \r\n            # Move tensors to the configured device\r\n            images = images.reshape(-1, 2).to(device)\r\n            labels = labels.to(device)\r\n    \r\n            # Forward pass\r\n            outputs = model(images)\r\n            loss = criterion(outputs, labels)\r\n    \r\n            # Backward and optimize\r\n            optimizer.zero_grad()\r\n            loss.backward()\r\n            optimizer.step()\r\n    \r\n        if (epoch) % 100 == 0:\r\n            print (&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39;.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\r\n\r\nCustom dropout is implemented as : \r\n\r\n    class Dropout(nn.Module):\r\n        def __init__(self, p=0.5, inplace=False):\r\n    #         print(p)\r\n            super(Dropout, self).__init__()\r\n            if p &lt; 0 or p &gt; 1:\r\n                raise ValueError(&quot;dropout probability has to be between 0 and 1, &quot;\r\n                                 &quot;but got {}&quot;.format(p))\r\n            self.p = p\r\n            self.inplace = inplace\r\n    \r\n        def forward(self, input):\r\n            print(list(input.shape))\r\n            return np.random.binomial([np.ones((len(input),np.array(list(input.shape))))],1-dropout_percent)[0] * (1.0/(1-self.p))\r\n    \r\n        def __repr__(self):\r\n            inplace_str = &#39;, inplace&#39; if self.inplace else &#39;&#39;\r\n            return self.__class__.__name__ + &#39;(&#39; \\\r\n                + &#39;p=&#39; + str(self.p) \\\r\n                + inplace_str + &#39;)&#39;\r\n        \r\n    class MyLinear(nn.Linear):\r\n        def __init__(self, in_feats, out_feats, drop_p, bias=True):\r\n            super(MyLinear, self).__init__(in_feats, out_feats, bias=bias)\r\n            self.custom_dropout = Dropout(p=drop_p)\r\n    \r\n        def forward(self, input):\r\n            dropout_value = self.custom_dropout(self.weight)\r\n            return F.linear(input, dropout_value, self.bias)\r\n\r\nIt seems I&#39;ve implemented the dropout function incorrectly ? :\r\n\r\n    np.random.binomial([np.ones((len(input),np.array(list(input.shape))))],1-dropout_percent)[0] * (1.0/(1-self.p))\r\n\r\nHow to modify in order to correctly utilize dropout ?\r\n\r\nThese posts were useful in getting to this point : \r\n\r\nHinton&#39;s Dropout in 3 Lines of Python : \r\nhttps://iamtrask.github.io/2015/07/28/dropout/\r\n\r\nMaking a Custom Dropout Function : https://discuss.pytorch.org/t/making-a-custom-dropout-function/14053/2\r\n\r\n\r\n",
        "accepted_answer_markdown": "&gt; It seems I&#39;ve implemented the dropout function incorrectly?\r\n&gt; \r\n&gt;     np.random.binomial([np.ones((len(input),np.array(list(input.shape))))],1 dropout_percent)[0] * (1.0/(1-self.p))\r\n\r\nIn fact, the above implementation is known as **Inverted Dropout**. Inverted Dropout is how Dropout is implemented in practice in the various deep learning frameworks.\r\n\r\n\r\n*What is inverted dropout?*\r\n\r\nBefore jump into the inverted dropout, it can be helpful to see how Dropout works for a single neuron:\r\n\r\n![](https://i.sstatic.net/G8WvT.gif)\r\n\r\nSince during train phase a neuron is kept on with probability `q` (=`1-p`), during the testing phase we have to emulate the behavior of the ensemble of networks used in the training phase. To this end, the authors suggest scaling the activation function by a factor of `q` during the test phase in order to use the expected output produced in the training phase as the single output required in the test phase ([Section 10, Multiplicative Gaussian Noise](http://jmlr.org/papers/volume15/srivastava14a.old/srivastava14a.pdf)). Thus:\r\n\r\n![](https://i.sstatic.net/OnN6y.gif)\r\n\r\n\r\nInverted dropout is a bit different. This approach consists in the scaling of the activations during the training phase, leaving the test phase untouched. The scale factor is the inverse of the keep probability `1/1-p` = `1/q`, thus:\r\n\r\n[![enter image description here][1]][1]\r\n\r\nInverted dropout helps to define the model once and just change a parameter (the keep/drop probability) to run train and test on the same model. Direct Dropout, instead, force you to modify the network during the test phase because if you don\u2019t multiply by `q` the output the neuron will produce values that are higher respect to the one expected by the successive neurons (thus the following neurons can saturate or explode): that\u2019s why Inverted Dropout is the more common implementation.\r\n\r\nReferences:\r\n\r\n - [Dropout Regularization, coursera by Andrew NG](https://www.coursera.org/lecture/deep-neural-network/dropout-regularization-eM33A)\r\n\r\n - [What is inverted dropout?][2]\r\n\r\n - [Dropout: scaling the activation versus inverting the dropout][3]\r\n \r\n - [Analysis of Dropout][4]\r\n\r\n---\r\n\r\n*How implement inverted dropout Pytorch?*\r\n\r\n    class MyDropout(nn.Module):\r\n        def __init__(self, p: float = 0.5):\r\n            super(MyDropout, self).__init__()\r\n            if p &lt; 0 or p &gt; 1:\r\n                raise ValueError(&quot;dropout probability has to be between 0 and 1, &quot; &quot;but got {}&quot;.format(p))\r\n            self.p = p\r\n    \r\n        def forward(self, X):\r\n            if self.training:\r\n                binomial = torch.distributions.binomial.Binomial(probs=1-self.p)\r\n                return X * binomial.sample(X.size()) * (1.0/(1-self.p))\r\n            return X\r\n\r\n*How to implement in Numpy?*\r\n\r\n    import numpy as np\r\n\r\n    pKeep = 0.8\r\n    weights = np.ones([1, 5])\r\n    binary_value = np.random.rand(weights.shape[0], weights.shape[1]) &lt; pKeep\r\n    res = np.multiply(weights, binary_value)\r\n    res /= pKeep  # this line is called inverted dropout technique\r\n    print(res)\r\n\r\n*How to implement in Tensorflow?*\r\n\r\n    import tensorflow as tf\r\n    tf.enable_eager_execution()\r\n    \r\n    weights = tf.ones(shape=[1, 5])\r\n    keep_prob = 0.8\r\n    random_tensor = keep_prob\r\n    random_tensor += tf.random_uniform(weights.shape)\r\n    # 0. if [keep_prob, 1.0) and 1. if [1.0, 1.0 + keep_prob)\r\n    binary_tensor = tf.floor(random_tensor)\r\n    ret = tf.div(weights, keep_prob) * binary_tensor\r\n    print(ret)\r\n\r\n  [1]: https://i.sstatic.net/OKh1Y.gif\r\n  [2]: https://www.quora.com/What-is-inverted-dropout\r\n  [3]: https://stats.stackexchange.com/questions/205932/dropout-scaling-the-activation-versus-inverting-the-dropout\r\n  [4]: https://pgaleone.eu/deep-learning/regularization/2017/01/10/anaysis-of-dropout/\r\n"
    },
    {
        "question_id": "54155969",
        "accepted_answer_id": "54156777",
        "question_title": "pytorch instance tensor not moved to gpu even with explicit cuda() call",
        "question_markdown": "I&#39;m working on a project where the model requires access to a tensor that i declare in the constructor _init_ of the class (im sub-classing torch.nn.Module class)  and then i need to use this tensor in the forward() method via a simple matmul() , the model is sent to gpu via a cuda() call:\r\n\r\n    model = Model()\r\n    model.cuda()\r\n\r\nHowever when i do forward-propagation of a simple input X through:\r\n\r\n    model(X) # or model.forward(X)\r\n\r\nI get \r\n\r\n&gt; RuntimeError: Expected object of type torch.cuda.FloatTensor but found\r\n&gt; type torch.FloatTensor for argument #2 &#39;mat2&#39;\r\n\r\nIndicating that the second argument of matmul(the instance tensor i declared) is on CPU and it was expected on GPU (as the rest of the model and data). \r\n\r\nIn matmul, the tensor is transposed via matrix.t()\r\n\r\nI even tried overriding the cuda() method thorugh:\r\n\r\n    def cuda(self):\r\n        super().cuda()\r\n        self.matrix.cuda()\r\n\r\nThe data is already in the GPU ,meaning the following line of code was already executed:\r\n\r\n    X = X.cuda()\r\n\r\nAlso the error explcitly says argument 2 of matmul which for this case is the tensor(called matrix) not X.",
        "accepted_answer_markdown": "Let&#39;s assume the following:\n\n 1. `X` is moved correctly to the GPU\n 2. The tensor declared in the `Model` class is a simple attribute.\n\n    i.e. Something like the following:\n ```python\n class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.matrix = torch.randn(784, 10)\n        \n    def forward(self, x):\n        return torch.matmul(x, self.matrix)\n ```\n\n&lt;hr&gt;\n\nIf so, your first attempt wouldn&#39;t work because the `nn.Module.cuda()` method only moves all of the `Parameters` and `Buffers` to the GPU.\n\nYou would need to make `Model.matrix` a [`Parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter) instead of regular attribute.\nWrap it in the parameter class.\nSomething like:\n```python\nself.matrix = nn.Parameter(torch.randn(784, 10))\n```\n\nNow, instead of automatically casting to the GPU like above, you tried to manually call the `.cuda()` method on `Model.matrix` within the override.\n\nThis doesn&#39;t work either because of a subtle difference between the [`nn.Module.cuda()`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.cuda) method and the [`torch.Tensor.cuda()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda) method.\n\nWhile `nn.Module.cuda()` moves all the `Parameters` and `Buffers` of the `Module` to GPU and returns itself, `torch.Tensor.cuda()` only returns a **copy** of the tensor on the GPU.\n\nThe original tensor is unaffected.\n\n&lt;hr&gt;\n\nIn summary, either:\n\n 1. Wrap your `matrix` attribute as a `Parameter` or\n 2. Assign the GPU copy back to matrix via:\n ```python\n self.matrix = self.matrix.cuda()\n ```\n In your override.\n\nI would suggest the first."
    },
    {
        "question_id": "54274089",
        "accepted_answer_id": "54275435",
        "question_title": "RuntimeError: PyTorch does not currently provide packages for PyPI",
        "question_markdown": "I&#39;m trying to run  this https://github.com/shariqiqbal2810/MAAC repository and it has a module called `torch`\r\n\r\n    import torch as McLawrence\r\n    from torch.autograd import Variable\r\n\r\nI&#39;m using python version `3.7.1` and I downgraded to `3.6.5` on win10\r\nI tried to use \r\n\r\n    pip install torch\r\n    pip install pytorch\r\n    pip install torchvision\r\nI change `torch` to `pytorch`\r\n\r\nI enter their website https://pytorch.org/ and I tried all suggestion that uses `pip` such as \r\n\r\n    pip3 install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-win_amd64.whl\r\n    pip3 install torchvision\r\n\r\nI read every comment relate to this issue in \r\n\r\n https://github.com/pytorch/pytorch/issues/566\r\n\r\nI read most of the answer here\r\n\r\nhttps://stackoverflow.com/search?q=module+named+torch\r\n\r\nBut I still get the same message:\r\n\r\n    Collecting torch\r\n      Using cached https://files.pythonhosted.org/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n        Complete output from command python setup.py egg_info:\r\n        Traceback (most recent call last):\r\n          File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\r\n          File &quot;C:\\Users\\iibra\\AppData\\Local\\Temp\\pip-install-yupbt_qk\\torch\\setup.py&quot;, line 11, in &lt;module&gt;\r\n            raise RuntimeError(README)\r\n        RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n    \r\n        Please follow the instructions at http://pytorch.org/ to install with miniconda instead.\r\n\r\n    pip install pytorch\r\n    Collecting pytorch\r\n      Using cached https://files.pythonhosted.org/packages/a9/41/4487bc23e3ac4d674943176f5aa309427b011e00607eb98899e9d951f67b/pytorch-0.1.2.tar.gz\r\n    Building wheels for collected packages: pytorch\r\n      Running setup.py bdist_wheel for pytorch ... error\r\n      Complete output from command c:\\users\\iibra\\appdata\\local\\programs\\python\\python36\\python.exe -u -c &quot;import setuptools, tokenize;__file__=&#39;C:\\\\Users\\\\iibra\\\\AppData\\\\Local\\\\Temp\\\\pip-install-7qwe1571\\\\pytorch\\\\setup.py&#39;;f=getattr(tokenize, &#39;open&#39;, open)(__file__);code=f.read().replace(&#39;\\r\\n&#39;, &#39;\\n&#39;);f.close();exec(compile(code, __file__, &#39;exec&#39;))&quot; bdist_wheel -d C:\\Users\\iibra\\AppData\\Local\\Temp\\pip-wheel-_mjh_olk --python-tag cp36:\r\n      Traceback (most recent call last):\r\n        File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\r\n        File &quot;C:\\Users\\iibra\\AppData\\Local\\Temp\\pip-install-7qwe1571\\pytorch\\setup.py&quot;, line 17, in &lt;module&gt;\r\n          raise Exception(message)\r\n      Exception: You should install pytorch from http://pytorch.org\r\n\r\n\r\n\r\n    pip install torchvision\r\n    Collecting torchvision\r\n      Using cached https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl\r\n    Requirement already satisfied: six in c:\\users\\iibra\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from torchvision) (1.11.0)\r\n    Requirement already satisfied: pillow&gt;=4.1.1 in c:\\users\\iibra\\appdata\\local\\programs\\python\\python36\\lib\\site-packages (from torchvision) (5.1.0)\r\n    Collecting torch (from torchvision)\r\n      Using cached https://files.pythonhosted.org/packages/5f/e9/bac4204fe9cb1a002ec6140b47f51affda1655379fe302a1caef421f9846/torch-0.1.2.post1.tar.gz\r\n        Complete output from command python setup.py egg_info:\r\n        Traceback (most recent call last):\r\n          File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\r\n          File &quot;C:\\Users\\iibra\\AppData\\Local\\Temp\\pip-install-k8dl2vhz\\torch\\setup.py&quot;, line 11, in &lt;module&gt;\r\n            raise RuntimeError(README)\r\n        RuntimeError: PyTorch does not currently provide packages for PyPI (see status at https://github.com/pytorch/pytorch/issues/566).\r\n    \r\n        Please follow the instructions at http://pytorch.org/ to install with miniconda instead.",
        "accepted_answer_markdown": "Installing from the PyTorch wheel should have worked. But, the problem turns out to be that `pip` is using the cached pytorch to install it as mentioned on GitHub [here][1].\r\n\r\n    Collecting pytorch\r\n      Using cached https://files.pythonhosted.org/packages...\r\n\r\nEither [removing][2] the pip&#39;s cache from `%LocalAppData%\\pip\\Cache` on Windows or disabling it by using `--no-cache-dir` would solve the issue as follows:\r\n\r\n    pip3 --no-cache-dir install https://download.pytorch.org/whl/cu80/torch-1.0.0-cp36-cp36m-win_amd64.whl\r\n\r\n\r\n  [1]: https://github.com/pytorch/pytorch/issues/566#issuecomment-302721953\r\n  [2]: https://stackoverflow.com/questions/9510474/removing-pips-cache"
    }
]