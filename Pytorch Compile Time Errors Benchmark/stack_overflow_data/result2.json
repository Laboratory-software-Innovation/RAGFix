[
    {
        "question_id": "70074789",
        "accepted_answer_id": "70075758",
        "question_title": "CUDA OOM - But the numbers don&#39;t add upp?",
        "question_markdown": "I am trying to train a model using PyTorch. When beginning model training I get the following error message:\r\n\r\n```RuntimeError: CUDA out of memory. Tried to allocate 5.37 GiB (GPU 0; 7.79 GiB total capacity; 742.54 MiB already allocated; 5.13 GiB free; 792.00 MiB reserved in total by PyTorch)```\r\n\r\nI am wondering why this error is occurring. From the way I see it, I have 7.79 GiB total capacity. The numbers it is stating (742 MiB + 5.13 GiB + 792 MiB) do not add up to be greater than 7.79 GiB. When I check ```nvidia-smi``` I see these processes running\r\n```\r\n|    0   N/A  N/A      1047      G   /usr/lib/xorg/Xorg                168MiB |\r\n|    0   N/A  N/A      5521      G   /usr/lib/xorg/Xorg                363MiB |\r\n|    0   N/A  N/A      5637      G   /usr/bin/gnome-shell              161MiB |\r\n```\r\n\r\nI realize that summing all of these numbers might cut it close (168 + 363 + 161 + 742 + 792 + 5130 = 7356 MiB) but this is still less than the stated capacity of my GPU.\r\n\r\n",
        "accepted_answer_markdown": "This is more of a comment, but worth pointing out.\r\n\r\nThe reason in general is indeed what talonmies commented, but you are summing up the numbers incorrectly. Let&#39;s see what happens when tensors are moved to GPU (I tried this on my PC with RTX2060 with 5.8G usable GPU memory in total):\r\n\r\nLet&#39;s run the following python commands interactively:\r\n```\r\nPython 3.8.10 (default, Sep 28 2021, 16:10:42) \r\n[GCC 9.3.0] on linux\r\nType &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.\r\n&gt;&gt;&gt; import torch\r\n&gt;&gt;&gt; a = torch.zeros(1).cuda()\r\n&gt;&gt;&gt; b = torch.zeros(500000000).cuda()\r\n&gt;&gt;&gt; c = torch.zeros(500000000).cuda()\r\n&gt;&gt;&gt; d = torch.zeros(500000000).cuda()\r\n```\r\n\r\nThe following are the outputs of `watch -n.1 nvidia-smi`:\r\n\r\nRight after `torch` import:\r\n```\r\n|    0   N/A  N/A      1121      G   /usr/lib/xorg/Xorg                  4MiB |\r\n```\r\n\r\nRight after the creation of `a`:\r\n```\r\n|    0   N/A  N/A      1121      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A     14701      C   python                           1251MiB |\r\n```\r\nAs you can see, you need `1251MB` to get pytorch to start using CUDA, even if you only need a single float.\r\n\r\nRight after the creation of `b`:\r\n```\r\n|    0   N/A  N/A      1121      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A     14701      C   python                           3159MiB |\r\n```\r\n`b` needs `500000000*4 bytes = 1907MB`, this is the same as the increment in memory used by the python process.\r\n\r\nRight after the creation of `c`:\r\n```\r\n|    0   N/A  N/A      1121      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A     14701      C   python                           5067MiB |\r\n```\r\nNo surprise here.\r\n\r\nRight after the creation of `d`:\r\n```\r\n|    0   N/A  N/A      1121      G   /usr/lib/xorg/Xorg                  4MiB |\r\n|    0   N/A  N/A     14701      C   python                           5067MiB |\r\n```\r\nNo further memory allocation, and the OOM error is thrown:\r\n```\r\nTraceback (most recent call last):\r\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\nRuntimeError: CUDA out of memory. Tried to allocate 1.86 GiB (GPU 0; 5.80 GiB total capacity; 3.73 GiB already allocated; 858.81 MiB free; 3.73 GiB reserved in total by PyTorch)\r\n```\r\n\r\nObviously:\r\n- The &quot;already allocated&quot; part is included in the &quot;reserved in total by PyTorch&quot; part. You can&#39;t sum them up, otherwise the sum exceeds the total available memory.\r\n- The minimum memory required to get pytorch running on GPU (`1251M`) is not included in the &quot;reserved in total&quot; part.\r\n\r\n\r\n----------\r\nSo in your case, the sum should consist of:\r\n\r\n- 792MB (reserved in total)\r\n- 1251MB (minimum to get pytorch running on GPU, assuming this is the same for both of us)\r\n- 5.13GB (free)\r\n- 168+363+161=692MB (other processes)\r\n\r\nThey sum up to approximately 7988MB=7.80GB, which is exactly you total GPU memory."
    },
    {
        "question_id": "70110429",
        "accepted_answer_id": "70123569",
        "question_title": "Pytorch: RuntimeError: result type Float can&#39;t be cast to the desired output type Long",
        "question_markdown": "I have a model which looks as follows:\r\n\r\n```py\r\nIMG_WIDTH = IMG_HEIGHT = 224\r\n\r\nclass AlexNet(nn.Module):\r\n  def __init__(self, output_dim):\r\n    super(AlexNet, self).__init__()\r\n    self._to_linear = None\r\n    self.x = torch.randn(3, IMG_WIDTH, IMG_HEIGHT).view(-1, 3, IMG_WIDTH, IMG_HEIGHT)\r\n    self.features = nn.Sequential(\r\n        nn.Conv2d(3, 64, 3, 2, 1), # in_channels, out_channels, kernel_size, stride, padding\r\n        nn.MaxPool2d(2),\r\n        nn.ReLU(inplace=True),\r\n        nn.Conv2d(64, 192, 3, padding=1),\r\n        nn.MaxPool2d(2),\r\n        nn.ReLU(inplace=True), \r\n        nn.Conv2d(192, 384, 3, padding=1),\r\n        nn.MaxPool2d(2),\r\n        nn.ReLU(inplace=True), \r\n        nn.Conv2d(384, 256, 3, padding=1),\r\n        nn.MaxPool2d(2),\r\n        nn.ReLU(inplace=True),\r\n        nn.Conv2d(256, 512, 3, padding=1),\r\n        nn.ReLU(inplace=True),\r\n        nn.Conv2d(512, 256, 3, padding=1),\r\n        nn.MaxPool2d(2),\r\n        nn.ReLU(inplace=True)\r\n  )\r\n    self.conv(self.x)\r\n    self.classifier = nn.Sequential(\r\n        nn.Dropout(.5),\r\n        nn.Linear(self._to_linear, 4096),\r\n        nn.ReLU(inplace=True),\r\n        nn.Dropout(.5),\r\n        nn.Linear(4096, 4096),\r\n        nn.ReLU(inplace=True),\r\n        nn.Linear(4096, output_dim),\r\n    )\r\n\r\n  def conv(self, x):\r\n    x = self.features(x)\r\n    if self._to_linear is None:\r\n        self._to_linear = x.shape[1] * x.shape[2] * x.shape[3]\r\n    return x\r\n\r\n  def forward(self, x):\r\n    x = self.conv(x)\r\n    h = x.view(x.shape[0], -1)\r\n    x = self.classifier(h)\r\n    return x, h\r\n```\r\nHere is my optimizer and loss functions:\r\n\r\n```py\r\noptimizer = torch.optim.Adam(model.parameters())\r\ncriterion = nn.BCEWithLogitsLoss().to(device)\r\n```\r\n\r\nHere is my `train` and `evaluate` functions:\r\n\r\n```py\r\ndef train(model, iterator, optimizer, criterion, device):\r\n  epoch_loss, epoch_acc = 0, 0\r\n  model.train()\r\n  for (x, y) in iterator:\r\n    # features and labels to the device\r\n    x = x.to(device)\r\n    y = y.to(device).long()\r\n    # Zero the gradients\r\n    optimizer.zero_grad()\r\n    y_pred, _ = model(x)\r\n  \r\n    # Calculate the loss and accuracy\r\n    loss = criterion(y_pred.squeeze(), y)\r\n    acc = binary_accuracy(y_pred, y)\r\n    # Backward propagate\r\n    loss.backward()\r\n    # Update the weights\r\n    optimizer.step()\r\n\r\n    epoch_loss +=loss.item()\r\n    epoch_acc += acc.item()\r\n\r\n  return epoch_loss/len(iterator), epoch_acc/len(iterator)\r\n\r\ndef evaluate(model, iterator, criterion, device):\r\n  epoch_loss, epoch_acc = 0, 0\r\n  model.eval()\r\n  with torch.no_grad():\r\n    for (x, y) in iterator:\r\n      x = x.to(device)\r\n      y = y.to(device).long()\r\n      y_pred, _ = model(x)\r\n      loss = criterion(y_pred, y)\r\n      acc = binary_accuracy(y_pred, y)\r\n\r\n      epoch_loss += loss.item()\r\n      epoch_acc += acc.item()\r\n  return epoch_loss/len(iterator), epoch_acc/len(iterator)\r\n```\r\n\r\nThis is the error that I&#39;m getting:\r\n\r\n```\r\nRuntimeError: result type Float can&#39;t be cast to the desired output type Long\r\n```\r\n\r\nWhat may be possibly my problem because I have tried to convert my labels to `long` tensors as follows:\r\n```\r\ny = y.to(device).long()\r\n```\r\n\r\nBut it seems not to work.",
        "accepted_answer_markdown": "I was getting the same error doing this:\r\n\r\n    loss_fn(output, target)\r\n\r\nwhere the ***output*** was Tensor **torch.float32** and ***target*** was Tensor **torch.int64**. What solved this problem was calling the loss function like this:\r\n\r\n    loss_fn(output, target.float())"
    },
    {
        "question_id": "70147878",
        "accepted_answer_id": "70150731",
        "question_title": "How to call &quot;backward&quot; in a loop with 2 optimizers?",
        "question_markdown": "I have 2 networks that I&#39;m trying to update:\r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.optim as optim\r\n    from torch.distributions import Normal\r\n    import matplotlib.pyplot as plt\r\n    from tqdm import tqdm\r\n    \r\n    softplus = torch.nn.Softplus()\r\n    \r\n    class Model_RL(nn.Module):\r\n        def __init__(self):\r\n            super(Model_RL, self).__init__()\r\n            self.fc1 = nn.Linear(3, 20)\r\n            self.fc2 = nn.Linear(20, 30)\r\n            self.fc3 = nn.Linear(30, 2)\r\n    \r\n        def forward(self, x):\r\n            x = torch.relu(self.fc1(x))\r\n            x = torch.relu(self.fc2(x))\r\n            x = softplus(self.fc3(x))\r\n            return x\r\n    \r\n    class Model_FA(nn.Module):\r\n        def __init__(self):\r\n            super(Model_FA, self).__init__()\r\n            self.fc1 = nn.Linear(1, 20)\r\n            self.fc2 = nn.Linear(20, 30)\r\n            self.fc3 = nn.Linear(30, 1)\r\n    \r\n        def forward(self, x):\r\n            x = torch.relu(self.fc1(x))\r\n            x = torch.relu(self.fc2(x))\r\n            x = softplus(self.fc3(x))\r\n            return x\r\n    \r\n    net_RL = Model_RL()\r\n    net_FA = Model_FA()\r\n\r\nThe training loop is\r\n\r\n    inps = torch.tensor([[1.0]])\r\n    y = torch.tensor(10.0)\r\n    \r\n    opt_RL = optim.Adam(net_RL.parameters())\r\n    opt_FA = optim.Adam(net_FA.parameters()) \r\n    \r\n    baseline = 0\r\n    baseline_lr = 0.1\r\n    \r\n    epochs = 100\r\n    \r\n    for _ in tqdm(range(epochs)):\r\n    \r\n        for inp in inps:\r\n    \r\n            with torch.no_grad():\r\n                net_FA(inp)\r\n                   \r\n            for layer in range(3):\r\n                out_RL = net_RL(torch.tensor([1.0,2.0,3.0]))\r\n                mu, std = out_RL\r\n                dist = Normal(mu, std)\r\n                update_values = dist.sample() \r\n                log_p = dist.log_prob(update_values).mean()\r\n    \r\n                out = net_FA(inp) \r\n                reward = -torch.square((y - out)) \r\n                baseline = (1 - baseline_lr) * baseline + baseline_lr * reward\r\n    \r\n                loss_RL = - (reward - baseline) * log_p            \r\n                opt_RL.zero_grad()\r\n                opt_FA.zero_grad()\r\n                loss_RL.backward()\r\n                opt_RL.step()            \r\n    \r\n                out = net_FA(inp) \r\n                loss_FA = torch.mean(torch.square(y - out)) \r\n                opt_RL.zero_grad()\r\n                opt_FA.zero_grad()\r\n                loss_FA.backward()\r\n                opt_FA.step()\r\n    \r\n    \r\n    \r\n    print(&quot;Mean: &quot; + str(mu.detach().numpy()) + &quot;, Goal: &quot; + str(y))\r\n    print(&quot;Standard deviation: &quot; + str(softplus(std).detach().numpy()) + &quot;, Goal: 0ish&quot;)    \r\n\r\nI&#39;m getting 2 main errors:\r\n\r\n    RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward()...\r\n\r\nAnd when I add `retain_graph=True` to both `backward` calls I get the following\r\n\r\n    RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [30, 1]], which is output 0 of TBackward, is at version 5; expected version 4 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)\r\n\r\nMy main question is **how can I make this training work**?\r\n\r\nBut intermediate questions are:\r\n\r\nwhy does `retain_graph=True` is needed here if I&#39;m using a loop? From [here][1]: &quot;there is no need to use retain_graph=True. In each loop, a new graph is created&quot;\r\n\r\nWhy does it seem as if the `retain_graph=True` makes training significantly slower (if I remove the other `backward` call)? This doesn&#39;t really makes sense to me as in each epoch a new computational graph should be created (and not just one that is being extended).\r\n\r\n\r\n  [1]: https://discuss.pytorch.org/t/how-computation-graph-in-pytorch-is-created-and-freed/3515/4",
        "accepted_answer_markdown": "I think the line `baseline = (1 - baseline_lr) * baseline + baseline_lr * reward` causing the error. Because: \r\n- **previous** state of `baseline` is used to get **new** state of `baseline`. \r\n- PyTorch will track all these states inside a graph. \r\n- `backward` will **flush** the graph.\r\n- variable `baseline` of time - **t + 1** will **try to backpropagate** through `baseline` of time - **t**.\r\n- But at time - **t + 1**  graph behind `baseline` of time - **t** doesn&#39;t exist.\r\n- This leads to error\r\n\r\nSolution:\r\nAs you are not **optimizing** variable `baseline` or anything behind `baseline`\r\n- Initialize`baseline` as **torch tensor**.\r\n- **detach** it from graph before updating state.\r\n\r\nTry this:\r\n\r\n```python\r\n# intialize baseline as torch tensor\r\nbaseline = torch.tensor(0.)\r\nbaseline_lr = 0.1\r\n\r\nepochs = 100\r\n\r\nfor _ in tqdm(range(epochs)):\r\n\r\n    for inp in inps:\r\n\r\n        with torch.no_grad():\r\n            net_FA(inp)\r\n               \r\n        for layer in range(3):\r\n            out_RL = net_RL(torch.tensor([1.0,2.0,3.0]))\r\n            mu, std = out_RL\r\n            dist = Normal(mu, std)\r\n            update_values = dist.sample() \r\n            log_p = dist.log_prob(update_values).mean()\r\n\r\n            out = net_FA(inp) \r\n            reward = -torch.square((y - out)) \r\n\r\n            # detach baseline from graph\r\n            baseline = (1 - baseline_lr) * baseline.detach() + baseline_lr * reward\r\n\r\n            loss_RL = - (reward - baseline) * log_p            \r\n            opt_RL.zero_grad()\r\n            opt_FA.zero_grad()\r\n            loss_RL.backward()\r\n            opt_RL.step()            \r\n\r\n            out = net_FA(inp) \r\n            loss_FA = torch.mean(torch.square(y - out)) \r\n            opt_RL.zero_grad()\r\n            opt_FA.zero_grad()\r\n            loss_FA.backward()\r\n            opt_FA.step()\r\n```\r\n\r\nBut actually I don&#39;t know why you are updating the networks, 3 times for the same input?\r\n"
    },
    {
        "question_id": "70163823",
        "accepted_answer_id": "70164895",
        "question_title": "How does a gradient backpropagates through random samples?",
        "question_markdown": "I&#39;m learning about policy gradients and I&#39;m having hard time understanding how does the gradient passes through a random operation. From [here][1]: `It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through`.\r\n\r\nThey have an example of the `score function`:\r\n\r\n    probs = policy_network(state)\r\n    # Note that this is equivalent to what used to be called multinomial\r\n    m = Categorical(probs)\r\n    action = m.sample()\r\n    next_state, reward = env.step(action)\r\n    loss = -m.log_prob(action) * reward\r\n    loss.backward()\r\n\r\nWhich I tried to create an example of:\r\n\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.optim as optim\r\n    from torch.distributions import Normal\r\n    import matplotlib.pyplot as plt\r\n    from tqdm import tqdm\r\n    \r\n    softplus = torch.nn.Softplus()\r\n    \r\n    class Model_RL(nn.Module):\r\n        def __init__(self):\r\n            super(Model_RL, self).__init__()\r\n            self.fc1 = nn.Linear(1, 20)\r\n            self.fc2 = nn.Linear(20, 30)\r\n            self.fc3 = nn.Linear(30, 2)\r\n    \r\n        def forward(self, x):\r\n            x1 = self.fc1(x)\r\n            x = torch.relu(x1)\r\n            x2 = self.fc2(x)\r\n            x = torch.relu(x2)\r\n            x3 = softplus(self.fc3(x))\r\n            return x3, x2, x1\r\n    \r\n    # basic \r\n    \r\n    net_RL = Model_RL()\r\n    \r\n    features = torch.tensor([1.0]) \r\n    x = torch.tensor([1.0]) \r\n    y = torch.tensor(3.0)\r\n    \r\n    baseline = 0\r\n    baseline_lr = 0.1\r\n    \r\n    epochs = 3\r\n    \r\n    opt_RL = optim.Adam(net_RL.parameters(), lr=1e-3)\r\n    losses = []\r\n    xs = []\r\n    for _ in tqdm(range(epochs)):\r\n        out_RL = net_RL(x)\r\n        mu, std = out_RL[0]\r\n        dist = Normal(mu, std)\r\n        print(dist)\r\n        a = dist.sample()\r\n        log_p = dist.log_prob(a)\r\n        \r\n        out = features * a\r\n        reward = -torch.square((y - out))\r\n        baseline = (1-baseline_lr)*baseline + baseline_lr*reward\r\n        \r\n        loss = -(reward-baseline)*log_p\r\n    \r\n        opt_RL.zero_grad()\r\n        loss.backward()\r\n        opt_RL.step()\r\n        losses.append(loss.item())\r\n\r\nThis seems to work magically fine which again, I don&#39;t understand how the gradient passes through as they mentioned that it can&#39;t pass through the random operation (but then somehow it does). \r\n\r\nNow since the gradient can&#39;t flow through the random operation I tried to replace \r\n`mu, std = out_RL[0]` with `mu, std = out_RL[0].detach()` and that caused the error:\r\n`RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn`. If the gradient doesn&#39;t pass through the random operation, I don&#39;t understand why would detaching a tensor before the operation matter.\r\n\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/distributions.html",
        "accepted_answer_markdown": "It is indeed true that sampling is not a differentiable operation *per se*. However, there exist two (broad) ways to mitigate this - [1] The *REINFORCE* way and [2] The *reparameterization* way. Since your example is related to [1], I will stick my answer to REINFORCE.\r\n\r\nWhat REINFORCE does is it entirely gets rid of sampling operation *in the computation graph*. However, the sampling operation remains outside the graph. So, your statement\r\n\r\n&gt; .. how does the gradient passes through a random operation ..\r\n\r\nisn&#39;t correct. It **does not** pass through any random operation. Let&#39;s see your example\r\n\r\n```\r\nmu, std = out_RL[0]\r\ndist = Normal(mu, std)\r\na = dist.sample()\r\nlog_p = dist.log_prob(a)\r\n```\r\n\r\nComputation of `a` does not involve creating a computation graph. It is technically equivalent to plugging in some offline data from a dataset (as in supervised learning)\r\n\r\n```Python\r\nmu, std = out_RL[0]\r\ndist = Normal(mu, std)\r\n# a = dist.sample()\r\na = torch.tensor([1.23, 4.01, -1.2, ...], device=&#39;cuda&#39;)\r\nlog_p = dist.log_prob(a)\r\n```\r\n\r\nSince we don&#39;t have offline data beforehand, we create them *on the fly* and the `.sample()` method does merely that.\r\n\r\nSo, there is no random operation on the graph. The `log_p` depends on `mu` and `std` deterministically, just like any standard computation graph. If you cut the connection like this\r\n\r\n```\r\nmu, std = out_RL[0].detach()\r\n```\r\n\r\n.. of course it is going to complaint.\r\n\r\nAlso, do not get confused by this operation\r\n\r\n```\r\ndist = Normal(mu, std)\r\nlog_p = dist.log_prob(a)\r\n```\r\n\r\nas it does not contain any randomness by itself. This is merely a shortcut for writing the tedious [log-likelihood formula](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) for `Normal` distribution."
    },
    {
        "question_id": "70178582",
        "accepted_answer_id": "70179198",
        "question_title": "Why loss function always return zero after first epoch?",
        "question_markdown": "Why the loss function is always printing zero after the first epoch?\r\n\r\nI suspect it&#39;s because of `loss = loss_fn(outputs, torch.max(labels, 1)[1])`.\r\n\r\nBut if I use `loss = loss_fn(outputs, labels)`, I will get the error\r\n\r\n    RuntimeError: 0D or 1D target tensor expected, multi-target not supported\r\n.\r\n    \r\n\r\n    nepochs = 5\r\n    \r\n    losses = np.zeros(nepochs)\r\n    \r\n    loss_fn = nn.CrossEntropyLoss()\r\n    \r\n    optimizer = optim.Adam(modell.parameters(), lr = 0.001)\r\n    \r\n    for epoch in range(nepochs):\r\n    \r\n        running_loss = 0.0\r\n        n = 0\r\n        \r\n        for data in train_loader:\r\n            \r\n            #single batch\r\n            if(n == 1):\r\n                break;\r\n                \r\n            inputs, labels = data\r\n            \r\n            optimizer.zero_grad()\r\n    \r\n            outputs = modell(inputs)\r\n            \r\n            #loss = loss_fn(outputs, labels)\r\n            loss = loss_fn(outputs, torch.max(labels, 1)[1])\r\n            loss.backward()\r\n            optimizer.step()\r\n        \r\n            running_loss += loss.item()\r\n            n += 1\r\n           \r\n        losses[epoch] = running_loss / n\r\n        print(f&quot;epoch: {epoch+1} loss: {losses[epoch] : .3f}&quot;)\r\n\r\nThe model is:\r\n\r\n    def __init__(self, labels=10):\r\n        super(Classifier, self).__init__()\r\n        self.fc = nn.Linear(3 * 64 * 64, labels)\r\n        \r\n    def forward(self, x):\r\n        out = x.reshape(x.size(0), -1) \r\n        out = self.fc (out)\r\n        return out\r\n\r\nAny idea?\r\n\r\nThe labels are a 64 elements tensor like this:\r\n\r\n    tensor([[7],[1],[ 2],[3],[ 2],[9],[9],[8],[9],[8],[ 1],[7],[9],[2],[ 5],[1],[3],[3],[8],[3],[7],[1],[7],[9],[8],[ 8],[3],[7],[ 5],[ 1],[7],[3],[2],[1],[ 3],[3],[2],[0],[3],[4],[0],[7],[1],[ 8],[4],[1],[ 5],[ 3],[4],[3],[ 4],[8],[4],[1],[ 9],[7],[3],[ 2],[ 6],[4],[ 8],[3],[ 7],[3]])",
        "accepted_answer_markdown": "Usually loss calculation is `loss = loss_fn(outputs, labels)` and here `outputs` is as following:\r\n```\r\n_ , outputs = torch.max(model(input), 1)\r\nor\r\noutputs = torch.max(predictions, 1)[0]\r\n```\r\nCommon practice is modifying `outputs` instead of `labels`:\r\n\r\n&gt; `torch.max()` returns a namedtuple `(values, indices)` where values is\r\n&gt; the maximum value of each row of the `input` tensor in the given\r\n&gt; dimension `dim`. And `indices` is the index location of each maximum value found (`argmax`).\r\n\r\nIn your code snippet the `labels` is not indices of the labels, so when you calculate the loss, the function should look like this:\r\n```\r\nloss = loss_fn(torch.max(outputs, 1)[0], labels)\r\n```\r\n"
    },
    {
        "question_id": "70449122",
        "accepted_answer_id": "70450157",
        "question_title": "Change last layer on pretrained huggingface model",
        "question_markdown": "I want to re-finetuned a transformer model but I get an unknown error when I tried to train the model.\r\nI can&#39;t change the &quot;num_labels&quot; on loading the model.\r\nSo, I tried to change it manually\r\n\r\n    \r\n    model_name = &quot;mrm8488/flaubert-small-finetuned-movie-review-sentiment-analysis&quot;\r\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\r\n    model = AutoModelForSequenceClassification.from_pretrained(model_name).to(&#39;cuda&#39;)\r\n    \r\n    num_labels = 3\r\n    model.sequence_summary.summary = torch.nn.Linear(in_features=model.sequence_summary.summary.in_features, out_features=num_labels, bias=True)\r\n    \r\n    \r\n\r\n    \r\n    trainer = Trainer(\r\n        model=model,\r\n        args=training_args,\r\n        train_dataset=tokenized_train[&#39;train&#39;],\r\n        eval_dataset=tokenized_test[&#39;train&#39;],\r\n        tokenizer=tokenizer,\r\n        compute_metrics=compute_metrics,\r\n        #data_collator=data_collator,\r\n    )\r\n    \r\n    trainer.train()\r\nThe error\r\n\r\n \r\n    ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    &lt;ipython-input-93-8139f38c5ec6&gt; in &lt;module&gt;()\r\n         20 )\r\n         21 \r\n    ---&gt; 22 trainer.train()\r\n    \r\n    7 frames\r\n    /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\r\n       2844     if size_average is not None or reduce is not None:\r\n       2845         reduction = _Reduction.legacy_get_string(size_average, reduce)\r\n    -&gt; 2846     return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\r\n       2847 \r\n       2848 \r\n    \r\n    ValueError: Expected input batch_size (24) to match target batch_size (16).\r\n\r\n",
        "accepted_answer_markdown": "So, There is a solution for this\r\nJust add `ignore_mismatched_sizes=True` when loading the model as:\r\n\r\n    model = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=3, ignore_mismatched_sizes=True).to(&#39;cuda&#39;)"
    },
    {
        "question_id": "70487666",
        "accepted_answer_id": "70488282",
        "question_title": "Concatenate layers with different sizes in PyTorch",
        "question_markdown": "In Keras, it is possible to concatenate two layers of different sizes:\r\n\r\n```python\r\n# Keras \u2014 this works, conceptually\r\nlayer_1 = Embedding(50, 5)(inputs)\r\nlayer_2 = Embedding(300, 20)(inputs)\r\nconcat = Concatenate()([layer_1, layer_2])\r\n# -&gt; `concat` now has shape `(*, 25)`, as desired\r\n```\r\n\r\nBut PyTorch keeps complaining that the two layers have different sizes:\r\n\r\n```python\r\n# PyTorch \u2014&#160;this does not work\r\nclass MyModel(torch.nn.Module):\r\n    def __init__(self):\r\n        self.layer1 = Embedding(50, 5)\r\n        self.layer2 = Embedding(300, 20)\r\n\r\n    def forward(self, inputs):\r\n        layer_1 = self.layer1(inputs)\r\n        layer_2 = self.layer2(inputs)\r\n        concat = torch.cat([layer_1, layer_2])\r\n```\r\n\r\nThe code just above results in this error:\r\n\r\n&gt; RuntimeError: Sizes of tensors must match except in dimension 0.\r\n&gt; Expected size 5 but got size 20 for tensor number 1 in the list.\r\n\r\nThe final `concat` layer I want is a layer of size `25` made of the concatenation of the two source layers.\r\n\r\nAs the two source layers are `Embedding` layers, I do not see as optimal that they would share the same dimension. In this example, using an embedding dimension of `5` for a vocabulary of `50` items, and an embedding dimension of size `20` for a vocabulary of `200` items.\r\n\r\nHow should this problem be solved in PyTorch?",
        "accepted_answer_markdown": "Indeed [`torch.cat`][1] will apply the concatenation on the first axis. Since you are looking to concatenate on the second axis, you should provide the `dim` argument as:\r\n\r\n    &gt;&gt;&gt; concat = torch.cat([layer_1, layer_2], dim=1)\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/generated/torch.cat.html"
    },
    {
        "question_id": "71166789",
        "accepted_answer_id": "71232059",
        "question_title": "HuggingFace: ValueError: expected sequence of length 165 at dim 1 (got 128)",
        "question_markdown": "I am trying to fine-tune the BERT language model on my own data. I&#39;ve gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here&#39;s my code:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer\r\nimport glob\r\nimport os\r\n\r\n\r\nbase_path = &#39;../data/&#39;\r\nmodel_name = &#39;bert-base-uncased&#39;\r\nmax_length = 512\r\ncheckpoints_dir = &#39;checkpoints&#39;\r\n\r\ntokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)\r\n\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[&#39;text&#39;], padding=True, truncation=True, max_length=max_length)\r\n\r\n\r\ndataset = load_dataset(&#39;text&#39;,\r\n        data_files={\r\n            &#39;train&#39;: f&#39;{base_path}train.txt&#39;,\r\n            &#39;test&#39;: f&#39;{base_path}test.txt&#39;,\r\n            &#39;validation&#39;: f&#39;{base_path}valid.txt&#39;\r\n        }\r\n)\r\n\r\nprint(&#39;Tokenizing data. This may take a while...&#39;)\r\ntokenized_dataset = dataset.map(tokenize_function, batched=True)\r\ntrain_dataset = tokenized_dataset[&#39;train&#39;]\r\neval_dataset = tokenized_dataset[&#39;test&#39;]\r\n\r\nmodel = AutoModel.from_pretrained(model_name)\r\n\r\ntraining_args = TrainingArguments(checkpoints_dir)\r\n\r\nprint(&#39;Training the model...&#39;)\r\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)\r\ntrainer.train()\r\n```\r\n\r\nI get the following error:\r\n\r\n```\r\n  File &quot;train_lm_hf.py&quot;, line 44, in &lt;module&gt;\r\n    trainer.train()\r\n...\r\n  File &quot;/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py&quot;, line 130, in torch_default_data_collator\r\n    batch[k] = torch.tensor([f[k] for f in features])\r\nValueError: expected sequence of length 165 at dim 1 (got 128)\r\n```\r\n\r\nWhat am I doing wrong?",
        "accepted_answer_markdown": "I fixed this solution by changing the tokenize function to:\r\n\r\n```\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[&#39;text&#39;], padding=&#39;max_length&#39;, truncation=True, max_length=max_length)\r\n```\r\n\r\n(note the `padding` argument). Also, I used a data collator like so:\r\n\r\n```\r\ndata_collator = DataCollatorForLanguageModeling(\r\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\r\n)\r\ntrainer = Trainer(\r\n        model=model,\r\n        args=training_args,\r\n        data_collator=data_collator,\r\n        train_dataset=train_dataset,\r\n        eval_dataset=eval_dataset\r\n)\r\n```"
    },
    {
        "question_id": "71261347",
        "accepted_answer_id": "71262029",
        "question_title": "RuntimeError: DataLoader worker exited unexpectedly",
        "question_markdown": "I am new to PyTorch and Machine Learning so I try to follow the tutorial from here: \r\nhttps://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\r\n\r\nBy copying the code step by step I got the following error for no reason. I tried the program on another computer and it gives syntax error. However, my IDE didn&#39;t warn my anything about syntax. I am really confused how I can fix the issue. Any help is appreciated. \r\n```\r\nRuntimeError: DataLoader worker exited unexpectedly\r\n```\r\nHere is the code. \r\n\r\n```python\r\nimport torch\r\nfrom torchvision import datasets\r\nfrom torchvision.transforms import ToTensor\r\nimport torch.nn as nn\r\nimport matplotlib.pyplot as plt\r\nfrom torch.utils.data import DataLoader\r\nfrom torch import optim\r\nfrom torch.autograd import Variable\r\n\r\ntrain_data = datasets.MNIST(\r\n    root=&#39;data&#39;,\r\n    train=True,\r\n    transform=ToTensor(),\r\n    download=True,\r\n)\r\ntest_data = datasets.MNIST(\r\n    root=&#39;data&#39;,\r\n    train=False,\r\n    transform=ToTensor()\r\n)\r\nprint(train_data)\r\nprint(test_data)\r\n\r\nprint(train_data.data.size())\r\nprint(train_data.targets.size())\r\n\r\nplt.imshow(train_data.data[0], cmap=&#39;gray&#39;)\r\nplt.title(&#39;%i&#39; % train_data.targets[0])\r\nplt.show()\r\n\r\nfigure = plt.figure(figsize=(10, 8))\r\ncols, rows = 5, 5\r\nfor i in range(1, cols * rows + 1):\r\n    sample_idx = torch.randint(len(train_data), size=(1,)).item()\r\n    img, label = train_data[sample_idx]\r\n    figure.add_subplot(rows, cols, i)\r\n    plt.title(label)\r\n    plt.axis(&quot;off&quot;)\r\n    plt.imshow(img.squeeze(), cmap=&quot;gray&quot;)\r\nplt.show()\r\n\r\nloaders = {\r\n    &#39;train&#39;: DataLoader(train_data,\r\n                        batch_size=100,\r\n                        shuffle=True,\r\n                        num_workers=1),\r\n\r\n    &#39;test&#39;: DataLoader(test_data,\r\n                       batch_size=100,\r\n                       shuffle=True,\r\n                       num_workers=1),\r\n}\r\n\r\n\r\nclass CNN(nn.Module):\r\n    def __init__(self):\r\n        super(CNN, self).__init__()\r\n        self.conv1 = nn.Sequential(\r\n            nn.Conv2d(\r\n                in_channels=1,\r\n                out_channels=16,\r\n                kernel_size=5,\r\n                stride=1,\r\n                padding=2,\r\n            ),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2),\r\n        )\r\n        self.conv2 = nn.Sequential(\r\n            nn.Conv2d(16, 32, 5, 1, 2),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(2),\r\n        )\r\n        # fully connected layer, output 10 classes\r\n        self.out = nn.Linear(32 * 7 * 7, 10)\r\n\r\n    def forward(self, x):\r\n        x = self.conv1(x)\r\n        x = self.conv2(x)\r\n        # flatten the output of conv2 to (batch_size, 32 * 7 * 7)\r\n        x = x.view(x.size(0), -1)\r\n        output = self.out(x)\r\n        return output, x  # return x for visualization\r\n\r\n\r\ncnn = CNN()\r\nprint(cnn)\r\n\r\nloss_func = nn.CrossEntropyLoss()\r\nprint(loss_func)\r\noptimizer = optim.Adam(cnn.parameters(), lr=0.01)\r\nprint(optimizer)\r\nnum_epochs = 10\r\n\r\n\r\ndef train(num_epochs, cnn, loaders):\r\n    cnn.train()\r\n\r\n    # Train the model\r\n    total_step = len(loaders[&#39;train&#39;])\r\n\r\n    for epoch in range(num_epochs):\r\n        for i, (images, labels) in enumerate(loaders[&#39;train&#39;]):\r\n            # gives batch data, normalize x when iterate train_loader\r\n            b_x = Variable(images)  # batch x\r\n            b_y = Variable(labels)  # batch y\r\n\r\n            output = cnn(b_x)[0]\r\n            loss = loss_func(output, b_y)\r\n\r\n            # clear gradients for this training step\r\n            optimizer.zero_grad()\r\n\r\n            # backpropagation, compute gradients\r\n            loss.backward()\r\n            # apply gradients\r\n            optimizer.step()\r\n\r\n            if (i + 1) % 100 == 0:\r\n                print(&#39;Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}&#39;\r\n                      .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\r\n                pass\r\n\r\n        pass\r\n\r\n    pass\r\n\r\n\r\ntrain(num_epochs, cnn, loaders)\r\n\r\n\r\ndef evalFunc():\r\n    # Test the model\r\n    cnn.eval()\r\n    with torch.no_grad():\r\n        correct = 0\r\n        total = 0\r\n        for images, labels in loaders[&#39;test&#39;]:\r\n            test_output, last_layer = cnn(images)\r\n            pred_y = torch.max(test_output, 1)[1].data.squeeze()\r\n            accuracy = (pred_y == labels).sum().item() / float(labels.size(0))\r\n            pass\r\n\r\n        print(&#39;Test Accuracy of the model on the 10000 test images: %.2f&#39; % accuracy)\r\n\r\n    pass\r\n\r\n\r\nevalFunc()\r\n\r\nsample = next(iter(loaders[&#39;test&#39;]))\r\nimgs, lbls = sample\r\n\r\nactual_number = lbls[:10].numpy()\r\n\r\ntest_output, last_layer = cnn(imgs[:10])\r\npred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\r\nprint(f&#39;Prediction number: {pred_y}&#39;)\r\nprint(f&#39;Actual number: {actual_number}&#39;)\r\n```",
        "accepted_answer_markdown": "If you are working on jupyter notebook. The problem is more likely to be `num_worker`. You should set `num_worker=0`. You can find [here][1] some solutions to follow. Because unfortunately, jupyter notebook has some issues with running multiprocessing. \r\n\r\n\r\n  [1]: https://stackoverflow.com/a/71193241/16733101"
    },
    {
        "question_id": "71300827",
        "accepted_answer_id": "71301175",
        "question_title": "TypeError: &#39;module&#39; object is not callable error?",
        "question_markdown": "I studying on a model training. when i called the training function i got this error &quot;TypeError: &#39;module&#39; object is not callable&quot; and i can&#39;t see where i missed it. \r\n\r\nhere is my calling function: \r\n\r\n    train(\r\n                model,\r\n                optimizer,\r\n                loss,\r\n                train_loader,\r\n                hyperparams[&quot;epoch&quot;],\r\n                scheduler=hyperparams[&quot;scheduler&quot;],\r\n                device=hyperparams[&quot;device&quot;],\r\n                val_loader=val_loader,\r\n               \r\n)\r\n[the error i got][1]\r\n\r\n \r\n\r\n\r\n  [1]: https://i.sstatic.net/cEZTG.png",
        "accepted_answer_markdown": "You are calling `tqdm` module, instead of `tqdm` method from `tqdm` module.\r\n\r\nReplace:\r\n\r\n    import tqdm\r\nwith:\r\n\r\n    from tqdm import tqdm"
    },
    {
        "question_id": "71396788",
        "accepted_answer_id": "72077110",
        "question_title": "Semantic segmentation with detectron2",
        "question_markdown": "I used Detectron2 to train a custom model with Instance Segmentation and worked well. There are several Tutorials on google colab with Detectron2 using Instance Segmentation, but nothing about Semantic Segmentation. So, to train the Custom Instance Segmentation the code based on colab (https://colab.research.google.com/drive/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5#scrollTo=7unkuuiqLdqd) is this:\r\n\r\n    from detectron2.engine import DefaultTrainer\r\n    \r\n    cfg = get_cfg()\r\n    cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))\r\n    cfg.DATASETS.TRAIN = (&quot;balloon_train&quot;,)\r\n    cfg.DATASETS.TEST = ()\r\n    cfg.DATALOADER.NUM_WORKERS = 2\r\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)  # Let training initialize from model zoo\r\n    cfg.SOLVER.IMS_PER_BATCH = 2\r\n    cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\r\n    cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\r\n    cfg.SOLVER.STEPS = []        # do not decay learning rate\r\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\r\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\r\n    # NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\r\n    \r\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\r\n    trainer = DefaultTrainer(cfg) \r\n    trainer.resume_or_load(resume=False)\r\n    trainer.train()\r\n\r\nAnd to run Semantic Segmentation train I replaced `&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;` to `&quot;/Misc/semantic_R_50_FPN_1x.yaml&quot;`, basicly I changed the pre-trainded model, just this. And I got this error: \r\n\r\n    TypeError: cross_entropy_loss(): argument &#39;target&#39; (position 2) must be Tensor, not NoneType\r\n\r\nHow I set up to Semantic Segmentation on Google Colab?\r\n",
        "accepted_answer_markdown": "To train for semantic segmentation you can use the same ``COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml`` model. You don&#39;t have to change this line.\r\n\r\nThe training code you showed in your question is correct and can be used for semantic segmentation as well. All that changes are the label files.\r\n\r\nOnce the model is trained, you can use it for inference by loading the model weights from the trained model\r\n\r\n    cfg = get_cfg()\r\n    cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))\r\n    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, &quot;model_final.pth&quot;)\r\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set the testing threshold for this model\r\n    cfg.DATASETS.TEST = (&quot;Detectron_terfspot_&quot; + &quot;test&quot;, )                      # the name given to your dataset when loading/registering it\r\n    cfg.DATALOADER.NUM_WORKERS = 2\r\n    cfg.SOLVER.IMS_PER_BATCH = 2\r\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\r\n    predictor = DefaultPredictor(cfg)"
    },
    {
        "question_id": "71547452",
        "accepted_answer_id": "71547625",
        "question_title": "Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 2])) is deprecated",
        "question_markdown": "I am trying to build a multiclass text classification using Pytorch and torchtext. but I am receiving this error whenever output in last hidden layer is 2, but running fine on 1 outputdim. I know there is a problem with batchsize and Data shape. What to do? I don&#39;t know the fix.\r\n\r\nConstructing iterator:\r\n```\r\n#set batch size\r\nBATCH_SIZE = 16\r\n\r\ntrain_iterator, valid_iterator = BucketIterator.splits(\r\n    (train_data, valid_data), \r\n    batch_size = BATCH_SIZE,\r\n    sort_key = lambda x: len(x.text),\r\n    sort_within_batch=True,\r\n    device = device)\r\n```\r\n\r\nModel class:\r\n```\r\nclass classifier(nn.Module):\r\n\r\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \r\n                 bidirectional, dropout):\r\n        super(classifier,self).__init__()          \r\n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\r\n        \r\n        self.gru = nn.GRU(embedding_dim, \r\n                           hidden_dim, \r\n                           num_layers=n_layers, \r\n                           bidirectional=bidirectional, \r\n                           dropout=dropout,\r\n                           batch_first=True)\r\n        \r\n        self.fc1 = nn.Linear(hidden_dim * 2, 128)\r\n        self.relu1 = nn.ReLU()\r\n        self.fc2 = nn.Linear(128, 64)\r\n        self.relu2 = nn.ReLU()\r\n        self.fc3 = nn.Linear(64, 16)\r\n        self.relu3 = nn.ReLU()\r\n        self.fc4 = nn.Linear(16, output_dim)\r\n        self.act = nn.Sigmoid()\r\n        \r\n    def forward(self, text, text_lengths):\r\n\r\n        embedded = self.embedding(text)\r\n        #embedded = [batch size, sent_len, emb dim]\r\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to(&#39;cpu&#39;),batch_first=True)\r\n\r\n        packed_output, hidden = self.gru(packed_embedded)\r\n\r\n        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\r\n                \r\n        dense_1=self.fc1(hidden)\r\n        x = self.relu1(dense_1)\r\n        x = self.fc2(x)\r\n        x = self.relu2(x)\r\n        x = self.fc3(x)\r\n        x = self.relu3(x)\r\n        dense_outputs = self.fc4(x)\r\n\r\n        #Final activation function\r\n        outputs=self.act(dense_outputs)\r\n        \r\n        return outputs\r\n```\r\ninstantiating the model:\r\n```\r\nsize_of_vocab = len(TEXT.vocab)\r\nembedding_dim = 300\r\nnum_hidden_nodes = 256\r\nnum_output_nodes = 2\r\nnum_layers = 4\r\nbidirection = True\r\ndropout = 0.2\r\n\r\nmodel = classifier(size_of_vocab, embedding_dim, num_hidden_nodes,num_output_nodes, num_layers, \r\n                   bidirectional = True, dropout = dropout).to(device)\r\n\r\ndef count_parameters(model):\r\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n    \r\nprint(f&#39;The model has {count_parameters(model):,} trainable parameters&#39;)\r\n\r\npretrained_embeddings = TEXT.vocab.vectors\r\nmodel.embedding.weight.data.copy_(pretrained_embeddings)\r\n\r\nprint(pretrained_embeddings.shape)\r\n```\r\nOptimizer and criterion used:\r\n```\r\noptimizer = optim.Adam(model.parameters())\r\ncriterion = nn.BCELoss()\r\nmodel = model.to(device)\r\ncriterion = criterion.to(device)\r\n```\r\nTraining function:\r\n```\r\nimport torchmetrics as tm\r\nmetrics = tm.Accuracy()\r\n\r\ndef train(model, iterator, optimizer, criterion):\r\n    \r\n    #initialize every epoch \r\n    epoch_loss = 0\r\n    epoch_acc = 0\r\n    \r\n    #set the model in training phase\r\n    model.train()  \r\n    \r\n    for batch in iterator:\r\n        \r\n        #resets the gradients after every batch\r\n        optimizer.zero_grad()   \r\n        \r\n        #retrieve text and no. of words\r\n        text, text_lengths = batch.text   \r\n        \r\n        #convert to 1D tensor\r\n        predictions = model(text, text_lengths).squeeze()  \r\n        \r\n        #compute the loss\r\n        loss = criterion(predictions, batch.label)        \r\n        \r\n        #compute the binary accuracy\r\n        # acc = binary_accuracy(predictions, batch.label)  \r\n        acc = metrics(predictions,batch.label)\r\n\r\n        #backpropage the loss and compute the gradients\r\n        loss.backward()       \r\n        \r\n        #update the weights\r\n        optimizer.step()      \r\n        \r\n        #loss and accuracy\r\n        epoch_loss += loss.item()  \r\n        epoch_acc += acc.item()    \r\n        \r\n    return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n```\r\n**Full error**\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n&lt;ipython-input-60-eeabf5bacadf&gt; in &lt;module&gt;()\r\n      5 \r\n      6     #train the model\r\n----&gt; 7     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n      8 \r\n      9     #evaluate the model\r\n\r\n3 frames\r\n/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py in binary_cross_entropy(input, target, weight, size_average, reduce, reduction)\r\n   2906         raise ValueError(\r\n   2907             &quot;Using a target size ({}) that is different to the input size ({}) is deprecated. &quot;\r\n-&gt; 2908             &quot;Please ensure they have the same size.&quot;.format(target.size(), input.size())\r\n   2909         )\r\n   2910 \r\n\r\nValueError: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 2])) is deprecated. Please ensure they have the same size.\r\n```",
        "accepted_answer_markdown": "What you want is `CrossEntropyLoss` instead of `BCELoss`."
    },
    {
        "question_id": "60365105",
        "accepted_answer_id": "60409953",
        "question_title": "Re-using a classification CNN model for autoencoding - pytorch",
        "question_markdown": "I am very new to pytorch so I need a bit of handholding. I am trying to re-use an old CNN classification model -- reusing the already trained convolutional layers as the encoder in an autoencoder and then training the decoder layers. The below code is what I have.\r\n\r\n```\r\nclass Autoencoder(nn.Module):\r\n  def __init__(self, model, specs):\r\n\r\n    super(Autoencoder, self).__init__()\r\n\r\n    self.encoder = nn.Sequential(\r\n        *list(model.conv_layer.children())\r\n        )\r\n    \r\n    self.decoder = nn.Sequential(\r\n        nn.ConvTranspose2d(in_channels=C7, out_channels=C6, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),\r\n        nn.ConvTranspose2d(in_channels=C6, out_channels=C5, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),\r\n        nn.ConvTranspose2d(in_channels=C5, out_channels=C4, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),\r\n        nn.ConvTranspose2d(in_channels=C4, out_channels=C3, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),\r\n        nn.ConvTranspose2d(in_channels=C3, out_channels=C2, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),\r\n        nn.ConvTranspose2d(in_channels=C2, out_channels=C1, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True), \r\n        nn.ConvTranspose2d(in_channels=C1, out_channels=C0, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True), \r\n        nn.ConvTranspose2d(in_channels=C0, out_channels=3, kernel_size=pooling, padding=0),\r\n        nn.ReLU(inplace=True),       \r\n        )\r\n    for param in self.encoder.parameters():\r\n      param.requires_grad = False\r\n    \r\n    for p in self.decoder.parameters():\r\n      if p.dim() &gt; 1:\r\n        nn.init.kaiming_normal_(p)\r\n        pass\r\n    \r\n    def forward(self, x):\r\n      x = self.encoder(x)\r\n      x = self.decoder(x)\r\n      return x\r\n\r\n\r\n```\r\n\r\nHowever, I am getting a &quot;NotImplementedError&quot;. What am I doing wrong? When I initiate an instance of that class, I would be passing the pretrained CNN classification model and self.encoder should take care of taking the layers I am interested from the model (those in conv_layer). When I: \r\n```\r\nmodel = pretrainedCNNmodel\r\nautoencoder = Autoencoder(model, specs)\r\nprint(autoencoder)\r\n```\r\nthe print looks okay, it has all layers and everything I am hoping for, but when I try to train on it I get the &quot;NotImplementedError:&quot;.\r\n\r\nEdit\r\n---\r\n\r\nHere is the entire error:\r\n\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\n&lt;ipython-input-20-9adc467b2472&gt; in &lt;module&gt;()\r\n      2 optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_lambda)\r\n      3 \r\n----&gt; 4 train(x, train_loader, test_loader, optimizer, criterion)\r\n\r\n2 frames\r\n&lt;ipython-input-5-b25edb14cf5f&gt; in train(model, train_loader, test_loader, optimizer, criterion)\r\n     15       data, target = data.cuda(), target.cuda()\r\n     16       optimizer.zero_grad()\r\n---&gt; 17       output = model(data)\r\n     18       loss = criterion(output, target)\r\n     19       loss.backward()\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n    530             result = self._slow_forward(*input, **kwargs)\r\n    531         else:\r\n--&gt; 532             result = self.forward(*input, **kwargs)\r\n    533         for hook in self._forward_hooks.values():\r\n    534             hook_result = hook(self, input, result)\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in forward(self, *input)\r\n     94             registered hooks while the latter silently ignores them.\r\n     95         &quot;&quot;&quot;\r\n---&gt; 96         raise NotImplementedError\r\n     97 \r\n     98     def register_buffer(self, name, tensor):\r\n\r\nNotImplementedError: \r\n```",
        "accepted_answer_markdown": "Since you have a bounty on this question, it cannot be closed. However, the *exact* same question was already asked and answered in [this thread][1].\r\n\r\nBasically, you have an indentation problem in your code: Your `forward` method is indented such that it is *inside* your `__init__` method, instead of being part of the `Autoencoder` class.\r\n\r\nPlease see [my other answer][2] for more details.\r\n\r\n\r\n [1]: https://stackoverflow.com/q/52241680/1714410\r\n [2]: https://stackoverflow.com/a/52242545/1714410"
    },
    {
        "question_id": "60414698",
        "accepted_answer_id": "60414990",
        "question_title": "pytorch DataLoader: `Tensors must have same number of dimensions`",
        "question_markdown": "I am trying to fit an LSTM model in Pytorch. My data is too big to be read into memory and so I want to create mini-batches of data using the `DataLoader` function from Pytorch.\r\n\r\nI have two features as input (`X1`, `X2`). I have one output feature (y). I am using 365 timesteps of `X1` &amp; `X2` as features used to predict `y`. \r\n\r\nThe dimensions of my training array is:\r\n\r\n`(n_observations, n_timesteps, n_features)` == `(9498, 365, 2)`\r\n\r\nI don&#39;t understand why the code below isn&#39;t working because I have seen other examples where the X, y pairs have different numbers of dimensions ([LSTM for runoff modelling](https://github.com/kratzert/pangeo_lstm_example/blob/master/LSTM_for_rainfall_runoff_modelling.ipynb), [Pytorch&#39;s own docs](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#iterating-through-the-dataset) )\r\n\r\n# Minimum Reproducible Example\r\n```python\r\nimport numpy as np\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\n\r\ntrain_x = torch.Tensor(np.random.random((9498, 365, 2)))\r\ntrain_y = torch.Tensor(np.random.random((9498, 1)))\r\nval_x = torch.Tensor(np.random.random((1097, 365, 2)))\r\nval_y = torch.Tensor(np.random.random((1097, 1)))\r\ntest_x = torch.Tensor(np.random.random((639, 365, 2)))\r\ntest_y = torch.Tensor(np.random.random((639, 1)))\r\n\r\ntrain_dataset = (train_x, train_y)\r\ntest_dataset = (test_x, test_y)\r\nval_dataset = (val_x, val_y)\r\n\r\ntrain_dataloader = DataLoader(train_dataset, batch_size=256)\r\n\r\niterator = train_dataloader.__iter__()\r\niterator.next()\r\n```\r\n\r\nOutput:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n&lt;ipython-input-47-2a0b28b53c8f&gt; in &lt;module&gt;\r\n     13 \r\n     14 iterator = train_dataloader.__iter__()\r\n---&gt; 15 iterator.next()\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py in __next__(self)\r\n    344     def __next__(self):\r\n    345         index = self._next_index()  # may raise StopIteration\r\n--&gt; 346         data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    347         if self._pin_memory:\r\n    348             data = _utils.pin_memory.pin_memory(data)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py in fetch(self, possibly_batched_index)\r\n     45         else:\r\n     46             data = self.dataset[possibly_batched_index]\r\n---&gt; 47         return self.collate_fn(data)\r\n\r\n/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch)\r\n     53             storage = elem.storage()._new_shared(numel)\r\n     54             out = elem.new(storage)\r\n---&gt; 55         return torch.stack(batch, 0, out=out)\r\n     56     elif elem_type.__module__ == &#39;numpy&#39; and elem_type.__name__ != &#39;str_&#39; \\\r\n     57             and elem_type.__name__ != &#39;string_&#39;:\r\n\r\nRuntimeError: invalid argument 0: Tensors must have same number of dimensions: got 4 and 3 at /tmp/pip-req-build-4baxydiv/aten/src/TH/generic/THTensor.cpp:680\r\n```\r\n",
        "accepted_answer_markdown": "The `torch.utils.data.DataLoader` must get a `torch.utils.data.Dataset` as parameters. You&#39;re giving a tuple of tensors. I suggest you use the `torch.utils.data.TensorDataset` as follows:\r\n\r\n```python\r\nfrom torch.utils.data import DataLoader, TensorDataset\r\n\r\ntrain_x = torch.rand(9498, 365, 2)     \r\ntrain_y = torch.rand(9498, 1)\r\n\r\ntrain_dataset = TensorDataset(train_x, train_y)\r\ntrain_dataloader = DataLoader(train_dataset, batch_size=256)\r\n\r\nfor x, y in train_dataloader:\r\n    print (x.shape)\r\n```\r\n\r\nCheck if it solves your problem."
    },
    {
        "question_id": "60439570",
        "accepted_answer_id": "60439832",
        "question_title": "Beginner PyTorch - RuntimeError: shape &#39;[16, 400]&#39; is invalid for input of size 9600",
        "question_markdown": "I&#39;m trying to build a CNN but I get this error:\r\n\r\n    ---&gt; 52         x = x.view(x.size(0), 5 * 5 * 16)\r\n    RuntimeError: shape &#39;[16, 400]&#39; is invalid for input of size 9600\r\n\r\nIt&#39;s not clear for me what the inputs of the &#39;x.view&#39; line should be. Also, I don&#39;t really understand how many times I should have this &#39;x.view&#39; function in my code. Is it only once, after the 3 convolutional layers and 2 linear layers? Or is it 5 times, one after every layer? \r\n\r\nHere&#39;s my code:\r\n\r\nCNN\r\n\r\n    import torch.nn.functional as F\r\n    \r\n    # Convolutional neural network\r\n    class ConvNet(nn.Module):\r\n        \r\n        def __init__(self, num_classes=10):\r\n            super(ConvNet, self).__init__()\r\n    \r\n            self.conv1 = nn.Conv2d(\r\n                in_channels=3, \r\n                out_channels=16, \r\n                kernel_size=3)\r\n            \r\n            self.conv2 = nn.Conv2d(\r\n                in_channels=16, \r\n                out_channels=24, \r\n                kernel_size=4)\r\n    \r\n            self.conv3 = nn.Conv2d(\r\n                in_channels=24, \r\n                out_channels=32, \r\n                kernel_size=4)\r\n            \r\n            self.dropout = nn.Dropout2d(p=0.3)\r\n    \r\n            self.pool = nn.MaxPool2d(2)\r\n            \r\n            self.fc1 = nn.Linear(16 * 5 * 5, 120)\r\n            self.fc2 = nn.Linear(512, 10)\r\n    \r\n            self.final = nn.Softmax(dim=1)\r\n            \r\n        def forward(self, x):\r\n    \r\n            print(&#39;shape 0 &#39; + str(x.shape))\r\n    \r\n            x = F.max_pool2d(F.relu(self.conv1(x)), 2)  \r\n            x = self.dropout(x)\r\n    \r\n            print(&#39;shape 1 &#39; + str(x.shape))\r\n    \r\n            x = F.max_pool2d(F.relu(self.conv2(x)), 2)  \r\n            x = self.dropout(x)\r\n    \r\n            print(&#39;shape 2 &#39; + str(x.shape))\r\n    \r\n            # x = F.max_pool2d(F.relu(self.conv3(x)), 2)  \r\n            # x = self.dropout(x)\r\n    \r\n            x = F.interpolate(x, size=(5, 5))  \r\n            x = x.view(x.size(0), 5 * 5 * 16)\r\n    \r\n            x = self.fc1(x) \r\n    \r\n            return x\r\n    \r\n    net = ConvNet()\r\n\r\nCan someone help me understand the problem?\r\n\r\nThe output of &#39;x.shape&#39; is:\r\n\r\nshape 0 torch.Size([16, 3, 256, 256])\r\n\r\nshape 1 torch.Size([16, 16, 127, 127])\r\n\r\nshape 2 torch.Size([16, 24, 62, 62])\r\n\r\nThanks\r\n",
        "accepted_answer_markdown": "This means that instead the product of the channel and spatial dimensions is not `5*5*16`. To flatten the tensor, replace `x = x.view(x.size(0), 5 * 5 * 16)` with:\r\n\r\n    x = x.view(x.size(0), -1)\r\n"
    },
    {
        "question_id": "60440292",
        "accepted_answer_id": "60440460",
        "question_title": "RuntimeError: expected scalar type Long but found Float",
        "question_markdown": "I can&#39;t get the dtypes to match, either the loss wants long or the model wants float if I change my tensors to long. The shape of the tensors are 42000, 1, 28, 28 and 42000. I&#39;m not sure where I can change what dtypes are required for the model or loss. \r\n\r\n\r\nI&#39;m not sure if dataloader is required, using Variable didn&#39;t work either.\r\n\r\n    dataloaders_train = torch.utils.data.DataLoader(Xt_train, batch_size=64)\r\n    \r\n    dataloaders_test = torch.utils.data.DataLoader(Yt_train, batch_size=64)\r\n    \r\n    class Network(nn.Module):\r\n        def __init__(self):\r\n            super().__init__()\r\n            \r\n    \r\n            self.hidden = nn.Linear(42000, 256)\r\n    \r\n            self.output = nn.Linear(256, 10)\r\n            \r\n    \r\n            self.sigmoid = nn.Sigmoid()\r\n            self.softmax = nn.Softmax(dim=1)\r\n            \r\n        def forward(self, x):\r\n    \r\n            x = self.hidden(x)\r\n            x = self.sigmoid(x)\r\n            x = self.output(x)\r\n            x = self.softmax(x)\r\n    \r\n            return x\r\n    \r\n    model = Network()\r\n    \r\n    input_size = 784\r\n    hidden_sizes = [28, 64]\r\n    output_size = 10 \r\n    model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\r\n                          nn.ReLU(),\r\n                          nn.Linear(hidden_sizes[0], hidden_sizes[1]),\r\n                          nn.ReLU(),\r\n                          nn.Linear(hidden_sizes[1], output_size),\r\n                          nn.Softmax(dim=1))\r\n    print(model)\r\n    \r\n    criterion = nn.NLLLoss()\r\n    optimizer = optim.SGD(model.parameters(), lr=0.003)\r\n    \r\n    epochs = 5\r\n    \r\n    for e in range(epochs):\r\n        running_loss = 0\r\n        for images, labels in zip(dataloaders_train, dataloaders_test):\r\n    \r\n            images = images.view(images.shape[0], -1)\r\n            #images, labels = Variable(images), Variable(labels)\r\n            print(images.dtype)\r\n            print(labels.dtype)\r\n    \r\n            optimizer.zero_grad()\r\n            \r\n            output = model(images)\r\n            loss = criterion(output, labels)\r\n            loss.backward()\r\n            optimizer.step()\r\n            \r\n            running_loss += loss.item()\r\n        else:\r\n            print(f&quot;Training loss: {running_loss}&quot;)\r\n    \r\n   \r\nWhich gives\r\n\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-128-68109c274f8f&gt; in &lt;module&gt;\r\n         11 \r\n         12         output = model(images)\r\n    ---&gt; 13         loss = criterion(output, labels)\r\n         14         loss.backward()\r\n         15         optimizer.step()\r\n    \r\n    /opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\r\n        530             result = self._slow_forward(*input, **kwargs)\r\n        531         else:\r\n    --&gt; 532             result = self.forward(*input, **kwargs)\r\n        533         for hook in self._forward_hooks.values():\r\n        534             hook_result = hook(self, input, result)\r\n    \r\n    /opt/conda/lib/python3.6/site-packages/torch/nn/modules/loss.py in forward(self, input, target)\r\n        202 \r\n        203     def forward(self, input, target):\r\n    --&gt; 204         return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)\r\n        205 \r\n        206 \r\n    \r\n    /opt/conda/lib/python3.6/site-packages/torch/nn/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\r\n       1836                          .format(input.size(0), target.size(0)))\r\n       1837     if dim == 2:\r\n    -&gt; 1838         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\n       1839     elif dim == 4:\r\n       1840         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\n    \r\n    RuntimeError: expected scalar type Long but found Float\r\n",
        "accepted_answer_markdown": "`LongTensor` is synonymous with integer. PyTorch won&#39;t accept a `FloatTensor` as categorical target, so it&#39;s telling you to cast your tensor to `LongTensor`. This is how you should change your target dtype:\r\n```\r\nYt_train = Yt_train.type(torch.LongTensor)\r\n```\r\nThis is very well [documented](https://pytorch.org/docs/stable/tensors.html) on the PyTorch website, you definitely won&#39;t regret spending a minute or two reading this page. PyTorch essentially defines nine CPU tensor types and nine GPU tensor types:\r\n\r\n```\r\n\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2566\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\r\n\u2551        Data type         \u2551             dtype             \u2551     CPU tensor     \u2551       GPU tensor        \u2551\r\n\u2560\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256c\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2563\r\n\u2551 32-bit floating point    \u2551 torch.float32 or torch.float  \u2551 torch.FloatTensor  \u2551 torch.cuda.FloatTensor  \u2551\r\n\u2551 64-bit floating point    \u2551 torch.float64 or torch.double \u2551 torch.DoubleTensor \u2551 torch.cuda.DoubleTensor \u2551\r\n\u2551 16-bit floating point    \u2551 torch.float16 or torch.half   \u2551 torch.HalfTensor   \u2551 torch.cuda.HalfTensor   \u2551\r\n\u2551 8-bit integer (unsigned) \u2551 torch.uint8                   \u2551 torch.ByteTensor   \u2551 torch.cuda.ByteTensor   \u2551\r\n\u2551 8-bit integer (signed)   \u2551 torch.int8                    \u2551 torch.CharTensor   \u2551 torch.cuda.CharTensor   \u2551\r\n\u2551 16-bit integer (signed)  \u2551 torch.int16 or torch.short    \u2551 torch.ShortTensor  \u2551 torch.cuda.ShortTensor  \u2551\r\n\u2551 32-bit integer (signed)  \u2551 torch.int32 or torch.int      \u2551 torch.IntTensor    \u2551 torch.cuda.IntTensor    \u2551\r\n\u2551 64-bit integer (signed)  \u2551 torch.int64 or torch.long     \u2551 torch.LongTensor   \u2551 torch.cuda.LongTensor   \u2551\r\n\u2551 Boolean                  \u2551 torch.bool                    \u2551 torch.BoolTensor   \u2551 torch.cuda.BoolTensor   \u2551\r\n\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2569\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\r\n```"
    },
    {
        "question_id": "60750288",
        "accepted_answer_id": "64316096",
        "question_title": "Invalid device id when using pytorch dataparallel\uff01",
        "question_markdown": "# Environment\uff1a\r\n- Win10  \r\n- Pytorch 1.3.0  \r\n- python3.7\r\n\r\n# Problem\uff1a\r\nI am using `dataparallel` in Pytorch to use the two 2080Ti GPUs. Code are like below:\r\n```python\r\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\r\n\r\nmodel = Darknet(opt.model_def)  \r\nmodel.apply(weights_init_normal) \r\n\r\nmodel = nn.DataParallel(model, device_ids=[0, 1]).to(device)\r\n```\r\nBut when run this code, I encounter errors below:\r\n```\r\nTraceback (most recent call last):\r\n  File &quot;C:/Users/Administrator/Desktop/PyTorch-YOLOv3-master/train.py&quot;, line 74, in &lt;module&gt;\r\n    model = nn.DataParallel(model, device_ids=[0, 1]).to(device)\r\n  File &quot;C:\\Users\\Administrator\\Anaconda3\\envs\\py37_torch1.3\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py&quot;, line 133, in __init__\r\n    _check_balance(self.device_ids)\r\n  File &quot;C:\\Users\\Administrator\\Anaconda3\\envs\\py37_torch1.3\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py&quot;, line 19, in _check_balance\r\n    dev_props = [torch.cuda.get_device_properties(i) for i in device_ids]\r\n  File &quot;C:\\Users\\Administrator\\Anaconda3\\envs\\py37_torch1.3\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py&quot;, line 19, in &lt;listcomp&gt;\r\n    dev_props = [torch.cuda.get_device_properties(i) for i in device_ids]\r\n  File &quot;C:\\Users\\Administrator\\Anaconda3\\envs\\py37_torch1.3\\lib\\site-packages\\torch\\cuda\\__init__.py&quot;, line 337, in get_device_properties\r\n    raise AssertionError(&quot;Invalid device id&quot;)\r\nAssertionError: Invalid device id\r\n```\r\nWhen I debug into it, I find the function `device_count()` in `get_device_properties()` returns 1 while I have 2 GPU on my machine. And `torch._C._cuda_getDeviceCount()` returns 2 in Anaconda Prompt. What is wrong?\r\n# Qustion:\r\nHow to solve this problem?\r\nHow can I manage to use the two GPUs using dataparallel?\r\nThank you guys!\r\n",
        "accepted_answer_markdown": "Basically as pointed out by @ToughMind, we need specify \r\n\r\n    os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0, 1&quot;\r\n\r\nIt depends though on the CUDA devices available in one&#39;s unit, so if someone has one GPU it may be appropriate to put, for example, \r\n\r\n\r\n    os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = &quot;0&quot;"
    },
    {
        "question_id": "60812032",
        "accepted_answer_id": "60813495",
        "question_title": "Using WeightedRandomSampler in PyTorch",
        "question_markdown": "I need to implement a multi-label image classification model in PyTorch. However my data is not balanced, so I used the `WeightedRandomSampler` in PyTorch to create a custom dataloader. But when I iterate through the custom dataloader, I get the error : `IndexError: list index out of range` \r\n\r\nImplemented the following code using this link :https://discuss.pytorch.org/t/balanced-sampling-between-classes-with-torchvision-dataloader/2703/3?u=surajsubramanian\r\n```python\r\ndef make_weights_for_balanced_classes(images, nclasses):                        \r\n    count = [0] * nclasses                                                      \r\n    for item in images:                                                         \r\n        count[item[1]] += 1                                                     \r\n    weight_per_class = [0.] * nclasses                                      \r\n    N = float(sum(count))                                                   \r\n    for i in range(nclasses):                                                   \r\n        weight_per_class[i] = N/float(count[i])                                 \r\n    weight = [0] * len(images)                                              \r\n    for idx, val in enumerate(images):                                          \r\n        weight[idx] = weight_per_class[val[1]]                                  \r\n    return weight \r\n```\r\n```python\r\nweights = make_weights_for_balanced_classes(train_dataset.imgs, len(full_dataset.classes))\r\nweights = torch.DoubleTensor(weights)\r\nsampler = WeightedRandomSampler(weights, len(weights))\r\n\r\ntrain_loader = DataLoader(train_dataset, batch_size=4,sampler = sampler, pin_memory=True)   \r\n```\r\n\r\nBased on the answer in https://stackoverflow.com/a/60813495/10077354, the following is my updated code. But then too when I create a dataloader :`loader = DataLoader(full_dataset, batch_size=4, sampler=sampler)`, `len(loader)` returns 1.\r\n```python\r\nclass_counts = [1691, 743, 2278, 1271]\r\nnum_samples = np.sum(class_counts)\r\nlabels = [tag for _,tag in full_dataset.imgs] \r\n\r\nclass_weights = [num_samples/class_counts[i] for i in range(len(class_counts)]\r\nweights = [class_weights[labels[i]] for i in range(num_samples)]\r\nsampler = WeightedRandomSampler(torch.DoubleTensor(weights), num_samples)\r\n```\r\n\r\nThanks a lot in advance !\r\n\r\nI included an utility function based on the accepted answer below :\r\n\r\n```python3\r\ndef sampler_(dataset):\r\n    dataset_counts = imageCount(dataset)\r\n    num_samples = sum(dataset_counts)\r\n    labels = [tag for _,tag in dataset]\r\n\r\n    class_weights = [num_samples/dataset_counts[i] for i in range(n_classes)]\r\n    weights = [class_weights[labels[i]] for i in range(num_samples)]\r\n    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))\r\n    return sampler\r\n```\r\n\r\nThe imageCount function finds number of images of each class in the dataset. Each row in the dataset contains the image and the class, so we take the second element in the tuple into consideration.\r\n```python\r\ndef imageCount(dataset):\r\n    image_count = [0]*(n_classes)\r\n    for img in dataset:\r\n        image_count[img[1]] += 1\r\n    return image_count\r\n```\r\n",
        "accepted_answer_markdown": "That code looks a bit complex... You can try the following:\r\n\r\n    #Let there be 9 samples and 1 sample in class 0 and 1 respectively\r\n    class_counts = [9.0, 1.0]\r\n    num_samples = sum(class_counts)\r\n    labels = [0, 0,..., 0, 1] #corresponding labels of samples\r\n    \r\n    class_weights = [num_samples/class_counts[i] for i in range(len(class_counts))]\r\n    weights = [class_weights[labels[i]] for i in range(int(num_samples))]\r\n    sampler = WeightedRandomSampler(torch.DoubleTensor(weights), int(num_samples))"
    },
    {
        "question_id": "60873477",
        "accepted_answer_id": "63588860",
        "question_title": "IndexError: tensors used as indices must be long, byte or bool tensors",
        "question_markdown": "I am getting this error only during the testing phase, but I do not face any problem in the training and validation phase.\r\n\r\n```\r\nIndexError: tensors used as indices must be long, byte or bool tensors\r\n```\r\nI get this error for the last line in the given code snippet.\r\n\r\nThe code snippet looks like the one below,\r\n```python\r\nNumClass = 10\r\nmask = torch.zeros(batch_size, self.mem_dim, 4, 4)\r\nones = torch.ones(1, 4, 4)\r\nNumRows = self.mem_dim\r\nElements = NumRows//NumClass\r\nfor i in range(batch_size):\r\n    lab = torch.arange(Elements * label[i], Elements*(label[i]+1), 1)\r\n    mask[i,lab] = ones\r\n```\r\nThe &quot;lab&quot; is a tensor value and prints out the range in such a way,\r\n\r\n    tensor([6, 7, 8])\r\n    tensor([ 9, 10, 11])\r\n    tensor([21, 22, 23])\r\n\r\n(Note*: the length of this lab tensor can be of length &#39;n&#39; based on the value of ElementsPerClass)",
        "accepted_answer_markdown": "Yes it works when I provide a dtype=long to the label tensor and leaving rest of the tensors with default dtypes. thanks!"
    },
    {
        "question_id": "61039700",
        "accepted_answer_id": "61039752",
        "question_title": "Using flatten in pytorch v1.0 Sequential module",
        "question_markdown": "Due to my CUDA version being 8, I am using torch 1.0.0\r\n\r\nI need to use the Flatten layer for Sequential model. Here&#39;s my code : \r\n\r\n```\r\nimport torch\r\nimport torch.nn as nn\r\nimport torch.nn.functional as F\r\nprint(torch.__version__)\r\n# 1.0.0\r\nfrom collections import OrderedDict\r\n\r\nlayers = OrderedDict()\r\nlayers[&#39;conv1&#39;] = nn.Conv2d(1, 5, 3)\r\nlayers[&#39;relu1&#39;] = nn.ReLU()\r\nlayers[&#39;conv2&#39;] = nn.Conv2d(5, 1, 3)\r\nlayers[&#39;relu2&#39;] = nn.ReLU()\r\nlayers[&#39;flatten&#39;] = nn.Flatten()\r\nlayers[&#39;linear1&#39;] = nn.Linear(3600, 1)\r\nmodel = nn.Sequential(\r\nlayers\r\n).cuda()\r\n```\r\n\r\nIt gives me the following error:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n&lt;ipython-input-38-080f7c5f5037&gt; in &lt;module&gt;\r\n      6 layers[&#39;conv2&#39;] = nn.Conv2d(5, 1, 3)\r\n      7 layers[&#39;relu2&#39;] = nn.ReLU()\r\n----&gt; 8 layers[&#39;flatten&#39;] = nn.Flatten()\r\n      9 layers[&#39;linear1&#39;] = nn.Linear(3600, 1)\r\n     10 model = nn.Sequential(\r\n\r\nAttributeError: module &#39;torch.nn&#39; has no attribute &#39;Flatten&#39;\r\n\r\n```\r\n\r\nHow can I flatten my conv layer output in pytorch 1.0.0?",
        "accepted_answer_markdown": "Just make a new Flatten layer.\r\n\r\n```\r\nfrom collections import OrderedDict\r\n\r\nclass Flatten(nn.Module):\r\n    def forward(self, input):\r\n        return input.view(input.size(0), -1)\r\n    \r\nlayers = OrderedDict()\r\nlayers[&#39;conv1&#39;] = nn.Conv2d(1, 5, 3)\r\nlayers[&#39;relu1&#39;] = nn.ReLU()\r\nlayers[&#39;conv2&#39;] = nn.Conv2d(5, 1, 3)\r\nlayers[&#39;relu2&#39;] = nn.ReLU()\r\nlayers[&#39;flatten&#39;] = Flatten()\r\nlayers[&#39;linear1&#39;] = nn.Linear(3600, 1)\r\nmodel = nn.Sequential(\r\nlayers\r\n).cuda()\r\n```"
    },
    {
        "question_id": "61117361",
        "accepted_answer_id": "64064088",
        "question_title": "How to use custom torch.autograd.Function in nn.Sequential model",
        "question_markdown": "Is there any way that I can use custom ```torch.autograd.Function``` in a ```nn.Sequential``` object or should I use explicitly an ```nn.Module``` object with forward function. Specifically I am trying to implement a sparse autoencoder and I need to add L1 distance of the code(hidden representation) to the loss.\r\nI have defined custom ```torch.autograd.Function``` L1Penalty below then tried to use it inside a ```nn.Sequential``` object as below. However when I run I got the error ```TypeError: __main__.L1Penalty is not a Module subclass``` How can I solve this issue?\r\n```\r\nclass L1Penalty(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, input, l1weight = 0.1):\r\n        ctx.save_for_backward(input)\r\n        ctx.l1weight = l1weight\r\n        return input, None\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        input, = ctx.saved_variables\r\n        grad_input = input.clone().sign().mul(ctx.l1weight)\r\n        grad_input+=grad_output\r\n        return grad_input\r\n```\r\n```\r\nmodel = nn.Sequential(\r\n    nn.Linear(10, 10),\r\n    nn.ReLU(),\r\n    nn.Linear(10, 6),\r\n    nn.ReLU(),\r\n    # sparsity\r\n    L1Penalty(),\r\n    nn.Linear(6, 10),\r\n    nn.ReLU(),\r\n    nn.Linear(10, 10),\r\n    nn.ReLU()\r\n).to(device)\r\n```\r\n\r\n",
        "accepted_answer_markdown": "The right way to do that would be this\r\n\r\n````python\r\nimport torch, torch.nn as nn\r\n\r\nclass L1Penalty(torch.autograd.Function):\r\n    @staticmethod\r\n    def forward(ctx, input, l1weight = 0.1):\r\n        ctx.save_for_backward(input)\r\n        ctx.l1weight = l1weight\r\n        return input\r\n\r\n    @staticmethod\r\n    def backward(ctx, grad_output):\r\n        input, = ctx.saved_variables\r\n        grad_input = input.clone().sign().mul(ctx.l1weight)\r\n        grad_input+=grad_output\r\n        return grad_input\r\n````\r\n\r\nCreating a Lambda class that acts as a wrapper\r\n\r\n````python\r\nclass Lambda(nn.Module):\r\n    &quot;&quot;&quot;\r\n    Input: A Function\r\n    Returns : A Module that can be used\r\n        inside nn.Sequential\r\n    &quot;&quot;&quot;\r\n    def __init__(self, func):\r\n        super().__init__()\r\n        self.func = func\r\n\r\n    def forward(self, x): return self.func(x)\r\n````\r\nTA-DA!\r\n\r\n````python\r\nmodel = nn.Sequential(\r\n    nn.Linear(10, 10),\r\n    nn.ReLU(),\r\n    nn.Linear(10, 6),\r\n    nn.ReLU(),\r\n    # sparsity\r\n    Lambda(L1Penalty.apply),\r\n    nn.Linear(6, 10),\r\n    nn.ReLU(),\r\n    nn.Linear(10, 10),\r\n    nn.ReLU())\r\n\r\na = torch.rand(50,10)\r\nb = model(a)\r\nprint(b.shape)\r\n````"
    },
    {
        "question_id": "61334483",
        "accepted_answer_id": "61334690",
        "question_title": "bool value of Tensor with more than one value is ambiguous",
        "question_markdown": "I&#39;m writing a neural network to do regression and here is my codes:\r\n\r\n    class Model(nn.Module):\r\n        def __init__(self, input_size, hidden_size, num_classes):\r\n            super().__init__()\r\n            self.h1 = nn.Linear(input_size, hidden_size)\r\n            self.h2 = nn.Linear(hidden_size, hidden_size)\r\n            self.h3 = nn.Linear(hidden_size, num_classes)\r\n    \r\n        def forward(self, x):\r\n            x = self.h1(x)\r\n            x = Fuc.tanh(x)\r\n            x = self.h2(x)\r\n            x = Fuc.relu(x)\r\n            x = self.h3(x)\r\n            return x\r\n    \r\n    model = Model(input_size=input_size, hidden_size=hidden_size, num_classes=num_classes)\r\n    opt = optim.Adam(params=model.parameters(), lr=learning_rate)\r\n    \r\n    \r\n    for epoch in range(1000):\r\n        out = model(data)\r\n        print(&#39;target&#39;, target)\r\n        print(&#39;pred&#39;, out)\r\n        loss = torch.nn.MSELoss(out, target)\r\n        print(&#39;loss&#39;, loss)\r\n    \r\n        model.zero_grad()\r\n        loss.backward()\r\n        opt.step()\r\n\r\nmy input is of shape (numberOfSample X 2) and out put is of the form [[2],[3],...], namely a list of lists where each inner list contain one number. \r\n\r\nOk so Now I train the neural network and got this error:\r\n\r\n           ...\r\n           [-0.1753],\r\n            [-0.1753],\r\n            [-0.1753]], grad_fn=&lt;AddmmBackward&gt;)\r\n    /usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\r\n      warnings.warn(&quot;nn.functional.tanh is deprecated. Use torch.tanh instead.&quot;)\r\n    ---------------------------------------------------------------------------\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-26-38e8026bfe54&gt; in &lt;module&gt;()\r\n         68     print(&#39;target&#39;, target)\r\n         69     print(&#39;pred&#39;, out)\r\n    ---&gt; 70     loss = torch.nn.MSELoss(out, target)\r\n         71     print(&#39;loss&#39;, loss)\r\n         72 \r\n    \r\n    2 frames\r\n    /usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py in legacy_get_string(size_average, reduce, emit_warning)\r\n         34         reduce = True\r\n         35 \r\n    ---&gt; 36     if size_average and reduce:\r\n         37         ret = &#39;mean&#39;\r\n         38     elif reduce:\r\n    \r\n    RuntimeError: bool value of Tensor with more than one value is ambiguous\r\n\r\n",
        "accepted_answer_markdown": "The issue originates from calling `torch.nn.MSELoss(out, target)` which is the constructor for the `MSELoss` which accepts `size_average` and `reduce` as the first and second optional positional arguments.\r\n```python\r\nloss = torch.nn.MSELoss(out, target)\r\n```\r\nInstead, you need to create an `MSELoss` object first and pass the `out` and the `target` to that object.\r\n\r\n```python\r\ncriterion = torch.nn.MSELoss()\r\n\r\nfor epoch in range(1000):\r\n    out = model(data)\r\n    loss = criterion(out, target)\r\n    loss.backward()\r\n```"
    },
    {
        "question_id": "61335217",
        "accepted_answer_id": "61335909",
        "question_title": "how to set values for layers in pytorch nn.module?",
        "question_markdown": "I have a model that I am trying to get working.  I am working through the errors, but now I think it has come down to the values in my layers.  I get this error:\r\n\r\n    RuntimeError: Given groups=1, weight of size 24 1 3 3, expected input[512, 50, 50, 3] to have 1 channels, \r\n    but got 50 channels instead\r\n\r\nMy parameters are:\r\n\r\n    LR = 5e-2\r\n    N_EPOCHS = 30\r\n    BATCH_SIZE = 512\r\n    DROPOUT = 0.5\r\n\r\nMy image information is:\r\n\r\n    depth=24\r\n    channels=3\r\n    original height = 1600\r\n    original width = 1200\r\n    resized to 50x50\r\n\r\nThis is the size of my data:\r\n\r\n    Train shape (743, 50, 50, 3) (743, 7)\r\n    Test shape (186, 50, 50, 3) (186, 7)\r\n    Train pixels 0 255 188.12228712427097 61.49539262385051\r\n    Test pixels 0 255 189.35559211469533 60.688278787628775\r\n\r\nI looked here to try to understand what values each layer is expecting, but when I put in what it says here, [https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd][1], it gives me errors about wrong channels and kernels.\r\n\r\nI found torch_summary to give me more understanding about the outputs, but it only poses more questions.  \r\n\r\nThis is my torch_summary code:\r\n\r\n    from torchvision import models\r\n    from torchsummary import summary\r\n    import torch\r\n    import torch.nn as nn\r\n    \r\n    class CNN(nn.Module):\r\n        def __init__(self):\r\n            super(CNN, self).__init__()\r\n            self.conv1 = nn.Conv2d(1,24, kernel_size=5)  # output (n_examples, 16, 26, 26)\r\n            self.convnorm1 = nn.BatchNorm2d(24) # channels from prev layer\r\n            self.pool1 = nn.MaxPool2d((2, 2))  # output (n_examples, 16, 13, 13)\r\n            self.conv2 = nn.Conv2d(24,48,kernel_size=5)  # output (n_examples, 32, 11, 11)\r\n            self.convnorm2 = nn.BatchNorm2d(48) # 2*channels?\r\n            self.pool2 = nn.AvgPool2d((2, 2))  # output (n_examples, 32, 5, 5)\r\n            self.linear1 = nn.Linear(400,120)  # input will be flattened to (n_examples, 32 * 5 * 5)\r\n            self.linear1_bn = nn.BatchNorm1d(400) # features?\r\n            self.drop = nn.Dropout(DROPOUT)\r\n            self.linear2 = nn.Linear(400, 10)\r\n            self.act = torch.relu\r\n    \r\n        def forward(self, x):\r\n            x = self.pool1(self.convnorm1(self.act(self.conv1(x))))\r\n            x = self.pool2(self.convnorm2(self.act(self.conv2(x))))\r\n            x = self.drop(self.linear1_bn(self.act(self.linear1(x.view(len(x), -1)))))\r\n            return self.linear2(x)\r\n    \r\n    \r\n    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\r\n    model=CNN().to(device)\r\n    summary(model, (3, 50, 50))\r\n\r\nThis is what it gave me:\r\n\r\n      File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py&quot;, line 342, in conv2d_forward\r\n    self.padding, self.dilation, self.groups)\r\n    RuntimeError: Given groups=1, weight of size 24 1 5 5, expected input[2, 3, 50, 50] to have 1 channels, but got 3 channels instead\r\n\r\nWhen I run my whole code, and unsqueeze_(0) my data, like so....`x_train = torch.from_numpy(x_train).unsqueeze_(0)`  I get this error:\r\n\r\n     File &quot;/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py&quot;, line 342, in conv2d_forward\r\n    self.padding, self.dilation, self.groups)\r\n    RuntimeError: Expected 4-dimensional input for 4-dimensional weight 24 1 5 5, but got 5-dimensional input of size [1, 743, 50, 50, 3] instead\r\n\r\nI don&#39;t know how to figure out how to fill in the proper values in the layers.  Will someone please help me find the correct values and understand how to understand this?  I do know that the output of one layer should be the input of another layer.  Nothing is matching up with what I thought I knew.\r\nThanks in advance!!\r\n\r\n  [1]: https://towardsdatascience.com/pytorch-layer-dimensions-what-sizes-should-they-be-and-why-4265a41e01fd\r\n",
        "accepted_answer_markdown": "It seems you have wrong order of input `x` tensor axis.  \r\nAs you can see in the [`doc`][1] `Conv2d` input must be `(N, C, H, W)`\r\n&gt; `N` is a batch size, `C` denotes a number of channels, `H` is a height of input planes in pixels, and `W` is width in pixels.\r\n\r\nSo, To make it right use [`torch.permute`][2] to swap axis in forward pass.\r\n```\r\n...\r\ndef forward(self, x):\r\n    x = x.permute(0, 3, 1, 2)\r\n    ...\r\n    ...\r\n    return self.linear2(x)\r\n...\r\n```\r\n\r\nExample of `permute`:  \r\n```python\r\nt = torch.rand(512, 50, 50, 3)\r\nt.size()\r\ntorch.Size([512, 50, 50, 3])\r\n\r\nt = t.permute(0, 3, 1, 2)\r\nt.size()\r\ntorch.Size([512, 3, 50, 50])\r\n```\r\n\r\n\r\n\r\n[1]:https://pytorch.org/docs/stable/nn.html#conv2d\r\n[2]:https://pytorch.org/docs/stable/tensors.html#torch.Tensor.permute"
    },
    {
        "question_id": "61352375",
        "accepted_answer_id": "61353066",
        "question_title": "Getting error when using print() or summary() in pytorch to see the layers and weight dimensions in a Pytorch model",
        "question_markdown": "When using print on an existing model,\r\nit doesn&#39;t print the model. Instead it shows: \r\n` &lt;function resnext101_32x8d at 0x00000178CC26BA68&gt;`\r\n\r\n    &gt;&gt;&gt; import torch\r\n    &gt;&gt;&gt; import torchvision.models as models \r\n    &gt;&gt;&gt; m1 = models.resnext101_32x8d\r\n    &gt;&gt;&gt; print(m1)\r\n    &lt;function resnext101_32x8d at 0x00000178CC26BA68&gt;\r\n    &gt;&gt;&gt;\r\n\r\nWhen using `summary`, it gives the following error:\r\n &gt;   AttributeError: &#39;function&#39; object has no attribute &#39;apply&#39;\r\n\r\n    &gt;&gt;&gt; import torch\r\n    &gt;&gt;&gt; import torchvision.models as models \r\n    &gt;&gt;&gt; from torchvision import summary\r\n    &gt;&gt;&gt; m1 = models.resnext101_32x8d\r\n    &gt;&gt;&gt;\r\n    &gt;&gt;&gt; summary(m1, (3, 224, 224))\r\n     Traceback(most recent call last):\r\n       File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n       File torchsummary.py, line 68, in summary\r\n          model.apply(register_hook)\r\n    AttributeError: &#39;function&#39; object has no attribute &#39;apply&#39;\r\n\r\n\r\nHow to fix these issues related to `print` and `summary`? Any other ways to easily see all pytorch layers and model topology?",
        "accepted_answer_markdown": "`models.resnext101_32x8d` is the class constructor, you need to call the constructor, just add parentheses at the end. \r\n\r\n```python\r\nm1 = models.resnext101_32x8d()\r\nprint(m1)\r\n```"
    },
    {
        "question_id": "61723290",
        "accepted_answer_id": "61724316",
        "question_title": "How do I use one-hot encoding with cross-entropy loss?",
        "question_markdown": "I have tried using one-hot encoding for a multiclass classification of about 120 classes using a dog breed dataset. Also using resnet18. But the following error is shown when I run the code. Please help me solve the issue.\r\n\r\nThe code of my model is shown below:\r\n\r\n```python\r\nmodel = torchvision.models.resnet18()\r\nop = torch.optim.Adam(model.parameters(),lr=0.001)\r\ncrit = nn.NLLLoss()\r\nmodel.fc = nn.Sequential(\r\n    nn.Linear(512,120),\r\n    nn.Dropout(inplace=True),\r\n    nn.ReLU(),\r\n    nn.LogSoftmax())\r\n\r\nfor i,(x,y) in enumerate(train_dl):\r\n    # prepare one-hot vector\r\n    y_oh=torch.zeors(y.shape[0],120)\r\n    y_oh.scatter_(1, y.unsqueeze(1), 1)\r\n\r\n    # do the prediction\r\n    y_hat=model(x)\r\n    y_=torch.max(y_hat)\r\n\r\n    loss=crit(y,y_)\r\n    op.zero_grad()\r\n    loss.backward()\r\n    op.step()\r\n```\r\n\r\nThe error:\r\n\r\n```\r\nRuntimeError Traceback (most recent call last) &lt;ipython-input-190-46a21ead759a&gt; in &lt;module&gt;\r\n       6 \r\n       7     y_hat=model(x)\r\n ----&gt; 8     loss=crit(y_oh,y_hat)\r\n       9     op.zero_grad()\r\n      10     loss.backward()\r\n \r\n***RuntimeError: 1D target tensor expected, multi-target not supported***\r\n```\r\n\r\n",
        "accepted_answer_markdown": "The [`NLLLoss`](https://pytorch.org/docs/stable/nn.html#nllloss) you are using expects indices of the ground-truth target classes. Btw. you do not have to convert your targets into one-hot vectors and use directly the `y` tensor.\r\n\r\nNote also that `NLLLoss` expect the distribution output distribution in the logarithmic domains, i.e., use `nn.LogSoftmax` instead of `nn.Softmax`.\r\n"
    },
    {
        "question_id": "61900138",
        "accepted_answer_id": "61902478",
        "question_title": "PyTorch &quot;Caught IndexError in DataLoader worker process 0&quot;, &quot;IndexError: too many indices for array&quot;",
        "question_markdown": "I am trying to implement a detection model based on &quot;finetuning object detection&quot; official tutorial of PyTorch. \r\nIt seemed to have worked with minimal data, (for 10 of images). However I uploaded my whole dataset to Drive and checked the index-data-label correspondences. There are not unmatching items in my setup, I have all the errors in that part solved. (I deleted extra items from the labels on GDrive)\r\n\r\n    class SomeDataset(torch.utils.data.Dataset):\r\n    def __init__(self, root_path, transforms):\r\n        self.root_path = root_path\r\n        self.transforms = transforms\r\n        # load all image files, sorting them to\r\n        # ensure that they are aligned\r\n        self.imgs = list(sorted(os.listdir(os.path.join(root_path, &quot;images&quot;))))\r\n        self.labels = list(sorted(os.listdir(os.path.join(root_path, &quot;labels&quot;))))\r\n        \r\n\r\n    def __getitem__(self, idx):\r\n        # load images ad masks\r\n        img_path = os.path.join(self.root_path, &quot;images&quot;, self.imgs[idx])\r\n        label_path = os.path.join(self.root_path, &quot;labels&quot;, self.labels[idx])\r\n\r\n        \r\n        img = Image.open(img_path).convert(&quot;RGB&quot;)\r\n        \r\n        # get labels and boxes\r\n        label_data = np.loadtxt(label_path, dtype=str, delimiter=&#39; &#39;);\r\n        print(f&quot;{len(label_data)} is the length of label data&quot;)\r\n        num_objs = label_data.shape[0];\r\n        if num_objs != 0:\r\n            print(f&quot;number of objects {num_objs}&quot;)        \r\n            # label values should start from 1\r\n            for i,label_name in enumerate(classnames):\r\n                label_data[np.where(label_name==label_data)] = i;\r\n            \r\n            label_data = label_data.astype(np.float);\r\n            print(f&quot;label data {label_data}&quot;)\r\n            xs = label_data[:,0:8:2];\r\n            ys = label_data[:,1:8:2];\r\n            \r\n            x_min = np.min(xs, axis=1)[...,np.newaxis];\r\n            x_max = np.max(xs, axis=1)[...,np.newaxis];\r\n            y_min = np.min(ys, axis=1)[...,np.newaxis];\r\n            y_max = np.max(ys, axis=1)[...,np.newaxis];\r\n            \r\n            boxes = np.hstack((x_min,y_min,x_max,y_max));\r\n            \r\n            labels = label_data[:,8];\r\n        else:\r\n            # if there is no label add background whose label is 0\r\n            boxes = [[0,0,1,1]];\r\n            labels = [0];\r\n            num_objs = 1;\r\n\r\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\r\n        labels = torch.as_tensor(labels, dtype=torch.int64)\r\n\r\n        image_id = torch.tensor([idx])\r\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\r\n        # suppose all instances are not crowd\r\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\r\n        \r\n        target = {}\r\n        target[&quot;boxes&quot;] = boxes\r\n        target[&quot;labels&quot;] = labels\r\n        target[&quot;image_id&quot;] = image_id\r\n        target[&quot;area&quot;] = area\r\n        target[&quot;iscrowd&quot;] = iscrowd\r\n\r\n        if self.transforms is not None:\r\n              img, target = self.transforms(img, target)\r\n\r\n        return img, target\r\n\r\n    def __len__(self):\r\n        return len(self.imgs)\r\n\r\n\r\nMy main method is like the following,\r\n\r\n    def main():\r\n    # train on the GPU or on the CPU, if a GPU is not available\r\n    device = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else torch.device(&#39;cpu&#39;)\r\n\r\n    # our dataset has 16 classes - background and others\r\n    num_classes = 16\r\n    # use our dataset and defined transformations\r\n    dataset = SomeDataset(&#39;trainImages&#39;, get_transform(train=True))\r\n    print(f&quot;{len(dataset)} number of images in training dataset&quot;)\r\n    dataset_validation = SomeDataset(&#39;valImages&#39;, get_transform(train=True))\r\n    print(f&quot;{len(dataset_validation)} number of images in validation dataset&quot;)\r\n\r\n    # define training and validation data loaders\r\n    data_loader = torch.utils.data.DataLoader(\r\n        dataset, batch_size=20, shuffle=True, num_workers=4,\r\n        collate_fn=utils.collate_fn)\r\n\r\n    data_loader_val = torch.utils.data.DataLoader(\r\n        dataset_validation, batch_size=10, shuffle=False, num_workers=4,\r\n        collate_fn=utils.collate_fn)\r\n\r\n    # get the model using our helper function\r\n    #model = get_model_instance_segmentation(num_classes)\r\n    model = get_rcnn(num_classes);\r\n\r\n    # move model to the right device\r\n    model.to(device)\r\n\r\n    # construct an optimizer\r\n    params = [p for p in model.parameters() if p.requires_grad]\r\n    #optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005);\r\n    optimizer = torch.optim.Adam(params, lr=0.0005);\r\n    # and a learning rate scheduler\r\n    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\r\n                                                   step_size=3,\r\n                                                   gamma=0.1)\r\n\r\n    # let&#39;s train it for 10 epochs\r\n    num_epochs = 5\r\n\r\n    for epoch in range(num_epochs):\r\n        # train for one epoch, printing every 10 iterations\r\n        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100)\r\n        # update the learning rate\r\n        lr_scheduler.step()\r\n        # evaluate on the test dataset\r\n        #evaluate(model, data_loader_test, device=device)\r\n\r\n    print(&quot;That&#39;s it!&quot;)\r\n    return model;\r\n\r\nWhen I run my code, it runs for a few number of data (for example 10 of them) and then stops and gives out this error.\r\n\r\n    IndexError: Caught IndexError in DataLoader worker process 0.\r\n    Original Traceback (most recent call last):\r\n      File &quot;/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py&quot;, line 178, in _worker_loop\r\n        data = fetcher.fetch(index)\r\n      File &quot;/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 44, in fetch\r\n        data = [self.dataset[idx] for idx in possibly_batched_index]\r\n      File &quot;/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py&quot;, line 44, in &lt;listcomp&gt;\r\n        data = [self.dataset[idx] for idx in possibly_batched_index]\r\n      File &quot;&lt;ipython-input-114-e0ccd94603fd&gt;&quot;, line 31, in __getitem__\r\n        xs = label_data[:,0:8:2];\r\n    IndexError: too many indices for array\r\n\r\n\r\nThe error goes from `model = main()` to &gt; `train_one_epoch()` and goes on.\r\n\r\nI do not understand why this is happening.\r\n\r\n\r\nAlso, this is an example from one instance of dataset,\r\n\r\n    (&lt;PIL.Image.Image image mode=RGB size=1024x1024 at 0x7F46FC0A94A8&gt;, {&#39;boxes&#39;: tensor([[ 628.,    6.,  644.,   26.],\r\n        [ 633.,   50.,  650.,   65.],\r\n        [ 620.,   27.,  637.,   44.],\r\n        [ 424.,  193.,  442.,  207.],\r\n        [ 474.,  188.,  496.,  204.],\r\n        [ 383.,  226.,  398.,  236.],\r\n        [ 399.,  218.,  418.,  231.],\r\n        [  42.,  189.,   63.,  203.],\r\n        [ 106.,  159.,  129.,  169.],\r\n        [ 273.,   17.,  287.,   34.],\r\n        [ 225.,  961.,  234.,  980.],\r\n        [ 220., 1004.,  230., 1024.]]), &#39;labels&#39;: tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]), &#39;image_id&#39;: tensor([0]), &#39;area&#39;: tensor([320., 255., 289., 252., 352., 150., 247., 294., 230., 238., 171., 200.]), &#39;iscrowd&#39;: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])})\r\n",
        "accepted_answer_markdown": "When using np.loadtxt() method, make sure to add ndims = 2 as a parameter. \r\nBecause the number of objects parameter num_obj becomes 10 even if it has only 1 object in it. \r\n\r\nIt is because 1 object becomes a column vector which shows up as 10 objects. (representing 10 columns)\r\n\r\nndims = 2, makes sure that the output of np.loadtxt() method does not give out any row or column vectors, only 2 dimensional outputs."
    },
    {
        "question_id": "61943896",
        "accepted_answer_id": "61945903",
        "question_title": "Pad torch tensors of different sizes to be equal",
        "question_markdown": "I am looking for a way to take an image/target batch for segmentation and return the batch where the image dimensions have been changed to be equal for the whole batch.  I have tried this using the code below:\r\n\r\n```\r\ndef collate_fn_padd(batch):\r\n    &#39;&#39;&#39;\r\n    Padds batch of variable length\r\n\r\n    note: it converts things ToTensor manually here since the ToTensor transform\r\n    assume it takes in images rather than arbitrary tensors.\r\n    &#39;&#39;&#39;\r\n    # separate the image and masks\r\n    image_batch,mask_batch = zip(*batch)\r\n    \r\n    # pad the images and masks\r\n    image_batch = torch.nn.utils.rnn.pad_sequence(image_batch, batch_first=True)\r\n    mask_batch = torch.nn.utils.rnn.pad_sequence(mask_batch, batch_first=True)\r\n    \r\n    # rezip the batch\r\n    batch = list(zip(image_batch, mask_batch))\r\n    \r\n    return batch\r\n```\r\n\r\nHowever, I get this error: \r\n\r\n```\r\nRuntimeError: The expanded size of the tensor (650) must match the existing size (439) at non-singleton dimension 2.  Target sizes: [3, 650, 650].  Tensor sizes: [3, 406, 439]\r\n```\r\n\r\nHow do I efficiently pad the tensors to be of equal dimensions and avoid this issue?",
        "accepted_answer_markdown": "[`rnn.pad_sequence`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence) only pads the sequence dimension, it requires all other dimensions to be equal. You cannot use it to pad images across two dimensions (height and width).\r\n\r\nTo pad an image [`torch.nn.functional.pad`](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.pad) can be used, but you need to manually determine the height and width it needs to get padded to.\r\n\r\n```py\r\nimport torch.nn.functional as F\r\n\r\n# Determine maximum height and width\r\n# The mask&#39;s have the same height and width\r\n# since they mask the image.\r\nmax_height = max([img.size(1) for img in image_batch])\r\nmax_width = max([img.size(2) for img in image_batch])\r\n\r\nimage_batch = [\r\n    # The needed padding is the difference between the\r\n    # max width/height and the image&#39;s actual width/height.\r\n    F.pad(img, [0, max_width - img.size(2), 0, max_height - img.size(1)])\r\n    for img in image_batch\r\n]\r\nmask_batch = [\r\n    # Same as for the images, but there is no channel dimension\r\n    # Therefore the mask&#39;s width is dimension 1 instead of 2\r\n    F.pad(mask, [0, max_width - mask.size(1), 0, max_height - mask.size(0)])\r\n    for mask in mask_batch\r\n]\r\n```\r\n\r\nThe padding lengths are specified in reverse order of the dimensions, where every dimension has two values, one for the padding at the beginning and one for the padding at the end. For an image with the dimensions `[channels, height, width]` the padding is given as: `[width_beginning, width_end, height_beginning, height_top]`, which can be reworded to `[left, right, top, bottom]`. Therefore the code above pads the images to the right and bottom. The channels are left out, because they are not being padded, which also means that the same padding could be directly applied to the masks."
    },
    {
        "question_id": "50425739",
        "accepted_answer_id": "50425967",
        "question_title": "can&#39;t import torch mac",
        "question_markdown": "I&#39;m trying to import torch and I&#39;m getting the next problem:\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\r\n      File &quot;/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/__init__.py&quot;, line 66, in &lt;module&gt;\r\n        import torch._dl as _dl_flags\r\n    ImportError: dlopen(/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/_dl.so, 2): no suitable image found.  Did find:\r\n    \t/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/_dl.so: mach-o, but wrong architecture\r\n    \t/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/torch/_dl.so: mach-o, but wrong architecture\r\n\r\nsomeone knows how can I solve this?\r\nThanks:)",
        "accepted_answer_markdown": "Try like that:\r\n\r\n    mkdir test_torch\r\n    cd test_torch\r\n    python3 -m venv .venv\r\n    source .venv/bin/activate\r\n    pip install torch torchvision\r\n    python3\r\n\r\n    &gt;&gt;&gt; import torch\r\n\r\nWorks for me. MacOS 10.13.4, Python 3.6.4\r\n\r\nOr like that:\r\n\r\n    mkdir test_torch\r\n    cd test_torch\r\n    virtualenv .venv\r\n    source .venv/bin/activate\r\n    pip install --upgrade pip\r\n    pip install torch torchvision\r\n    python2\r\n\r\n    &gt;&gt;&gt; import torch\r\n\r\nWorks for me. MacOS 10.13.4, Python 2.7.10\r\n\r\nIf you don&#39;t need to use `torch` then you can install only torchvision\r\n\r\n    pip install --no-deps torchvision"
    },
    {
        "question_id": "50446675",
        "accepted_answer_id": "50454509",
        "question_title": "Pytorch, can&#39;t run backward() on even the most simple network without getting an error",
        "question_markdown": "I am new to pytorch and I can&#39;t run backward() on even the most simple network without generating an error. \r\nFor example: \r\n\r\n    (Linear(6, 6)(Variable(torch.zeros([10, 6]))) - Variable(torch.zeros([10, 6]))).backward()\r\n\r\nThrows the following error \r\n\r\n    {RuntimeError}element 0 of variables does not require grad and does not have a grad_fn\r\n\r\nWhat have I done wrong in the code to create this issue? ",
        "accepted_answer_markdown": "Try adding a grad_output of matching shape as a parameter to backward:\r\n\r\n(Linear(6, 6)(Variable(torch.zeros([10, 6]))) - Variable(torch.zeros([10, 6]))).backward(torch.zeros([10, 6]))\r\n\r\nThe following answer has more details: https://stackoverflow.com/questions/44264443/why-should-be-the-function-backward-be-called-only-on-1-element-tensor-or-with-g"
    },
    {
        "question_id": "50617917",
        "accepted_answer_id": "50619112",
        "question_title": "Overflow when unpacking long - Pytorch",
        "question_markdown": "I am running the following code \r\n\r\n    import torch\r\n    from __future__ import print_function\r\n    x = torch.empty(5, 3)\r\n    print(x)\r\n\r\non an Ubuntu machine in CPU mode, which gives me following error, what would be the reason and how to fix\r\n\r\n          x = torch.empty(5, 3)\r\n    ----&gt; print(x)\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/torch/tensor.py in __repr__(self)\r\n         55         # characters to replace unicode characters with.\r\n         56         if sys.version_info &gt; (3,):\r\n    ---&gt; 57             return torch._tensor_str._str(self)\r\n         58         else:\r\n         59             if hasattr(sys.stdout, &#39;encoding&#39;):\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py in _str(self)\r\n        216             suffix = &#39;, dtype=&#39; + str(self.dtype) + suffix\r\n        217 \r\n    --&gt; 218         fmt, scale, sz = _number_format(self)\r\n        219         if scale != 1:\r\n        220             prefix = prefix + SCALE_FORMAT.format(scale) + &#39; &#39; * indent\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/torch/_tensor_str.py in _number_format(tensor, min_sz)\r\n         94     # TODO: use fmod?\r\n         95     for value in tensor:\r\n    ---&gt; 96         if value != math.ceil(value.item()):\r\n         97             int_mode = False\r\n         98             break\r\n    \r\n    RuntimeError: Overflow when unpacking long",
        "accepted_answer_markdown": "Since, torch.empty() gives uninitialized memory, so you may or may not get a large value from it. Try \r\n\r\n    x = torch.rand(5, 3)\r\n    print(x)\r\n\r\nthis would give the response."
    }
]