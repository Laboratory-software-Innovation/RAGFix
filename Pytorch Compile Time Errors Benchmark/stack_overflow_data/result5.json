[
    {
        "question_id": "65498782",
        "accepted_answer_id": "73388839",
        "question_title": "How to dump confusion matrix using TensorBoard logger in pytorch-lightning?",
        "question_markdown": "[The official doc][1] only states \r\n\r\n    &gt;&gt;&gt; from pytorch_lightning.metrics import ConfusionMatrix\r\n    &gt;&gt;&gt; target = torch.tensor([1, 1, 0, 0])\r\n    &gt;&gt;&gt; preds = torch.tensor([0, 1, 0, 0])\r\n    &gt;&gt;&gt; confmat = ConfusionMatrix(num_classes=2)\r\n    &gt;&gt;&gt; confmat(preds, target)\r\n\r\nThis doesn&#39;t show how to use the metric with the framework.\r\n\r\nMy attempt (methods are not complete and only show relevant parts):\r\n\r\n```\r\ndef __init__(...):\r\n    self.val_confusion = pl.metrics.classification.ConfusionMatrix(num_classes=self._config.n_clusters)\r\n\r\ndef validation_step(self, batch, batch_index):\r\n    ...\r\n    log_probs = self.forward(orig_batch)\r\n    loss = self._criterion(log_probs, label_batch)\r\n   \r\n    self.val_confusion.update(log_probs, label_batch)\r\n    self.log(&#39;validation_confusion_step&#39;, self.val_confusion, on_step=True, on_epoch=False)\r\n\r\ndef validation_step_end(self, outputs):\r\n    return outputs\r\n\r\ndef validation_epoch_end(self, outs):\r\n    self.log(&#39;validation_confusion_epoch&#39;, self.val_confusion.compute())\r\n\r\n```\r\n\r\nAfter the 0th epoch, this gives\r\n\r\n        Traceback (most recent call last):\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py&quot;, line 521, in train\r\n            self.train_loop.run_training_epoch()\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\training_loop.py&quot;, line 588, in run_training_epoch\r\n            self.trainer.run_evaluation(test_mode=False)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py&quot;, line 613, in run_evaluation\r\n            self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py&quot;, line 346, in log_evaluation_step_metrics\r\n            self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\evaluation_loop.py&quot;, line 350, in __log_result_step_metrics\r\n            cached_batch_pbar_metrics, cached_batch_log_metrics = cached_results.update_logger_connector()\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 378, in update_logger_connector\r\n            batch_log_metrics = self.get_latest_batch_log_metrics()\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 418, in get_latest_batch_log_metrics\r\n            batch_log_metrics = self.run_batch_from_func_name(&quot;get_batch_log_metrics&quot;)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 414, in run_batch_from_func_name\r\n            results = [func(include_forked_originals=False) for func in results]\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 414, in &lt;listcomp&gt;\r\n            results = [func(include_forked_originals=False) for func in results]\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 122, in get_batch_log_metrics\r\n            return self.run_latest_batch_metrics_with_func_name(&quot;get_batch_log_metrics&quot;,\r\n    *args, **kwargs)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 115, in run_latest_batch_metrics_with_func_name\r\n            for dl_idx in range(self.num_dataloaders)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 115, in &lt;listcomp&gt;\r\n            for dl_idx in range(self.num_dataloaders)\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\epoch_result_store.py&quot;, line 100, in get_latest_from_func_name\r\n            results.update(func(*args, add_dataloader_idx=add_dataloader_idx, **kwargs))\r\n          File &quot;C:\\code\\EPMD\\Kodex\\Templates\\Testing\\venv\\lib\\site-packages\\pytorch_lightning\\core\\step_result.py&quot;, line 298, in get_batch_log_metrics\r\n            result[dl_key] = self[k]._forward_cache.detach()\r\n        AttributeError: &#39;NoneType&#39; object has no attribute &#39;detach&#39;\r\n\r\n                                                          \r\nIt does pass the sanity validation check before training.\r\n\r\nThe failure happens on the return in `validation_step_end`. Makes little sense to me.\r\n\r\nThe exact same method of using mertics works fine with accuracy.\r\n\r\n**How to get a correct confusion matrix?**\r\n\r\n  [1]: https://pytorch-lightning.readthedocs.io/en/0.8.3/metrics.html#tensormetric\r\n",
        "accepted_answer_markdown": "Updated answer, August 2022\r\n---------------------------\r\n\r\n```\r\n\r\nclass IntHandler:\r\n    def legend_artist(self, legend, orig_handle, fontsize, handlebox):\r\n        x0, y0 = handlebox.xdescent, handlebox.ydescent\r\n        text = plt.matplotlib.text.Text(x0, y0, str(orig_handle))\r\n        handlebox.add_artist(text)\r\n        return text\r\n\r\n\r\n\r\nclass LightningClassifier(LightningModule):\r\n    ...\r\n\r\n    def _common_step(self, batch, batch_nb, stage: str):\r\n        assert stage in (&quot;train&quot;, &quot;val&quot;, &quot;test&quot;)\r\n        logger = self._logger\r\n        augmented_image, labels = batch\r\n\r\n        outputs, aux_outputs = self(augmented_image)\r\n        loss = self._criterion(outputs, labels)\r\n\r\n        return outputs, labels, loss\r\n\r\n    def validation_step(self, batch, batch_nb):\r\n        stage = &quot;val&quot;\r\n        outputs, labels, loss = self._common_step(batch, batch_nb, stage=stage)\r\n        self._common_log(loss, stage=stage)\r\n\r\n        return {&quot;loss&quot;: loss, &quot;outputs&quot;: outputs, &quot;labels&quot;: labels}\r\n\r\n\r\n    def validation_epoch_end(self, outs):\r\n        # see https://github.com/Lightning-AI/metrics/blob/ff61c482e5157b43e647565fa0020a4ead6e9d61/docs/source/pages/lightning.rst\r\n        # each forward pass, thus leading to wrong accumulation. In practice do the following:\r\n        tb = self.logger.experiment  # noqa\r\n\r\n        outputs = torch.cat([tmp[&#39;outputs&#39;] for tmp in outs])\r\n        labels = torch.cat([tmp[&#39;labels&#39;] for tmp in outs])\r\n\r\n        confusion = torchmetrics.ConfusionMatrix(num_classes=self.n_labels).to(outputs.get_device())\r\n        confusion(outputs, labels)\r\n        computed_confusion = confusion.compute().detach().cpu().numpy().astype(int)\r\n\r\n        # confusion matrix\r\n        df_cm = pd.DataFrame(\r\n            computed_confusion,\r\n            index=self._label_ind_by_names.values(),\r\n            columns=self._label_ind_by_names.values(),\r\n        )\r\n\r\n        fig, ax = plt.subplots(figsize=(10, 5))\r\n        fig.subplots_adjust(left=0.05, right=.65)\r\n        sn.set(font_scale=1.2)\r\n        sn.heatmap(df_cm, annot=True, annot_kws={&quot;size&quot;: 16}, fmt=&#39;d&#39;, ax=ax)\r\n        ax.legend(\r\n            self._label_ind_by_names.values(),\r\n            self._label_ind_by_names.keys(),\r\n            handler_map={int: IntHandler()},\r\n            loc=&#39;upper left&#39;,\r\n            bbox_to_anchor=(1.2, 1)\r\n        )\r\n        buf = io.BytesIO()\r\n\r\n        plt.savefig(buf, format=&#39;jpeg&#39;, bbox_inches=&#39;tight&#39;)\r\n        buf.seek(0)\r\n        im = Image.open(buf)\r\n        im = torchvision.transforms.ToTensor()(im)\r\n        tb.add_image(&quot;val_confusion_matrix&quot;, im, global_step=self.current_epoch)\r\n\r\n```\r\n\r\n\r\noutput:\r\n\r\n[![enter image description here][1]][1]\r\n\r\n\r\nAlso based on [this][2]\r\n\r\n\r\n  [1]: https://i.sstatic.net/fbOjk.png\r\n  [2]: https://stackoverflow.com/a/73387061/913098"
    },
    {
        "question_id": "65537991",
        "accepted_answer_id": "65538052",
        "question_title": "ValueError: Target size (torch.Size([1000])) must be the same as input size (torch.Size([1000, 1]))",
        "question_markdown": "I am trying to train my first neural net in pyTorch (I&#39;m not a programmer, just a confused chemist).\r\n\r\nThe net itself is supposed to take 1064 element vectors and rate them with a float number. \r\n\r\nSo far I have encountered a variety of errors, ranging from &#39;float instead of long&#39; to &#39;Target 1 is out of bounds&#39;. Thus I have redefined the dtypes, corrected the dimensions of the input vector, changed the loss function and now I am stuck in the situation when correcting the current error sets me back to the previous ones.\r\n\r\nWhich is: \r\n\r\n    ValueError: Target size (torch.Size([1000])) must be the same as input size (torch.Size([1000, 1]))\r\n\r\n \r\nat the `&#39;loss=loss_calc(outputs, target)&#39;` line. \r\n\r\nI tried unsqueezing the label during the DataSet class definition, but this solution sets me back. When I tried to `label = label.view(1,1)`, the resulting error changed to \r\n\r\n&gt; Target size (torch.Size([1000, 1, 1])) must be the same as input size (torch.Size([1000, 1]))\r\n\r\nCould anyone please help me figure this out?\r\n\r\n    import pandas as pd\r\n    import numpy as np\r\n    import rdkit\r\n    from rdkit import Chem\r\n    from rdkit.Chem import AllChem\r\n    import torch\r\n    import torch.nn as nn\r\n    import torch.nn.functional as F\r\n    import torch.optim as optim\r\n    from torch.utils.data import Dataset, DataLoader\r\n    \r\n    class dataset(Dataset):\r\n        def __init__(self, path, transform=None):\r\n            self.data = pd.read_excel(path)\r\n            self.transform = transform\r\n            \r\n        def __len__(self):\r\n            return len(self.data)\r\n        \r\n        def __getitem__(self, index):\r\n            smiles=self.data.at[index, &#39;smiles&#39;]\r\n            mol=Chem.MolFromSmiles(smiles)\r\n            morgan = torch.tensor(list(AllChem.GetMorganFingerprintAsBitVect(mol, 1, nBits=1064)), dtype=torch.float)\r\n            \r\n            label=torch.tensor(self.data.at[index, &#39;score&#39;], dtype=torch.long)\r\n            \r\n            if self.transform is not None:\r\n                morgan=self.transform(morgan)\r\n            \r\n            return morgan, label\r\n    \r\n    class Net(nn.Module):\r\n    \r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            self.fc1 = nn.Linear(1064, 1064)\r\n            self.fc2 = nn.Linear(1064, 1)\r\n            self.act = nn.Tanh()\r\n        \r\n        def forward(self, x):\r\n            x = self.act(self.fc1(x))\r\n            x = self.act(self.fc2(x))\r\n            x = self.fc2(x)\r\n            return x\r\n    \r\n    trainSet=dataset(r&#39;C:\\Users\\BajMic\\Peptides\\trainingSet.xlsx&#39;)\r\n    testSet=dataset(r&#39;C:\\Users\\BajMic\\Peptides\\testSet.xlsx&#39;)\r\n    \r\n    net = Net()\r\n    loss_calc = nn.CrossEntropyLoss()\r\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\r\n    \r\n    for epoch in range(2):\r\n        running_loss=0.0\r\n        for data in DataLoader(trainSet, batch_size=1000, shuffle=True):\r\n            inputs, target = data\r\n            optimizer.zero_grad()\r\n            outputs = net(inputs)\r\n            print(outputs)\r\n            loss = loss_calc(outputs, target)\r\n            loss.backward()\r\n            optimizer.step()        # print statistics\r\n            running_loss += loss.item()\r\n            print(&#39;[%d, %5d] loss: %.3f&#39; %(epoch + 1, i + 1, running_loss))\r\n            running_loss = 0.0\r\n    print(&#39;Finished Training&#39;)",
        "accepted_answer_markdown": "When working with a loss function and having problems with shapes you will usually have an error message of this type: \r\n\r\n&gt; Target size (torch.Size([...])) must be the same as input size (torch.Size([...]))\r\n\r\n*&#39;Target&#39;* refers to the label, the ground-truth, while *&#39;input&#39;* refers to your model&#39;s output. In this case, the output is *1000* predictions (you set `batch_size=1000`) of 1 scalar value. Therefore the shape is `(1000, 1)`. This last axis is the one bothering you, since the prediction vector is just a 1D tensor containing 1000 scalars, i.e. `(1000)`.\r\n\r\nTo solve this you can expand your target tensor with an extra dimension. With [`torch.unsqueeze()`][1]:\r\n\r\n    target = target.unsqueeze(-1) # -1 stands for last here equivalent to 1\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/generated/torch.unsqueeze.html"
    },
    {
        "question_id": "65647833",
        "accepted_answer_id": "65647965",
        "question_title": "ResNet object has no attribute &#39;predict&#39;",
        "question_markdown": "I have trained a CNN model in PyTorch to detect skin diseases in 6 different classes. My model came out with an accuracy of 92% and I saved it in a .pth file. I wish to use this model for predictions but I don&#39;t know how to do so. If anyone can aid me in the necessary steps, I will be grateful. \r\n\r\nI have tried just taking the image input straight from the folder, resizing it, and then running it through the model for predictions. The error I face is a ModuleAttributeAError which says there is no attribute named predict. Now I do not understand where I went wrong and I know this is a simple task for most but I was hoping for some guidance in this regard. The dataset I used is the Skin Cancer MNIST: HAM10000 dataset from Kaggle and trained it on ResNet18. If anyone has any pointers on fine-tuning the model, I would greatly appreciate it.\r\n\r\nTLDR: I get an error called ModuleAttributeError that says the &#39;ResNet&#39; module has no attribute &#39;predict&#39;.\r\n\r\nThe image is preprocessed here as follows:\r\n```\r\nimport os, cv2,itertools\r\nimport matplotlib.pyplot as plt\r\nimport numpy as np\r\nimport pandas as pd\r\nimport pickle\r\nfrom tqdm import tqdm\r\nfrom glob import glob\r\nfrom PIL import Image\r\n\r\n# pytorch libraries\r\nimport torch\r\nfrom torch import optim,nn\r\nfrom torch.autograd import Variable\r\nfrom torch.utils.data import DataLoader,Dataset\r\nfrom torchvision import models,transforms\r\n\r\n# sklearn libraries\r\nfrom sklearn.metrics import confusion_matrix\r\nfrom sklearn.model_selection import train_test_split\r\nfrom sklearn.metrics import classification_report\r\n\r\n\r\nnp.random.seed(10)\r\ntorch.manual_seed(10)\r\ntorch.cuda.manual_seed(10)\r\n\r\nprint(os.listdir(&quot;/content/drive/My Drive/input&quot;))\r\n\r\nfrom google.colab import drive\r\ndrive.mount(&#39;/content/drive&#39;)\r\n\r\n&quot;&quot;&quot;**Data analysis and preprocessing**&quot;&quot;&quot;\r\n\r\ndata_dir = &#39;/content/drive/My Drive/input&#39;\r\nall_image_path = glob(os.path.join(data_dir, &#39;*&#39;, &#39;*.jpg&#39;))\r\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\r\nlesion_type_dict = {\r\n    &#39;nv&#39;: &#39;Melanocytic nevi&#39;,\r\n    &#39;mel&#39;: &#39;Melanoma&#39;,\r\n    &#39;bkl&#39;: &#39;Benign keratosis-like lesions &#39;,\r\n    &#39;bcc&#39;: &#39;Basal cell carcinoma&#39;,\r\n    &#39;akiec&#39;: &#39;Actinic keratoses&#39;,\r\n    &#39;vasc&#39;: &#39;Vascular lesions&#39;,\r\n    &#39;df&#39;: &#39;Dermatofibroma&#39;\r\n}\r\n\r\ndef compute_img_mean_std(image_paths):\r\n    &quot;&quot;&quot;\r\n        computing the mean and std of three channel on the whole dataset,\r\n        first we should normalize the image from 0-255 to 0-1\r\n    &quot;&quot;&quot;\r\n\r\n    img_h, img_w = 224, 224\r\n    imgs = []\r\n    means, stdevs = [], []\r\n\r\n    for i in tqdm(range(len(image_paths))):\r\n        img = cv2.imread(image_paths[i])\r\n        img = cv2.resize(img, (img_h, img_w))\r\n        imgs.append(img)\r\n\r\n    imgs = np.stack(imgs, axis=3)\r\n    print(imgs.shape)\r\n\r\n    imgs = imgs.astype(np.float32) / 255.\r\n\r\n    for i in range(3):\r\n        pixels = imgs[:, :, i, :].ravel()  # resize to one row\r\n        means.append(np.mean(pixels))\r\n        stdevs.append(np.std(pixels))\r\n\r\n    means.reverse()  # BGR --&gt; RGB\r\n    stdevs.reverse()\r\n\r\n    print(&quot;normMean = {}&quot;.format(means))\r\n    print(&quot;normStd = {}&quot;.format(stdevs))\r\n    return means,stdevs\r\n\r\n# norm_mean,norm_std = compute_img_mean_std(all_image_path)\r\n\r\nnorm_mean = (0.763035, 0.54564625, 0.5700399)\r\nnorm_std = (0.1409281, 0.15261264, 0.16997051)\r\n\r\ndf_original = pd.read_csv(os.path.join(data_dir, &#39;HAM10000_metadata.csv&#39;))\r\ndf_original[&#39;path&#39;] = df_original[&#39;image_id&#39;].map(imageid_path_dict.get)\r\ndf_original[&#39;cell_type&#39;] = df_original[&#39;dx&#39;].map(lesion_type_dict.get)\r\ndf_original[&#39;cell_type_idx&#39;] = pd.Categorical(df_original[&#39;cell_type&#39;]).codes\r\ndf_original.head()\r\n\r\n# this will tell us how many images are associated with each lesion_id\r\ndf_undup = df_original.groupby(&#39;lesion_id&#39;).count()\r\n# now we filter out lesion_id&#39;s that have only one image associated with it\r\ndf_undup = df_undup[df_undup[&#39;image_id&#39;] == 1]\r\ndf_undup.reset_index(inplace=True)\r\ndf_undup.head()\r\n\r\n# here we identify lesion_id&#39;s that have duplicate images and those that have only one image.\r\ndef get_duplicates(x):\r\n    unique_list = list(df_undup[&#39;lesion_id&#39;])\r\n    if x in unique_list:\r\n        return &#39;unduplicated&#39;\r\n    else:\r\n        return &#39;duplicated&#39;\r\n\r\n# create a new colum that is a copy of the lesion_id column\r\ndf_original[&#39;duplicates&#39;] = df_original[&#39;lesion_id&#39;]\r\n# apply the function to this new column\r\ndf_original[&#39;duplicates&#39;] = df_original[&#39;duplicates&#39;].apply(get_duplicates)\r\ndf_original.head()\r\n\r\ndf_original[&#39;duplicates&#39;].value_counts()\r\n\r\n# now we filter out images that don&#39;t have duplicates\r\ndf_undup = df_original[df_original[&#39;duplicates&#39;] == &#39;unduplicated&#39;]\r\ndf_undup.shape\r\n\r\n# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\r\ny = df_undup[&#39;cell_type_idx&#39;]\r\n_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\r\ndf_val.shape\r\n\r\ndf_val[&#39;cell_type_idx&#39;].value_counts()\r\n\r\n# This set will be df_original excluding all rows that are in the val set\r\n# This function identifies if an image is part of the train or val set.\r\ndef get_val_rows(x):\r\n    # create a list of all the lesion_id&#39;s in the val set\r\n    val_list = list(df_val[&#39;image_id&#39;])\r\n    if str(x) in val_list:\r\n        return &#39;val&#39;\r\n    else:\r\n        return &#39;train&#39;\r\n\r\n# identify train and val rows\r\n# create a new colum that is a copy of the image_id column\r\ndf_original[&#39;train_or_val&#39;] = df_original[&#39;image_id&#39;]\r\n# apply the function to this new column\r\ndf_original[&#39;train_or_val&#39;] = df_original[&#39;train_or_val&#39;].apply(get_val_rows)\r\n# filter out train rows\r\ndf_train = df_original[df_original[&#39;train_or_val&#39;] == &#39;train&#39;]\r\nprint(len(df_train))\r\nprint(len(df_val))\r\n\r\ndf_train[&#39;cell_type_idx&#39;].value_counts()\r\n\r\ndf_val[&#39;cell_type&#39;].value_counts()\r\n\r\n# Copy fewer class to balance the number of 7 classes\r\ndata_aug_rate = [15,10,5,50,0,40,5]\r\nfor i in range(7):\r\n    if data_aug_rate[i]:\r\n        df_train=df_train.append([df_train.loc[df_train[&#39;cell_type_idx&#39;] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\r\ndf_train[&#39;cell_type&#39;].value_counts()\r\n\r\n# # We can split the test set again in a validation set and a true test set:\r\n# df_val, df_test = train_test_split(df_val, test_size=0.5)\r\ndf_train = df_train.reset_index()\r\ndf_val = df_val.reset_index()\r\n# df_test = df_test.reset_index()\r\n```\r\n\r\nHere is where I build the model:\r\n```\r\n# feature_extract is a boolean that defines if we are finetuning or feature extracting. \r\n# If feature_extract = False, the model is finetuned and all model parameters are updated. \r\n# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\r\ndef set_parameter_requires_grad(model, feature_extracting):\r\n    if feature_extracting:\r\n        for param in model.parameters():\r\n            param.requires_grad = False\r\n\r\ndef initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\r\n    # Initialize these variables which will be set in this if statement. Each of these\r\n    #   variables is model specific.\r\n    model_ft = None\r\n    input_size = 0\r\n\r\n    if model_name == &quot;resnet&quot;:\r\n        &quot;&quot;&quot; Resnet18, resnet34, resnet50, resnet101\r\n        &quot;&quot;&quot;\r\n        model_ft = models.resnet18(pretrained=use_pretrained)\r\n        set_parameter_requires_grad(model_ft, feature_extract)\r\n        num_ftrs = model_ft.fc.in_features\r\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\r\n        input_size = 224\r\n\r\n\r\n    elif model_name == &quot;vgg&quot;:\r\n        &quot;&quot;&quot; VGG11_bn\r\n        &quot;&quot;&quot;\r\n        model_ft = models.vgg11_bn(pretrained=use_pretrained)\r\n        set_parameter_requires_grad(model_ft, feature_extract)\r\n        num_ftrs = model_ft.classifier[6].in_features\r\n        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\r\n        input_size = 224\r\n\r\n\r\n    elif model_name == &quot;densenet&quot;:\r\n        &quot;&quot;&quot; Densenet121\r\n        &quot;&quot;&quot;\r\n        model_ft = models.densenet121(pretrained=use_pretrained)\r\n        set_parameter_requires_grad(model_ft, feature_extract)\r\n        num_ftrs = model_ft.classifier.in_features\r\n        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\r\n        input_size = 224\r\n\r\n    elif model_name == &quot;inception&quot;:\r\n        &quot;&quot;&quot; Inception v3\r\n        &quot;&quot;&quot;\r\n        model_ft = models.inception_v3(pretrained=use_pretrained)\r\n        set_parameter_requires_grad(model_ft, feature_extract)\r\n        # Handle the auxilary net\r\n        num_ftrs = model_ft.AuxLogits.fc.in_features\r\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\r\n        # Handle the primary net\r\n        num_ftrs = model_ft.fc.in_features\r\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\r\n        input_size = 299\r\n\r\n    else:\r\n        print(&quot;Invalid model name, exiting...&quot;)\r\n        exit()\r\n    return model_ft, input_size\r\n\r\n# resnet,vgg,densenet,inception\r\nmodel_name = &#39;resnet&#39;\r\nnum_classes = 7\r\nfeature_extract = False\r\n# Initialize the model for this run\r\nmodel_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\r\n# Define the device:\r\ndevice = torch.device(&#39;cuda:0&#39;)\r\n# Put the model on the device:\r\nmodel = model_ft.to(device)\r\n\r\n# norm_mean = (0.49139968, 0.48215827, 0.44653124)\r\n# norm_std = (0.24703233, 0.24348505, 0.26158768)\r\n# define the transformation of the train images.\r\ntrain_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\r\n                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\r\n                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\r\n                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\r\n# define the transformation of the val images.\r\nval_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\r\n                                    transforms.Normalize(norm_mean, norm_std)])\r\n\r\n# Define a pytorch dataloader for this dataset\r\nclass HAM10000(Dataset):\r\n    def __init__(self, df, transform=None):\r\n        self.df = df\r\n        self.transform = transform\r\n\r\n    def __len__(self):\r\n        return len(self.df)\r\n\r\n    def __getitem__(self, index):\r\n        # Load data and get label\r\n        X = Image.open(self.df[&#39;path&#39;][index])\r\n        y = torch.tensor(int(self.df[&#39;cell_type_idx&#39;][index]))\r\n\r\n        if self.transform:\r\n            X = self.transform(X)\r\n\r\n        return X, y\r\n\r\n# Define the training set using the table train_df and using our defined transitions (train_transform)\r\ntraining_set = HAM10000(df_train, transform=train_transform)\r\ntrain_loader = DataLoader(training_set, batch_size=64, shuffle=True, num_workers=4)\r\n# Same for the validation set:\r\nvalidation_set = HAM10000(df_val, transform=train_transform)\r\nval_loader = DataLoader(validation_set, batch_size=64, shuffle=False, num_workers=4)\r\n\r\n# we use Adam optimizer, use cross entropy loss as our loss function\r\noptimizer = optim.Adam(model.parameters(), lr=1e-5)\r\ncriterion = nn.CrossEntropyLoss().to(device)\r\n```\r\nLastly, is the training process with a prediction function:\r\n```\r\n# this function is used during training process, to calculation the loss and accuracy\r\nclass AverageMeter(object):\r\n    def __init__(self):\r\n        self.reset()\r\n\r\n    def reset(self):\r\n        self.val = 0\r\n        self.avg = 0\r\n        self.sum = 0\r\n        self.count = 0\r\n\r\n    def update(self, val, n=1):\r\n        self.val = val\r\n        self.sum += val * n\r\n        self.count += n\r\n        self.avg = self.sum / self.count\r\n\r\ntotal_loss_train, total_acc_train = [],[]\r\ndef train(train_loader, model, criterion, optimizer, epoch):\r\n    model.train()\r\n    train_loss = AverageMeter()\r\n    train_acc = AverageMeter()\r\n    curr_iter = (epoch - 1) * len(train_loader)\r\n    for i, data in enumerate(train_loader):\r\n        images, labels = data\r\n        N = images.size(0)\r\n        # print(&#39;image shape:&#39;,images.size(0), &#39;label shape&#39;,labels.size(0))\r\n        images = Variable(images).to(device)\r\n        labels = Variable(labels).to(device)\r\n\r\n        optimizer.zero_grad()\r\n        outputs = model(images)\r\n\r\n        loss = criterion(outputs, labels)\r\n        loss.backward()\r\n        optimizer.step()\r\n        prediction = outputs.max(1, keepdim=True)[1]\r\n        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\r\n        train_loss.update(loss.item())\r\n        curr_iter += 1\r\n        if (i + 1) % 100 == 0:\r\n            print(&#39;[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]&#39; % (\r\n                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\r\n            total_loss_train.append(train_loss.avg)\r\n            total_acc_train.append(train_acc.avg)\r\n    return train_loss.avg, train_acc.avg\r\n\r\ndef validate(val_loader, model, criterion, optimizer, epoch):\r\n    model.eval()\r\n    val_loss = AverageMeter()\r\n    val_acc = AverageMeter()\r\n    with torch.no_grad():\r\n        for i, data in enumerate(val_loader):\r\n            images, labels = data\r\n            N = images.size(0)\r\n            images = Variable(images).to(device)\r\n            labels = Variable(labels).to(device)\r\n\r\n            outputs = model(images)\r\n            prediction = outputs.max(1, keepdim=True)[1]\r\n\r\n            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\r\n\r\n            val_loss.update(criterion(outputs, labels).item())\r\n\r\n    print(&#39;------------------------------------------------------------&#39;)\r\n    print(&#39;[epoch %d], [val loss %.5f], [val acc %.5f]&#39; % (epoch, val_loss.avg, val_acc.avg))\r\n    print(&#39;------------------------------------------------------------&#39;)\r\n    return val_loss.avg, val_acc.avg\r\n\r\n\r\nimport cv2\r\nfrom PIL import Image, ImageOps\r\nimport numpy as np\r\n\r\nmodel = model_ft\r\nmodel.load_state_dict(torch.load(&quot;/content/drive/MyDrive/input/trainbest.pth&quot;))\r\nmodel.eval()\r\n\r\ndef import_and_predict(image_data, model):\r\n  size = (224, 224)\r\n  image = ImageOps.fit(image_data, size, Image.ANTIALIAS)\r\n  img = np.asarray(image)\r\n  image_reshape = img[np.newaxis,...]\r\n  prediction = model.predict(img_reshape)\r\n  return prediction\r\n\r\nimage = Image.open(&#39;/content/0365-0596-abd-88-05-0712-gf03.jpg&#39;)\r\n# st.image(image, use_column_width = True)\r\npredictions = import_and_predict(image, model)\r\nclass_names = [&quot;Melanocytic nevi&quot;, &quot;dermatofibroma&quot;, &quot;Benign keratosis-like lesions&quot;, &quot;Basal cell carcinoma&quot;, &quot;Actinic keratoses&quot;, &quot;Vascular lesions&quot;, &quot;Dermatofibroma&quot;]\r\nstring = &quot;It is: &quot; + class_names[np.argmax(predictions)]\r\nprint(string)\r\n```\r\nHere is the error that comes immediately after this is executed.\r\n```\r\n---------------------------------------------------------------------------\r\nModuleAttributeError                      Traceback (most recent call last)\r\n&lt;ipython-input-219-d563271b78c6&gt; in &lt;module&gt;()\r\n     32 image = Image.open(&#39;/content/0365-0596-abd-88-05-0712-gf03.jpg&#39;)\r\n     33 # st.image(image, use_column_width = True)\r\n---&gt; 34 predictions = import_and_predict(image, model)\r\n     35 class_names = [&quot;Melanocytic nevi&quot;, &quot;dermatofibroma&quot;, &quot;Benign keratosis-like lesions&quot;, &quot;Basal cell carcinoma&quot;, &quot;Actinic keratoses&quot;, &quot;Vascular lesions&quot;, &quot;Dermatofibroma&quot;]\r\n     36 string = &quot;It is: &quot; + class_names[np.argmax(predictions)]\r\n\r\n1 frames\r\n&lt;ipython-input-219-d563271b78c6&gt; in import_and_predict(image_data, model)\r\n     27   img = np.asarray(image)\r\n     28   image_reshape = img[np.newaxis,...]\r\n---&gt; 29   prediction = model.predict(img_reshape)\r\n     30   return prediction\r\n     31 \r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py in __getattr__(self, name)\r\n    777                 return modules[name]\r\n    778         raise ModuleAttributeError(&quot;&#39;{}&#39; object has no attribute &#39;{}&#39;&quot;.format(\r\n--&gt; 779             type(self).__name__, name))\r\n    780 \r\n    781     def __setattr__(self, name: str, value: Union[Tensor, &#39;Module&#39;]) -&gt; None:\r\n\r\nModuleAttributeError: &#39;ResNet&#39; object has no attribute &#39;predict&#39;\r\n```\r\nIf anyone can help me fix the issue and get this to work as a classifier for skin diseases, I would be ever so thankful. \r\n",
        "accepted_answer_markdown": "[`nn.Module`][1] don&#39;t have a `predict` function, just call the object for inference:\r\n\r\n    prediction = model(img_reshape)\r\n\r\nThis will call the object&#39;s `__call__` function which, in turns, callsthe model `forward` function.\r\n\r\n  [1]: https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
    },
    {
        "question_id": "65755730",
        "accepted_answer_id": "65757047",
        "question_title": "Estimating mixture of Gaussian models in Pytorch",
        "question_markdown": "I actually want to estimate a normalizing flow with a mixture of gaussians as the base distribution, so I&#39;m sort of stuck with torch.  However you can reproduce my error in my code by just estimating a mixture of Gaussian model in torch.  My code is below:\r\n\r\n```\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nimport sklearn.datasets as datasets\r\n\r\nimport torch\r\nfrom torch import nn\r\nfrom torch import optim\r\nimport torch.distributions as D\r\n\r\nnum_layers = 8\r\nweights = torch.ones(8,requires_grad=True).to(device)\r\nmeans = torch.tensor(np.random.randn(8,2),requires_grad=True).to(device)#torch.randn(8,2,requires_grad=True).to(device)\r\nstdevs = torch.tensor(np.abs(np.random.randn(8,2)),requires_grad=True).to(device)\r\nmix = D.Categorical(weights)\r\ncomp = D.Independent(D.Normal(means,stdevs), 1)\r\ngmm = D.MixtureSameFamily(mix, comp)\r\n\r\nnum_iter = 10001#30001\r\nnum_iter2 = 200001\r\nloss_max1 = 100\r\nfor i in range(num_iter):\r\n    x = torch.randn(5000,2)#this can be an arbitrary x samples\r\n    loss2 = -gmm.log_prob(x).mean()#-densityflow.log_prob(inputs=x).mean()\r\n    optimizer1.zero_grad()\r\n    loss2.backward()\r\n    optimizer1.step()\r\n```\r\n\r\nThe error I get is: \r\n\r\n```\r\n0\r\n8.089411823514835\r\nTraceback (most recent call last):\r\n\r\n  File &quot;/home/cameron/AnacondaProjects/gmm.py&quot;, line 183, in &lt;module&gt;\r\n    loss2.backward()\r\n\r\n  File &quot;/home/cameron/anaconda3/envs/torch/lib/python3.7/site-packages/torch/tensor.py&quot;, line 221, in backward\r\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n\r\n  File &quot;/home/cameron/anaconda3/envs/torch/lib/python3.7/site-packages/torch/autograd/__init__.py&quot;, line 132, in backward\r\n    allow_unreachable=True)  # allow_unreachable flag\r\n\r\nRuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.\r\n```\r\nAfter as you see the model runs for 1 iteration.  ",
        "accepted_answer_markdown": "There is ordering problem in your code, since you create Gaussian mixture model outside of training loop, then when calculate the loss the Gaussian mixture model will try to use the initial value of the parameters that you set when you define the model, but the `optimizer1.step()` already modify that value so even you set `loss2.backward(retain_graph=True)` there will still be the error: `RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation`\r\n\r\nSolution to this problem is simply create new Gaussian mixture model whenever you update the parameters, example code running as expected:\r\n\r\n    import numpy as np\r\n    import matplotlib.pyplot as plt\r\n    import sklearn.datasets as datasets\r\n    \r\n    import torch\r\n    from torch import nn\r\n    from torch import optim\r\n    import torch.distributions as D\r\n    \r\n    num_layers = 8\r\n    weights = torch.ones(8,requires_grad=True)\r\n    means = torch.tensor(np.random.randn(8,2),requires_grad=True)\r\n    stdevs = torch.tensor(np.abs(np.random.randn(8,2)),requires_grad=True)\r\n    \r\n    parameters = [weights, means, stdevs]\r\n    optimizer1 = optim.SGD(parameters, lr=0.001, momentum=0.9)\r\n    \r\n    num_iter = 10001\r\n    for i in range(num_iter):\r\n        mix = D.Categorical(weights)\r\n        comp = D.Independent(D.Normal(means,stdevs), 1)\r\n        gmm = D.MixtureSameFamily(mix, comp)\r\n\r\n        optimizer1.zero_grad()\r\n        x = torch.randn(5000,2)#this can be an arbitrary x samples\r\n        loss2 = -gmm.log_prob(x).mean()#-densityflow.log_prob(inputs=x).mean()\r\n        loss2.backward()\r\n        optimizer1.step()\r\n\r\n        print(i, loss2)\r\n\r\n"
    },
    {
        "question_id": "65945996",
        "accepted_answer_id": "65946386",
        "question_title": "1D CNN on Pytorch: mat1 and mat2 shapes cannot be multiplied (10x3 and 10x2)",
        "question_markdown": "I have a time series with sample of 500 size and 2 types of labels and want to construct a 1D CNN with pytorch on them:\r\n\r\n```\r\nclass Simple1DCNN(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Simple1DCNN, self).__init__()\r\n        self.layer1 = torch.nn.Conv1d(in_channels=50, \r\n                                      out_channels=20, \r\n                                      kernel_size=5, \r\n                                      stride=2)\r\n        self.act1 = torch.nn.ReLU()\r\n        self.layer2 = torch.nn.Conv1d(in_channels=20, \r\n                                      out_channels=10, \r\n                                      kernel_size=1)\r\n        \r\n        self.fc1 = nn.Linear(10* 1 * 1, 2)\r\n    def forward(self, x):\r\n        x = x.view(1, 50,-1)\r\n        x = self.layer1(x)\r\n        x = self.act1(x)\r\n        x = self.layer2(x)\r\n        x = self.fc1(x)\r\n        \r\n        return x\r\n\r\nmodel = Simple1DCNN()\r\nmodel(torch.tensor(np.random.uniform(-10, 10, 500)).float())\r\n```\r\nBut got this error message: \r\n```\r\nTraceback (most recent call last):\r\n  File &quot;so_pytorch.py&quot;, line 28, in &lt;module&gt;\r\n    model(torch.tensor(np.random.uniform(-10, 10, 500)).float())\r\n  File &quot;/Users/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File &quot;so_pytorch.py&quot;, line 23, in forward\r\n    x = self.fc1(x)\r\n  File &quot;/Users/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File &quot;/Users/lib/python3.8/site-packages/torch/nn/modules/linear.py&quot;, line 93, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File &quot;/Users/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 1692, in linear\r\n    output = input.matmul(weight.t())\r\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x3 and 10x2)\r\n```\r\nwhat am I doing wrong?",
        "accepted_answer_markdown": "The shape of the output of the line `x = self.layer2(x)` (which is also the input of the next line `x = self.fc1(x)`) is `torch.Size([1, 10, 3])`.\r\n\r\nNow from the definition of `self.fc1`, it expects the last dimension of it&#39;s input to be `10 * 1 * 1` which is `10` whereas your input has `3` hence the error.\r\n\r\nI don&#39;t know what it is you&#39;re trying to do, but assuming what you want to do is;\r\n\r\n1. label the entire `500` size sequence to one of two labels, the you do this.\r\n\r\n```\r\n# replace self.fc1 = nn.Linear(10* 1 * 1, 2) with\r\nself.fc1 = nn.Linear(10 * 3, 2)\r\n\r\n# replace x = self.fc1(x) with\r\nx = x.view(1, -1)\r\nx = self.fc1(x)\r\n```\r\n\r\n2. label `10` timesteps each to one of two labels, then you do this.\r\n```\r\n# replace self.fc1 = nn.Linear(10* 1 * 1, 2) with\r\nself.fc1 = nn.Linear(2, 2)\r\n```\r\n\r\nThe output shape for **1** will be (batch size, 2), and for **2** will be (batch size, 10, 2)."
    },
    {
        "question_id": "65951522",
        "accepted_answer_id": "65951759",
        "question_title": "Pytorch: 1D target tensor expected, multi-target not supported",
        "question_markdown": "I want to train a 1D CNN on time series. I get the following error message `1D target tensor expected, multi-target not supported`\r\n\r\nHere is the code with simulated data corresponding to the structures of my data as well as the error message\r\n```\r\nimport torch\r\nfrom torch.utils.data import DataLoader\r\nimport torch.utils.data as data\r\nimport torch.nn as nn\r\n\r\nimport numpy as np\r\nimport random\r\nfrom tqdm.notebook import tqdm\r\n\r\ndevice = torch.device(&#39;cuda&#39; if torch.cuda.is_available() else &#39;cpu&#39;)\r\nprint(device)\r\n\r\n\r\ntrain_dataset = []\r\nn_item = 20\r\nfor i in range(0,n_item):\r\n    train_data = np.random.uniform(-10, 10, 500)\r\n    train_dataset.append(train_data)\r\ntrain_dataset = np.asarray(train_dataset)\r\ntrain_dataset.shape\r\necg_train = torch.from_numpy(train_dataset).float()\r\nlabels_train = np.random.randint(2, size=n_item)\r\nlabels_train = torch.from_numpy(labels_train).long()\r\n\r\n\r\nval_dataset = []\r\nn_item = 10\r\nfor i in range(0,n_item):\r\n    val_data = np.random.uniform(-10, 10, 500)\r\n    val_dataset.append(val_data)\r\nval_dataset = np.asarray(val_dataset)\r\nval_dataset.shape\r\necg_validation = torch.from_numpy(val_dataset).float()\r\nlabels_validation = np.random.randint(2, size=n_item)\r\nlabels_validation = torch.from_numpy(labels_validation).long()\r\n\r\nclass ECGNet(data.Dataset):\r\n    &quot;&quot;&quot;ImageNet Limited dataset.&quot;&quot;&quot;\r\n    \r\n    def __init__(self, ecgs, labls, transform=None):\r\n            self.ecg = ecgs\r\n            self.target = labls\r\n            self.transform = transform\r\n\r\n    def __getitem__(self, idx):\r\n        ecgVec = self.ecg[idx] #.reshape(10, -1)\r\n        labelID = self.target[idx].reshape(1)\r\n\r\n        return ecgVec,labelID\r\n\r\n    def __len__(self):\r\n        return len(self.ecg)\r\n\r\n\r\ntrain_data = ECGNet(ecg_train, \r\n                             labels_train, \r\n                   )\r\nprint(&quot;size of Training dataset: {}&quot;.format(len(train_data)))\r\n\r\nvalidation_data = ECGNet(ecg_validation, \r\n                             labels_validation, \r\n                   )\r\nprint(&quot;size of Training dataset: {}&quot;.format(len(validation_data)))\r\n\r\n\r\nbatch_size = 1\r\ntrain_dataloader = DataLoader(dataset = train_data,\r\n                              batch_size=batch_size, \r\n                              shuffle = True, \r\n                              num_workers = 0)\r\n\r\nval_dataloader = DataLoader(dataset = validation_data,\r\n                              batch_size=batch_size, \r\n                              shuffle = True, \r\n                              num_workers = 0)\r\n\r\n\r\ndef train_epoch(model, train_dataloader, optimizer, loss_fn):\r\n    losses = []\r\n    correct_predictions = 0\r\n    # Iterate mini batches over training dataset\r\n    for images, labels in tqdm(train_dataloader):\r\n        images = images.to(device)\r\n\t#labels = labels.squeeze_()\r\n        labels = labels.to(device)\r\n\r\n        #labels = labels.to(device=device, dtype=torch.int64)\r\n        # Run predictions\r\n        output = model(images)\r\n        # Set gradients to zero\r\n        optimizer.zero_grad()\r\n        # Compute loss\r\n        loss = loss_fn(output, labels)\r\n        # Backpropagate (compute gradients)\r\n        loss.backward()\r\n        # Make an optimization step (update parameters)\r\n        optimizer.step()\r\n        # Log metrics\r\n        losses.append(loss.item())\r\n        predicted_labels = output.argmax(dim=1)\r\n        correct_predictions += (predicted_labels == labels).sum().item()\r\n    accuracy = 100.0 * correct_predictions / len(train_dataloader.dataset)\r\n    # Return loss values for each iteration and accuracy\r\n    mean_loss = np.array(losses).mean()\r\n    return mean_loss, accuracy\r\n\r\ndef evaluate(model, dataloader, loss_fn):\r\n    losses = []\r\n    correct_predictions = 0\r\n    with torch.no_grad():\r\n        for images, labels in dataloader:\r\n            images = images.to(device)\r\n            #labels = labels.squeeze_()\r\n            labels = labels.to(device=device, dtype=torch.int64)\r\n            # Run predictions\r\n            output = model(images)\r\n            # Compute loss\r\n            loss = loss_fn(output, labels)\r\n            # Save metrics\r\n            predicted_labels = output.argmax(dim=1)\r\n            correct_predictions += (predicted_labels == labels).sum().item()\r\n            losses.append(loss.item())\r\n    mean_loss = np.array(losses).mean()\r\n    accuracy = 100.0 * correct_predictions / len(dataloader.dataset)\r\n    # Return mean loss and accuracy\r\n    return mean_loss, accuracy\r\n\r\ndef train(model, train_dataloader, val_dataloader, optimizer, n_epochs, loss_function):\r\n    # We will monitor loss functions as the training progresses\r\n    train_losses = []\r\n    val_losses = []\r\n    train_accuracies = []\r\n    val_accuracies = []\r\n\r\n    for epoch in range(n_epochs):\r\n        model.train()\r\n        train_loss, train_accuracy = train_epoch(model, train_dataloader, optimizer, loss_fn)\r\n        model.eval()\r\n        val_loss, val_accuracy = evaluate(model, val_dataloader, loss_fn)\r\n        train_losses.append(train_loss)\r\n        val_losses.append(val_loss)\r\n        train_accuracies.append(train_accuracy)\r\n        val_accuracies.append(val_accuracy)\r\n        print(&#39;Epoch {}/{}: train_loss: {:.4f},        train_accuracy: {:.4f}, val_loss: {:.4f},        val_accuracy: {:.4f}&#39;.format(epoch+1, n_epochs,\r\n                                     train_losses[-1],\r\n                                     train_accuracies[-1],\r\n                                     val_losses[-1],\r\n                                     val_accuracies[-1]))\r\n    return train_losses, val_losses, train_accuracies, val_accuracies\r\n\r\n\r\nclass Simple1DCNN(torch.nn.Module):\r\n    def __init__(self):\r\n        super(Simple1DCNN, self).__init__()\r\n        self.layer1 = torch.nn.Conv1d(in_channels=50, \r\n                                      out_channels=20, \r\n                                      kernel_size=5, \r\n                                      stride=2)\r\n        self.act1 = torch.nn.ReLU()\r\n        self.layer2 = torch.nn.Conv1d(in_channels=20, \r\n                                      out_channels=10, \r\n                                      kernel_size=1)\r\n        \r\n        self.fc1 = nn.Linear(10* 3, 2)\r\n    def forward(self, x):\r\n        print(x.shape)\r\n        x = x.view(1, 50,-1)\r\n        print(x.shape)\r\n        x = self.layer1(x)\r\n        print(x.shape)\r\n        x = self.act1(x)\r\n        print(x.shape)\r\n        x = self.layer2(x)\r\n        print(x.shape)\r\n        x = x.view(1,-1)\r\n        print(x.shape)\r\n        x = self.fc1(x)\r\n        print(x.shape)\r\n        print(x)\r\n        \r\n        return x\r\n\r\n\r\n\r\nmodel_a = Simple1DCNN()\r\nmodel_a = model_a.to(device)\r\n\r\ncriterion = nn.CrossEntropyLoss()\r\n\r\nloss_fn = torch.nn.CrossEntropyLoss()\r\nn_epochs_a = 50\r\nlearning_rate_a = 0.01\r\nalpha_a = 1e-5\r\nmomentum_a = 0.9\r\noptimizer = torch.optim.SGD(model_a.parameters(), \r\n                            momentum = momentum_a,\r\n                            nesterov = True,\r\n                            weight_decay = alpha_a,\r\n                            lr=learning_rate_a)\r\ntrain_losses_a, val_losses_a, train_acc_a, val_acc_a = train(model_a,\r\n                                                             train_dataloader,\r\n                                                             val_dataloader,\r\n                                                             optimizer,\r\n                                                             n_epochs_a,\r\n                                                             loss_fn\r\n                                                            )\r\n\r\n\r\n\r\n```\r\nError message:\r\n```\r\ncpu\r\nsize of Training dataset: 20\r\nsize of Training dataset: 10\r\n  0%|          | 0/20 [00:00&lt;?, ?it/s]\r\ntorch.Size([1, 500])\r\ntorch.Size([1, 50, 10])\r\ntorch.Size([1, 20, 3])\r\ntorch.Size([1, 20, 3])\r\ntorch.Size([1, 10, 3])\r\ntorch.Size([1, 30])\r\ntorch.Size([1, 2])\r\ntensor([[ 0.5785, -1.0169]], grad_fn=&lt;AddmmBackward&gt;)\r\nTraceback (most recent call last):\r\n  File &quot;SO_question.py&quot;, line 219, in &lt;module&gt;\r\n    train_losses_a, val_losses_a, train_acc_a, val_acc_a = train(model_a,\r\n  File &quot;SO_question.py&quot;, line 137, in train\r\n    train_loss, train_accuracy = train_epoch(model, train_dataloader, optimizer, loss_fn)\r\n  File &quot;SO_question.py&quot;, line 93, in train_epoch\r\n    loss = loss_fn(output, labels)\r\n  File &quot;/Users/mymac/Documents/programming/python/mainenv/lib/python3.8/site-packages/torch/nn/modules/module.py&quot;, line 727, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File &quot;/Users/mymac/Documents/programming/python/mainenv/lib/python3.8/site-packages/torch/nn/modules/loss.py&quot;, line 961, in forward\r\n    return F.cross_entropy(input, target, weight=self.weight,\r\n  File &quot;/Users/mymac/Documents/programming/python/mainenv/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 2468, in cross_entropy\r\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n  File &quot;/Users/mymac/Documents/programming/python/mainenv/lib/python3.8/site-packages/torch/nn/functional.py&quot;, line 2264, in nll_loss\r\n    ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\nRuntimeError: 1D target tensor expected, multi-target not supported\r\n```\r\n\r\nWhat am I doing wrong?",
        "accepted_answer_markdown": "You are using `nn.CrossEntropyLoss` as the criterion for your training. You correctly passed the labels as indices of the ground truth class: `0`s and `1`s. However, as the error message suggests, it needs to be a 1D tensor!\r\n\r\nSimply remove the reshape in `ECGNet`&#39;s `__getitem__`:\r\n\r\n    def __getitem__(self, idx):\r\n        ecgVec = self.ecg[idx]\r\n        labelID = self.target[idx]\r\n        return ecgVec,labelID\r\n\r\n---\r\n\r\n*Edit*\r\n\r\n&gt;  I want to increase the `batch_size` to *8*. But now I get the error [...]\r\n\r\n\r\nYou are doing a lot of broadcasting (flattening) which surely will affect the batch size. As a general rule of thumb never fiddle with `axis=0`. For instance, if you have an input shape of `(8, 500)`, straight off you have a problem when doing `x.view(1, 50, -1)`. Since the resulting tensor will be `(1, 50, 80)` (the desired shape would have been `(8, 50, 10)`). Instead, you could broadcast with `x.view(x.size(0), 50, -1)`.\r\n\r\nSame with `x.view(1, -1)` later down `forward`. You are looking to flatten the tensor, but you should **not** flatten it along with the batches, they need to stay separated! It&#39;s safer to use [`torch.flatten`][1], yet I prefer [`nn.Flatten`][2] which flattens from `axis=1` to `axis=-1` by default.\r\n\r\n---\r\n\r\nMy personal advice is to start with a simple setup (without train loops etc...) to verify the architecture and intermediate output shapes. Then, add the necessary logic to handle the training.\r\n\r\n    class ECGNet(data.Dataset):\r\n        &quot;&quot;&quot;ImageNet Limited dataset.&quot;&quot;&quot;\r\n        \r\n        def __init__(self, ecgs, labls, transform=None):\r\n            self.ecg = ecgs\r\n            self.target = labls\r\n            self.transform = transform\r\n    \r\n        def __getitem__(self, idx):\r\n            ecgVec = self.ecg[idx]\r\n            labelID = self.target[idx]\r\n            return ecgVec, labelID\r\n    \r\n        def __len__(self):\r\n            return len(self.ecg)\r\n    \r\n    \r\n    class Simple1DCNN(nn.Module):\r\n        def __init__(self):\r\n            super(Simple1DCNN, self).__init__()\r\n            self.layer1 = nn.Conv1d(in_channels=50, \r\n                                    out_channels=20, \r\n                                    kernel_size=5, \r\n                                    stride=2)\r\n            self.act1 = nn.ReLU()\r\n            self.layer2 = nn.Conv1d(in_channels=20, \r\n                                    out_channels=10, \r\n                                    kernel_size=1)\r\n            \r\n            self.fc1 = nn.Linear(10*3, 2)\r\n            self.flatten = nn.Flatten()\r\n    \r\n        def forward(self, x):\r\n            x = x.view(x.size(0), 50, -1)\r\n            x = self.layer1(x)\r\n            x = self.act1(x)\r\n            x = self.layer2(x)\r\n            x = self.flatten(x)\r\n            x = self.fc1(x)\r\n            \r\n            return x\r\n    \r\n    batch_size = 8\r\n    train_data = ECGNet(ecg_train, labels_train)\r\n    train_dl = DataLoader(dataset=train_data,\r\n                          batch_size=batch_size, \r\n                          shuffle=True,\r\n                          num_workers=0)\r\n    \r\n    model = Simple1DCNN()\r\n    criterion = nn.CrossEntropyLoss()\r\n\r\nThen \r\n\r\n\r\n    &gt;&gt;&gt; x, y = next(iter(train_dl))\r\n    &gt;&gt;&gt; y_hat = model(x)\r\n\r\n    &gt;&gt;&gt; y_hat.shape\r\n    torch.Size([8, 2])\r\n\r\nAlso, make sure your loss works:\r\n\r\n    &gt;&gt;&gt; criterion(y_hat, y)\r\n    tensor(..., grad_fn=&lt;NllLossBackward&gt;)\r\n\r\n  [1]: https://pytorch.org/docs/stable/generated/torch.flatten.html\r\n  [2]: https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html"
    },
    {
        "question_id": "66037566",
        "accepted_answer_id": "66037822",
        "question_title": "How to save a model using DefaultTrainer in Detectron2?",
        "question_markdown": "How can I save a checkpoint in Detectron2, using a DefaultTrainer? \r\nThis is my setup:\r\n\r\n    cfg = get_cfg()\r\n    cfg.merge_from_file(model_zoo.get_config_file(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;))\r\n    \r\n    cfg.DATASETS.TRAIN = (DatasetLabels.TRAIN,)\r\n    cfg.DATASETS.TEST = ()\r\n    cfg.DATALOADER.NUM_WORKERS = 2\r\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 273  # Number of output classes\r\n    \r\n    cfg.OUTPUT_DIR = &quot;outputs&quot;\r\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\r\n    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(&quot;COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml&quot;)\r\n    cfg.SOLVER.IMS_PER_BATCH = 2\r\n    cfg.SOLVER.BASE_LR = 0.00025#0.00025  # Learning Rate\r\n    cfg.SOLVER.MAX_ITER = 10000  # 20000 MAx Iterations\r\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # Batch Size\r\n    \r\n    trainer = DefaultTrainer(cfg) \r\n    trainer.resume_or_load(resume=False)\r\n    trainer.train()\r\n\r\n\r\n    # Save the model\r\n    from detectron2.checkpoint import DetectionCheckpointer, Checkpointer\r\n    checkpointer = DetectionCheckpointer(trainer, save_dir=cfg.OUTPUT_DIR)\r\n    checkpointer.save(&quot;mymodel_0&quot;)  \r\n\r\nI get the error: \r\n\r\n    ---------------------------------------------------------------------------\r\n    AttributeError                            Traceback (most recent call last)\r\n    &lt;ipython-input-94-c1116902655a&gt; in &lt;module&gt;()\r\n    \r\n          4 checkpointer = DetectionCheckpointer(trainer, save_dir=cfg.OUTPUT_DIR)\r\n    ----&gt; 5 checkpointer.save(&quot;mymodel_0&quot;)\r\n    \r\n    /usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py in save(self, name, **kwargs)\r\n        102 \r\n        103         data = {}\r\n    --&gt; 104         data[&quot;model&quot;] = self.model.state_dict()\r\n        105         for key, obj in self.checkpointables.items():\r\n        106             data[key] = obj.state_dict()\r\n    \r\n    AttributeError: &#39;DefaultTrainer&#39; object has no attribute &#39;state_dict&#39;\r\n\r\n\r\nDocs: https://detectron2.readthedocs.io/en/latest/modules/checkpoint.html",
        "accepted_answer_markdown": "    checkpointer = DetectionCheckpointer(trainer.model, save_dir=cfg.OUTPUT_DIR)\r\n\r\nis the way to go.\r\n\r\nAlternatively:\r\n\r\n    torch.save(trainer.model.state_dict(), os.path.join(cfg.OUTPUT_DIR, &quot;mymodel.pth&quot;))"
    },
    {
        "question_id": "66074684",
        "accepted_answer_id": "66075139",
        "question_title": "&quot;RuntimeError: expected scalar type Double but found Float&quot; in Pytorch CNN training",
        "question_markdown": "   \r\n\r\nI just begin to learn Pytorch and create my first CNN. The dataset contains 3360 RGB images and I converted them to a `[3360, 3, 224, 224]` tensor. The data and label are in the `dataset(torch.utils.data.TensorDataset)`. Below is the training code.\r\n\r\n```python\r\n    def train_net():\r\n        dataset = ld.load()\r\n        data_iter = Data.DataLoader(dataset, batch_size=168, shuffle=True)\r\n        net = model.VGG_19()\r\n        summary(net, (3, 224, 224), device=&quot;cpu&quot;)\r\n        loss_func = nn.CrossEntropyLoss()\r\n        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, dampening=0.1)\r\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\r\n        for epoch in range(5):\r\n            print(&quot;epoch:&quot;, epoch + 1)\r\n            train_loss = 0\r\n            for i, data in enumerate(data_iter, 0):\r\n                x, y = data\r\n                print(x.dtype)\r\n                optimizer.zero_grad()\r\n                out = net(x)\r\n                loss = loss_func(out, y)\r\n                loss.backward()\r\n                optimizer.step()\r\n                train_loss += loss.item()\r\n                if i % 100 == 99:\r\n                    print(&quot;loss:&quot;, train_loss / 100)\r\n                    train_loss = 0.0\r\n        print(&quot;finish train&quot;)\r\n```\r\nThen I have this error:\r\n```\r\n    Traceback (most recent call last):\r\n              File &quot;D:/python/DeepLearning/VGG/train.py&quot;, line 52, in &lt;module&gt;\r\n                train_net()\r\n              File &quot;D:/python/DeepLearning/VGG/train.py&quot;, line 29, in train_net\r\n                out = net(x)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 727, in _call_impl\r\n                result = self.forward(*input, **kwargs)\r\n              File &quot;D:\\python\\DeepLearning\\VGG\\model.py&quot;, line 37, in forward\r\n                out = self.conv3_64(x)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 727, in _call_impl\r\n                result = self.forward(*input, **kwargs)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\container.py&quot;, line 117, in forward\r\n                input = module(input)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 727, in _call_impl\r\n                result = self.forward(*input, **kwargs)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py&quot;, line 423, in forward\r\n                return self._conv_forward(input, self.weight)\r\n              File &quot;D:\\python\\lib\\site-packages\\torch\\nn\\modules\\conv.py&quot;, line 419, in _conv_forward\r\n                return F.conv2d(input, weight, self.bias, self.stride,\r\n            RuntimeError: expected scalar type Double but found Float\r\n```\r\nI think there is something wrong with x and I print its type by `print(x.dtype)`:\r\n\r\n    torch.float64\r\n\r\nwhich is double instead of float. Do you know what`s wrong? Thanks for your help!\r\n",
        "accepted_answer_markdown": "that error is actually refering to the weights of the conv layer which are in `float32` by default when the matrix multiplication is called. Since your input is `double`(`float64` in pytorch)  while the weights in conv are `float`  \r\nSo the solution in your case is : \r\n\r\n    def train_net():\r\n        dataset = ld.load()\r\n        data_iter = Data.DataLoader(dataset, batch_size=168, shuffle=True)\r\n        net = model.VGG_19()\r\n        summary(net, (3, 224, 224), device=&quot;cpu&quot;)\r\n        loss_func = nn.CrossEntropyLoss()\r\n        optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, dampening=0.1)\r\n        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\r\n        for epoch in range(5):\r\n            print(&quot;epoch:&quot;, epoch + 1)\r\n            train_loss = 0\r\n            for i, data in enumerate(data_iter, 0):\r\n                x, y = data                        #      //_______________\r\n                x = x.float()    # HERE IS THE CHANGE     \\\\\r\n                print(x.dtype)\r\n                optimizer.zero_grad()\r\n                out = net(x)\r\n                loss = loss_func(out, y)\r\n                loss.backward()\r\n                optimizer.step()\r\n                train_loss += loss.item()\r\n                if i % 100 == 99:\r\n                    print(&quot;loss:&quot;, train_loss / 100)\r\n                    train_loss = 0.0\r\n        print(&quot;finish train&quot;)\r\n\r\n\r\nThis will work for sure"
    },
    {
        "question_id": "66203862",
        "accepted_answer_id": "66214159",
        "question_title": "Pytorch model running out of memory on both CPU and GPU, can\u2019t figure out what I\u2019m doing wrong",
        "question_markdown": "Trying to implement a simple multi-label image classifier using Pytorch Lightning. Here&#39;s the model definition:\r\n\r\n```\r\nimport torch\r\nfrom torch import nn\r\n\r\n# creates network class\r\nclass Net(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # defines conv layers\r\n        self.conv_layer_b1 = nn.Sequential(\r\n            nn.Conv2d(in_channels=3, out_channels=32,\r\n                      kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2),\r\n            nn.Flatten(),\r\n        )\r\n\r\n        # passes dummy x matrix to find the input size of the fc layer\r\n        x = torch.randn(1, 3, 800, 600)\r\n        self._to_linear = None\r\n        self.forward(x)\r\n\r\n        # defines fc layer\r\n        self.fc_layer = nn.Sequential(\r\n            nn.Linear(in_features=self._to_linear,\r\n                      out_features=256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 5),\r\n        )\r\n\r\n        # defines accuracy metric\r\n        self.accuracy = pl.metrics.Accuracy()\r\n        self.confusion_matrix = pl.metrics.ConfusionMatrix(num_classes=5)\r\n\r\n    def forward(self, x):\r\n        x = self.conv_layer_b1(x)\r\n\r\n        if self._to_linear is None:\r\n            # does not run fc layer if input size is not determined yet\r\n            self._to_linear = x.shape[1]\r\n        else:\r\n            x = self.fc_layer(x)\r\n        return x\r\n\r\n    def cross_entropy_loss(self, logits, y):\r\n        criterion = nn.CrossEntropyLoss()\r\n        \r\n        return criterion(logits, y)\r\n\r\n    def training_step(self, train_batch, batch_idx):\r\n        x, y = train_batch\r\n        logits = self.forward(x)\r\n\r\n        train_loss = self.cross_entropy_loss(logits, y)\r\n        train_acc = self.accuracy(logits, y)\r\n        train_cm = self.confusion_matrix(logits, y)\r\n\r\n        self.log(&#39;train_loss&#39;, train_loss)\r\n        self.log(&#39;train_acc&#39;, train_acc)\r\n        self.log(&#39;train_cm&#39;, train_cm)\r\n\r\n        return train_loss\r\n\r\n    def validation_step(self, val_batch, batch_idx):\r\n        x, y = val_batch\r\n        logits = self.forward(x)\r\n\r\n        val_loss = self.cross_entropy_loss(logits, y)\r\n        val_acc = self.accuracy(logits, y)\r\n\r\n        return {&#39;val_loss&#39;: val_loss, &#39;val_acc&#39;: val_acc}\r\n\r\n    def validation_epoch_end(self, outputs):\r\n        avg_val_loss = torch.stack([x[&#39;val_loss&#39;] for x in outputs]).mean()\r\n        avg_val_acc = torch.stack([x[&#39;val_acc&#39;] for x in outputs]).mean()\r\n\r\n        self.log(&quot;val_loss&quot;, avg_val_loss)\r\n        self.log(&quot;val_acc&quot;, avg_val_acc)\r\n\r\n    def configure_optimizers(self):\r\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0008)\r\n\r\n        return optimizer\r\n```\r\n\r\nThe issue is probably not the machine since I&#39;m using a cloud instance with 60 GBs of RAM and 12 GBs of VRAM. Whenever I run this model even for a single epoch, I get an out of memory error. On the CPU it looks like this:\r\n\r\n`RuntimeError: [enforce fail at CPUAllocator.cpp:64] . DefaultCPUAllocator: can&#39;t allocate memory: you tried to allocate 1966080000 bytes. Error code 12 (Cannot allocate memory)`\r\n\r\nand on the GPU it looks like this:\r\n\r\n```\r\nRuntimeError: CUDA out of memory. Tried to allocate 7.32 GiB (GPU 0; 11.17 GiB total capacity; 4.00 KiB already allocated; 2.56 GiB free; 2.00 MiB reserved in total by PyTorch)\r\n```\r\n\r\nClearing the cache and reducing the batch size did not work. I&#39;m a novice so clearly something here is exploding but I can&#39;t tell what. Any help would be appreciated.\r\n\r\nThank you!",
        "accepted_answer_markdown": "Indeed, it&#39;s not a machine issue; the model itself is simply unreasonably big. Typically, if you take a look at common CNN models, the fc layers occur near the end, after the inputs already pass through quite a few convolutional blocks (and have their spatial resolutions reduced).\r\n\r\nAssuming inputs are of shape `(batch, 3, 800, 600)`, while passing the `conv_layer_b1` layer, the feature map shape would be `(batch, 32, 400, 300)` after the `MaxPool` operation. After flattening, the inputs become `(batch, 32 * 400 * 300)`, ie, `(batch, 3840000)`.\r\n\r\nThe immediately following `fc_layer` thus contains `nn.Linear(3840000, 256)`, which is simply absurd. This single linear layer contains ~983 million trainable parameters! For reference, popular image classification CNNs roughly have 3 to 30 million parameters on average, with larger variants reaching 60 to 80 million. Few ever really cross the 100 million mark.\r\n\r\nYou can count your model params with this:\r\n\r\n```\r\ndef count_params(model):\r\n    return sum(map(lambda p: p.data.numel(), model.parameters()))\r\n```\r\n\r\nMy advice: 800 x 600 is really a massive input size. Reduce it to something like 400 x 300, if possible. Furthermore, add several convolutional blocks similar to `conv_layer_b1`, before the FC layer. For example:\r\n\r\n```\r\ndef get_conv_block(C_in, C_out):\r\n    return nn.Sequential(\r\n            nn.Conv2d(in_channels=C_in, out_channels=C_out,\r\n                      kernel_size=3, padding=1),\r\n            nn.ReLU(),\r\n            nn.MaxPool2d(kernel_size=2, stride=2)\r\n        )\r\n\r\nclass Net(pl.LightningModule):\r\n    def __init__(self):\r\n        super().__init__()\r\n\r\n        # defines conv layers\r\n        self.conv_layer_b1 = get_conv_block(3, 16)\r\n        self.conv_layer_b2 = get_conv_block(16, 32)\r\n        self.conv_layer_b3 = get_conv_block(32, 64)\r\n        self.conv_layer_b4 = get_conv_block(64, 128)\r\n        self.conv_layer_b5 = get_conv_block(128, 256)\r\n\r\n        # passes dummy x matrix to find the input size of the fc layer\r\n        x = torch.randn(1, 3, 800, 600)\r\n        self._to_linear = None\r\n        self.forward(x)\r\n\r\n        # defines fc layer\r\n        self.fc_layer = nn.Sequential(\r\n            nn.Flatten(),\r\n            nn.Linear(in_features=self._to_linear,\r\n                      out_features=256),\r\n            nn.ReLU(),\r\n            nn.Linear(256, 5)\r\n        )\r\n\r\n        # defines accuracy metric\r\n        self.accuracy = pl.metrics.Accuracy()\r\n        self.confusion_matrix = pl.metrics.ConfusionMatrix(num_classes=5)\r\n\r\n    def forward(self, x):\r\n        \r\n        x = self.conv_layer_b1(x)\r\n        x = self.conv_layer_b2(x)\r\n        x = self.conv_layer_b3(x)\r\n        x = self.conv_layer_b4(x)\r\n        x = self.conv_layer_b5(x)\r\n        \r\n        if self._to_linear is None:\r\n            # does not run fc layer if input size is not determined yet\r\n            self._to_linear = nn.Flatten()(x).shape[1]\r\n        else:\r\n            x = self.fc_layer(x)\r\n        return x\r\n```\r\n\r\nHere, because more conv-relu-pool layers are applied, the input is reduced to a feature map of a much smaller shape, `(batch, 256, 25, 18)`, and the overall number of trainable parameters would be reduced to about ~30 million parameters."
    },
    {
        "question_id": "66337562",
        "accepted_answer_id": "66425941",
        "question_title": "UnpicklingError: A load persistent id instruction was encountered, but no persistent_load function was specified",
        "question_markdown": "I was trying to run a python file named `api.py`. In this file, I&#39;m loading the pickle file of the Deep Learning model that was built and trained using PyTorch.\r\n\r\n[api.py][1]\r\nIn `api.py` the below-given functions are the most important ones.\r\n\r\n```\r\ndef load_model_weights(model_architecture, weights_path):\r\n  if os.path.isfile(weights_path):\r\n      cherrypy.log(&quot;CHERRYPYLOG Loading model from: {}&quot;.format(weights_path))\r\n      model_architecture.load_state_dict(torch.load(weights_path))\r\n  else:\r\n      raise ValueError(&quot;Path not found {}&quot;.format(weights_path))\r\n\r\n        \r\ndef load_recommender(vector_dim, hidden, activation, dropout, weights_path):\r\n\r\n    rencoder_api = model.AutoEncoder(layer_sizes=[vector_dim] + [int(l) for l in hidden.split(&#39;,&#39;)],\r\n                               nl_type=activation,\r\n                               is_constrained=False,\r\n                               dp_drop_prob=dropout,\r\n                               last_layer_activations=False)\r\n    load_model_weights(rencoder_api, weights_path) \r\n    rencoder_api.eval()\r\n    rencoder_api = rencoder_api.cuda()\r\n    return rencoder_api\r\n```\r\n        \r\n\r\nThe directory structure\r\n```none\r\n&#128230;MP1\r\n \u2523 &#128194;.ipynb_checkpoints\r\n \u2503 \u2517 &#128220;RS_netflix3months_100epochs_64,128,128-checkpoint.ipynb\r\n \u2523 &#128194;data\r\n \u2503 \u2523 &#128220;AutoEncoder.png\r\n \u2503 \u2523 &#128220;collaborative_filtering.gif\r\n \u2503 \u2523 &#128220;movie_titles.txt\r\n \u2503 \u2517 &#128220;shut_up.gif\r\n \u2523 &#128194;DeepRecommender\r\n \u2503 \u2523 &#128194;data_utils\r\n \u2503 \u2503 \u2523 &#128220;movielens_data_convert.py\r\n \u2503 \u2503 \u2517 &#128220;netflix_data_convert.py\r\n \u2503 \u2523 &#128194;reco_encoder\r\n \u2503 \u2503 \u2523 &#128194;data\r\n \u2503 \u2503 \u2503 \u2523 &#128194;__pycache__\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;input_layer.cpython-37.pyc\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;input_layer_api.cpython-37.pyc\r\n \u2503 \u2503 \u2503 \u2503 \u2517 &#128220;__init__.cpython-37.pyc\r\n \u2503 \u2503 \u2503 \u2523 &#128220;input_layer.py\r\n \u2503 \u2503 \u2503 \u2523 &#128220;input_layer_api.py\r\n \u2503 \u2503 \u2503 \u2517 &#128220;__init__.py\r\n \u2503 \u2503 \u2523 &#128194;model\r\n \u2503 \u2503 \u2503 \u2523 &#128194;__pycache__\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;model.cpython-37.pyc\r\n \u2503 \u2503 \u2503 \u2503 \u2517 &#128220;__init__.cpython-37.pyc\r\n \u2503 \u2503 \u2503 \u2523 &#128220;model.py\r\n \u2503 \u2503 \u2503 \u2517 &#128220;__init__.py\r\n \u2503 \u2503 \u2523 &#128194;__pycache__\r\n \u2503 \u2503 \u2503 \u2517 &#128220;__init__.cpython-37.pyc\r\n \u2503 \u2503 \u2517 &#128220;__init__.py\r\n \u2503 \u2523 &#128194;__pycache__\r\n \u2503 \u2503 \u2517 &#128220;__init__.cpython-37.pyc\r\n \u2503 \u2523 &#128220;compute_RMSE.py\r\n \u2503 \u2523 &#128220;infer.py\r\n \u2503 \u2523 &#128220;run.py\r\n \u2503 \u2517 &#128220;__init__.py\r\n \u2523 &#128194;model_save\r\n \u2503 \u2523 &#128194;model.epoch_99\r\n \u2503 \u2503 \u2517 &#128194;archive\r\n \u2503 \u2503 \u2503 \u2523 &#128194;data\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92901648\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92901728\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92901808\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92901888\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92901968\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902048\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902128\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902208\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902288\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902368\r\n \u2503 \u2503 \u2503 \u2503 \u2523 &#128220;92902448\r\n \u2503 \u2503 \u2503 \u2503 \u2517 &#128220;92902608\r\n \u2503 \u2503 \u2503 \u2523 &#128220;data.pkl\r\n \u2503 \u2503 \u2503 \u2517 &#128220;version\r\n \u2503 \u2523 &#128220;model.epoch_99.zip\r\n \u2503 \u2517 &#128220;model.onnx\r\n \u2523 &#128194;Netflix\r\n \u2503 \u2523 &#128194;N1Y_TEST\r\n \u2503 \u2503 \u2517 &#128220;n1y.test.txt\r\n \u2503 \u2523 &#128194;N1Y_TRAIN\r\n \u2503 \u2503 \u2517 &#128220;n1y.train.txt\r\n \u2503 \u2523 &#128194;N1Y_VALID\r\n \u2503 \u2503 \u2517 &#128220;n1y.valid.txt\r\n \u2503 \u2523 &#128194;N3M_TEST\r\n \u2503 \u2503 \u2517 &#128220;n3m.test.txt\r\n \u2503 \u2523 &#128194;N3M_TRAIN\r\n \u2503 \u2503 \u2517 &#128220;n3m.train.txt\r\n \u2503 \u2523 &#128194;N3M_VALID\r\n \u2503 \u2503 \u2517 &#128220;n3m.valid.txt\r\n \u2503 \u2523 &#128194;N6M_TEST\r\n \u2503 \u2503 \u2517 &#128220;n6m.test.txt\r\n \u2503 \u2523 &#128194;N6M_TRAIN\r\n \u2503 \u2503 \u2517 &#128220;n6m.train.txt\r\n \u2503 \u2523 &#128194;N6M_VALID\r\n \u2503 \u2503 \u2517 &#128220;n6m.valid.txt\r\n \u2503 \u2523 &#128194;NF_TEST\r\n \u2503 \u2503 \u2517 &#128220;nf.test.txt\r\n \u2503 \u2523 &#128194;NF_TRAIN\r\n \u2503 \u2503 \u2517 &#128220;nf.train.txt\r\n \u2503 \u2517 &#128194;NF_VALID\r\n \u2503 \u2503 \u2517 &#128220;nf.valid.txt\r\n \u2523 &#128194;test\r\n \u2503 \u2523 &#128194;testData_iRec\r\n \u2503 \u2503 \u2523 &#128220;.part-00199-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt.crc\r\n \u2503 \u2503 \u2523 &#128220;part-00000-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt\r\n \u2503 \u2503 \u2523 &#128220;part-00003-f683aa3b-8840-4835-b8bc-a8d1eaa11c78.txt\r\n \u2503 \u2503 \u2517 &#128220;_SUCCESS\r\n \u2503 \u2523 &#128194;testData_uRec\r\n \u2503 \u2503 \u2523 &#128220;.part-00000-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt.crc\r\n \u2503 \u2503 \u2523 &#128220;._SUCCESS.crc\r\n \u2503 \u2503 \u2523 &#128220;part-00161-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt\r\n \u2503 \u2503 \u2523 &#128220;part-00196-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt\r\n \u2503 \u2503 \u2517 &#128220;part-00199-4a844096-8dd9-425e-9d9d-bd9062cc6940.txt\r\n \u2503 \u2523 &#128220;data_layer_tests.py\r\n \u2503 \u2523 &#128220;test_model.py\r\n \u2503 \u2517 &#128220;__init__.py\r\n \u2523 &#128194;__pycache__\r\n \u2503 \u2523 &#128220;api.cpython-37.pyc\r\n \u2503 \u2523 &#128220;load_test.cpython-37.pyc\r\n \u2503 \u2523 &#128220;parameters.cpython-37.pyc\r\n \u2503 \u2517 &#128220;utils.cpython-37.pyc\r\n \u2523 &#128220;api.py\r\n \u2523 &#128220;compute_RMSE.py\r\n \u2523 &#128220;load_test.py\r\n \u2523 &#128220;logger.py\r\n \u2523 &#128220;netflix_1y_test.csv\r\n \u2523 &#128220;netflix_1y_train.csv\r\n \u2523 &#128220;netflix_1y_valid.csv\r\n \u2523 &#128220;netflix_3m_test.csv\r\n \u2523 &#128220;netflix_3m_train.csv\r\n \u2523 &#128220;netflix_3m_valid.csv\r\n \u2523 &#128220;netflix_6m_test.csv\r\n \u2523 &#128220;netflix_6m_train.csv\r\n \u2523 &#128220;netflix_6m_valid.csv\r\n \u2523 &#128220;netflix_full_test.csv\r\n \u2523 &#128220;netflix_full_train.csv\r\n \u2523 &#128220;netflix_full_valid.csv\r\n \u2523 &#128220;parameters.py\r\n \u2523 &#128220;preds.txt\r\n \u2523 &#128220;RS_netflix3months_100epochs_64,128,128.ipynb\r\n \u2517 &#128220;utils.py\r\n```\r\n\r\nI am getting such an error ([serialization.py][2]). Can someone help me with this error?\r\n\r\n    D:\\Anaconda\\envs\\practise\\lib\\site-packages\\torch\\serialization.py in _legacy_load(f, map_location, pickle_module, **pickle_load_args)\r\n        762             &quot;functionality.&quot;)\r\n        763 \r\n    --&gt; 764     magic_number = pickle_module.load(f, **pickle_load_args)\r\n        765     if magic_number != MAGIC_NUMBER:\r\n        766         raise RuntimeError(&quot;Invalid magic number; corrupt file?&quot;)\r\n    \r\n    UnpicklingError: A load persistent id instruction was encountered,\r\n    but no persistent_load function was specified.\r\n\r\n\r\n  [1]: https://github.com/miguelgfierro/sciblog_support/blob/master/Intro_to_Recommendation_Systems/api.py\r\n  [2]: https://github.com/pytorch/pytorch/blob/master/torch/serialization.py",
        "accepted_answer_markdown": "After searching through PyTorch documentation, I ended up saving the model in the [ONNX][1] format and later loaded that ONNX model into PyTorch model and used it for inference.\r\n\r\n    import onnx\r\n    from onnx2pytorch import ConvertModel\r\n    \r\n    \r\n    def load_model_weights(model_architecture, weights_path):\r\n        if os.path.isfile(&quot;model.onnx&quot;):\r\n            cherrypy.log(&quot;CHERRYPYLOG Loading model from: {}&quot;.format(weights_path))\r\n            onnx_model = onnx.load(&quot;model.onnx&quot;)\r\n            pytorch_model = ConvertModel(onnx_model)\r\n            ## model_architecture.load_state_dict(torch.load(weights_path))\r\n        else:\r\n            raise ValueError(&quot;Path not found {}&quot;.format(weights_path))\r\n    \r\n            \r\n    def load_recommender(vector_dim, hidden, activation, dropout, weights_path):\r\n    \r\n        rencoder_api = model.AutoEncoder(layer_sizes=[vector_dim] + [int(l) for l in hidden.split(&#39;,&#39;)],\r\n                                   nl_type=activation,\r\n                                   is_constrained=False,\r\n                                   dp_drop_prob=dropout,\r\n                                   last_layer_activations=False)\r\n        load_model_weights(rencoder_api, weights_path) \r\n        rencoder_api.eval()\r\n        rencoder_api = rencoder_api.cuda()\r\n        return rencoder_api\r\n\r\n\r\nSome useful resources:\r\n\r\n[torch.save][2]\r\n\r\n[torch.load][3]\r\n\r\n[ONNX tutorials][4]\r\n\r\n\r\n  [1]: https://onnx.ai/\r\n  [2]: https://pytorch.org/docs/stable/generated/torch.save.html\r\n  [3]: https://pytorch.org/docs/stable/generated/torch.load.html\r\n  [4]: https://github.com/onnx/tutorials"
    },
    {
        "question_id": "66474197",
        "accepted_answer_id": "66474198",
        "question_title": "Pytorch lightning metrics: ValueError: preds and target must have same number of dimensions, or one additional dimension for preds",
        "question_markdown": "Googling this gets you no where, so I decided to help future me and others by posting this as a searchable question.\r\n___\r\n\r\n```\r\ndef __init__():\r\n    ...\r\n    self.val_acc = pl.metrics.Accuracy()\r\n\r\ndef validation_step(self, batch, batch_index):\r\n    ...\r\n    self.val_acc.update(log_probs, label_batch)\r\n```\r\n\r\ngives \r\n\r\n    ValueError: preds and target must have same number of dimensions, or one additional dimension for preds\r\n\r\nfor `log_probs.shape == (16, 4)` and for `label_batch.shape == (16, 4)`\r\n\r\nWhat&#39;s the issue?",
        "accepted_answer_markdown": "`pl.metrics.Accuracy()` expects a batch of `dtype=torch.long` labels, not one-hot encoded labels.\r\n\r\nThus, it should be fed\r\n\r\n`self.val_acc.update(log_probs, torch.argmax(label_batch.squeeze(), dim=1))`\r\n\r\n___\r\n\r\nThis is just the same as `torch.nn.CrossEntropyLoss`"
    },
    {
        "question_id": "66600362",
        "accepted_answer_id": "66850736",
        "question_title": "RuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle)` with GPU only",
        "question_markdown": "I&#39;m working on the CNN with one-dimensional signal. It works totally fine with CPU device. However, when I training model in GPU, CUDA error occurred. I set `os.environ[&#39;CUDA_LAUNCH_BLOCKING&#39;] = &quot;1&quot;` command after I got `RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED` when calling `cublasCreate(handle)`. With doing this, a `cublasSgemm` error occurred instead of `cublasCreate` error.\r\nThough the nvidia document doubt the hardware problem, I can training other CNN with images without any error. Below is my code for the data loading and set data in training model.\r\n\r\n        idx = np.arange(len(dataset))  # dataset &amp; label shuffle in once\r\n        np.random.shuffle(idx)\r\n\r\n        dataset = dataset[idx]\r\n        sdnn = np.array(sdnn)[idx.astype(int)]        \r\n\r\n        train_data, val_data = dataset[:int(0.8 * len(dataset))], dataset[int(0.8 * len(dataset)):]\r\n        train_label, val_label = sdnn[:int(0.8 * len(sdnn))], sdnn[int(0.8 * len(sdnn)):]\r\n        train_set = DataLoader(dataset=train_data, batch_size=opt.batch_size, num_workers=opt.workers)\r\n\r\n        for i, data in enumerate(train_set, 0):  # data.shape = [batch_size, 3000(len(signal)), 1(channel)] tensor\r\n\r\n            x = data.transpose(1, 2)\r\n            label = torch.Tensor(train_label[i * opt.batch_size:i * opt.batch_size + opt.batch_size])\r\n            x = x.to(device, non_blocking=True)\r\n            label = label.to(device, non_blocking=True) # [batch size]\r\n            label = label.view([len(label), 1])\r\n            optim.zero_grad()\r\n\r\n            # Feature of signal extract\r\n            y_predict = model(x) # [batch size, fc3 output] # Error occurred HERE\r\n            loss = mse(y_predict, label)\r\n\r\nBelow is the error message from this code.\r\n\r\n```\r\nFile C:/Users/Me/Desktop/Me/Study/Project/Analysis/Regression/main.py&quot;, line 217, in Processing\r\n    y_predict = model(x) # [batch size, fc3 output]\r\n  File &quot;C:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File &quot;C:\\Users\\ME\\Desktop\\ME\\Study\\Project\\Analysis\\Regression\\cnn.py&quot;, line 104, in forward\r\n    x = self.fc1(x)\r\n  File &quot;C:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py&quot;, line 722, in _call_impl\r\n    result = self.forward(*input, **kwargs)\r\n  File &quot;C:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py&quot;, line 91, in forward\r\n    return F.linear(input, self.weight, self.bias)\r\n  File &quot;C:\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py&quot;, line 1674, in linear\r\n    ret = torch.addmm(bias, input, weight.t())\r\nRuntimeError: CUDA error: CUBLAS_STATUS_EXECUTION_FAILED when calling `cublasSgemm( handle, opa, opb, m, n, k, &amp;alpha, a, lda, b, ldb, &amp;beta, c, ldc)`\r\n```\r\n\r\nI&#39;ve tried to solve this error for weeks but can&#39;t find the solution. If you can see anything wrong here, please let me know.\r\n",
        "accepted_answer_markdown": "With searched with the partial keywords, I finally got the similar situation.\r\nBecause of the stability, I used the CUDA 10.2 version. The reference asked to upgrade CUDA toolkit to higher - 11.2 in my case - and problem solved!\r\nI&#39;ve deal with other training processes but this one only caused error. As the CUDA error occurred with various reasons, changes the version could be counted for solution."
    },
    {
        "question_id": "66720543",
        "accepted_answer_id": "66721995",
        "question_title": "Pytorch 1D tensors expected but got 2D tensors",
        "question_markdown": "I have been working on making neural network from scratch in python. The input tensor is of shape `[400,3]` and target_tensor has the shape `[400]`. I am getting error during taking derivative of weights. Below are functions\r\n\r\n```python\r\n    def sigmoid(z):\r\n           return 1 / (1 + torch.exp(-z))\r\n\r\n    def nueral_net(data,weights,bias):\r\n           return sigmoid( ( data @ weights ) + bias )\r\n\r\n    def loss_function(prediction,actual,m):\r\n           return (-1/m) * (torch.sum(actual * torch.log(prediction) + (1-actual) \r\n           * torch.log(1- prediction)))\r\n\r\n    w = torch.randn(input_tensor.shape[1],1)\r\n    b = torch.randn(1,1)\r\n\r\n    predictions = nueral_net(input_tensor.float() , w, b) #Applying model\r\n    loss = loss_function(predictions,target_tensor.unsqueeze(1),400)\r\n    dw = (1/400) * torch.dot(input_tensor,(predictions - target_tensor).T)\r\n ```\r\n\r\nRunning this throws an error.\r\n\r\n```\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-26-632338d8fd16&gt; in &lt;module&gt;\r\n          1 predictions = nueral_net(input_tensor.float() , w, b) #Applying model\r\n          2 loss = loss_function(predictions,target_tensor.unsqueeze(1),400)\r\n    ----&gt; 3 dw = (1/400) * torch.dot(input_tensor,(predictions - target_tensor).T)\r\n          4 db = (1/400) * torch.sum(predictions - target_tensor)\r\n          5 #m = input_tensor.shape[0]\r\n    \r\n    RuntimeError: 1D tensors expected, but got 2D and 2D tensor\r\n\r\n```\r\n\r\n",
        "accepted_answer_markdown": "If we see the doc of `torch.dot` :  \r\n`torch.dot(input, other, *, out=None)` \u2192 `Tensor`\r\nComputes the dot product of two 1D tensors.\r\n\r\nNOTE : Unlike NumPy\u2019s dot, torch.dot intentionally only supports computing the dot product of two 1D tensors with the same number of elements.\r\n\r\nParameters  \r\n`input` (Tensor) \u2013 first tensor in the dot product, must be 1D.  \r\n`other` (Tensor) \u2013 second tensor in the dot product, must be 1D.\r\n\r\n\r\nComing to your question....both `input_tensor` &amp; `(predictions - target_tensor).T` in 2D.  \r\nPlease make it `1D` \r\n"
    },
    {
        "question_id": "66902573",
        "accepted_answer_id": "66903847",
        "question_title": "How to run LSTM on very long sequence using Truncated Backpropagation in Pytorch (lightning)?",
        "question_markdown": "I have a very long time series I want to feed into an LSTM for classification per-frame.\r\n\r\nMy data is labeled per frame, and I know some rare events happen that influence the classification heavily ever since they occur.\r\n\r\nThus, I have to feed the entire sequence to get meaningful predictions.\r\n\r\nIt is known that just feeding very long sequences into LSTM is sub-optimal, since the gradients vanish or explode just like normal RNNs.\r\n\r\n___\r\n\r\nI wanted to use a simple technique of cutting the sequence to shorter (say, 100-long) sequences, and run the LSTM on each, then pass the final LSTM hidden and cell states as the start hidden and cell state of the next forward pass.\r\n\r\n[Here][1] is an example I found of someone who did just that. There it is called &quot;Truncated Back propagation through time&quot;. I was not able to make the same work for me.\r\n\r\n___\r\n\r\nMy attempt in Pytorch lightning (stripped of irrelevant parts):\r\n\r\n```\r\ndef __init__(self, config, n_classes, datamodule):\r\n    ...\r\n    self._criterion = nn.CrossEntropyLoss(\r\n        reduction=&#39;mean&#39;,\r\n    )\r\n\r\n    num_layers = 1\r\n    hidden_size = 50\r\n    batch_size=1\r\n\r\n    self._lstm1 = nn.LSTM(input_size=len(self._in_features), hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\r\n    self._log_probs = nn.Linear(hidden_size, self._n_predicted_classes)\r\n    self._last_h_n = torch.zeros((num_layers, batch_size, hidden_size), device=&#39;cuda&#39;, dtype=torch.double, requires_grad=False)\r\n    self._last_c_n = torch.zeros((num_layers, batch_size, hidden_size), device=&#39;cuda&#39;, dtype=torch.double, requires_grad=False)\r\n\r\ndef training_step(self, batch, batch_index):\r\n    orig_batch, label_batch = batch\r\n    n_labels_in_batch = np.prod(label_batch.shape)\r\n    lstm_out, (self._last_h_n, self._last_c_n) = self._lstm1(orig_batch, (self._last_h_n, self._last_c_n))\r\n    log_probs = self._log_probs(lstm_out)\r\n    loss = self._criterion(log_probs.view(n_labels_in_batch, -1), label_batch.view(n_labels_in_batch))\r\n\r\n    return loss\r\n```\r\n\r\nRunning this code gives the following error:\r\n\r\n&gt;     RuntimeError: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling backward the first time.\r\n\r\n\r\nThe same happens if I add\r\n\r\n```\r\ndef on_after_backward(self) -&gt; None:\r\n    self._last_h_n.detach()\r\n    self._last_c_n.detach()\r\n```\r\n\r\nThe error does not happen if I use\r\n\r\n```\r\nlstm_out, (self._last_h_n, self._last_c_n) = self._lstm1(orig_batch,)\r\n```\r\n\r\nBut obviously this is useless, as the output from the current frame-batch is not forwarded to the next one.\r\n\r\n___\r\n\r\nWhat is causing this error? I thought detaching the output `h_n` and `c_n` should be enough.\r\n\r\nHow do I pass the output of a previous frame-batch to the next one and have torch back propagate each frame batch separately?\r\n\r\n  [1]: https://stackoverflow.com/questions/62901561/truncated-backpropagation-in-pytorch-code-check",
        "accepted_answer_markdown": "Apparently, I missed the trailing `_` for `detach()`:\r\n\r\nUsing\r\n\r\n```\r\ndef on_after_backward(self) -&gt; None:\r\n    self._last_h_n.detach_()\r\n    self._last_c_n.detach_()\r\n```\r\n\r\nworks.\r\n\r\n___\r\n\r\nThe problem was `self._last_h_n.detach()` does not update the reference to the *new memory* allocated by detach(), thus the graph is still de-referencing the old variable which backprop went through.\r\n[The reference answer][1] solved that by `H = H.detach()`.\r\n\r\nCleaner (and probably faster) is `self._last_h_n.detach_()` which does the operation in place.\r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/62901561/truncated-backpropagation-in-pytorch-code-check"
    },
    {
        "question_id": "66979328",
        "accepted_answer_id": "66982530",
        "question_title": "PyTorch ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15]))",
        "question_markdown": "I&#39;m currently using [this repo][1] to perform NLP and learn more about CNN&#39;s using my own dataset, and I keep running into an error regarding a shape mismatch:\r\n\r\n    ValueError: Target size (torch.Size([64])) must be the same as input size (torch.Size([15]))\r\n    \r\n         10 }\r\n         11 for epoch in tqdm(range(params[&#39;epochs&#39;])):\r\n    ---&gt; 12     train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\r\n         13     valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\r\n         14     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\r\n    \r\n         57         print(&quot;PredictionShapeAfter:&quot;)\r\n         58         print(predictions.shape)\r\n    ---&gt; 59         loss = criterion(predictions, batch.l)\r\n         60 \r\n         61         acc = binary_accuracy(predictions, batch.l)\r\n\r\n\r\nDoing some digging, I found that my CNN&#39;s prediction is a different size compared to the training data truth it&#39;s being compared to:\r\n\r\n\r\n      Input Shape:\r\n        torch.Size([15, 64])\r\n        Truth Shape:\r\n        torch.Size([64])\r\n        embedded unsqueezed: torch.Size([15, 1, 64, 100])\r\n        cat shape: torch.Size([15, 300])\r\n        Prediction Shape Before Squeeze:\r\n        torch.Size([15, 1])\r\n        PredictionShapeAfter:\r\n        torch.Size([15])\r\n\r\nThe model is making the prediction shape (the last value in this list) as the first dimension of the inputs. Is this a common problem and is there a way to rectify this issue?\r\n\r\n\r\nMy Model:\r\n\r\n    class CNN(nn.Module):\r\n        def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \r\n                     dropout, pad_idx):\r\n            super().__init__()\r\n                    \r\n            self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\r\n            \r\n            self.convs = nn.ModuleList([\r\n                                        nn.Conv2d(in_channels = 1, \r\n                                                  out_channels = n_filters, \r\n                                                  kernel_size = (fs, embedding_dim)) \r\n                                        for fs in filter_sizes\r\n                                        ])\r\n            \r\n            self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\r\n            \r\n            self.dropout = nn.Dropout(dropout)\r\n            \r\n        def forward(self, text): \r\n            embedded = self.embedding(text)\r\n            embedded = embedded.unsqueeze(1)\r\n            print(f&quot;embedded unsqueezed: {embedded.shape}&quot;)\r\n            conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]          \r\n            pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\r\n            cat = self.dropout(torch.cat(pooled, dim = 1))   \r\n            print(f&quot;cat shape: {cat.shape}&quot;)       \r\n            return self.fc(cat)\r\n\r\nMy Training function:\r\n\r\n    def train(model, iterator, optimizer, criterion):\r\n        \r\n        epoch_loss = 0\r\n        epoch_acc = 0\r\n        \r\n        model.train()\r\n        \r\n        for batch in iterator:\r\n            \r\n            optimizer.zero_grad()\r\n    \r\n            print(&quot;InputShape:&quot;)\r\n            print(batch.t.shape)\r\n            print(&quot;Truth Shape:&quot;)\r\n            print(batch.l.shape)\r\n    \r\n            predictions = model(batch.t)\r\n            print(&quot;Prediction Shape Before Squeeze:&quot;)\r\n            print(predictions.shape)\r\n    \r\n            predictions = predictions.squeeze(1)\r\n            print(&quot;PredictionShapeAfter:&quot;)\r\n            print(predictions.shape)\r\n            loss = criterion(predictions, batch.l)\r\n            \r\n            acc = binary_accuracy(predictions, batch.l)\r\n            \r\n            loss.backward()\r\n            \r\n            optimizer.step()\r\n            \r\n            epoch_loss += loss.item()\r\n            epoch_acc += acc.item()\r\n            \r\n        return epoch_loss / len(iterator), epoch_acc / len(iterator)\r\n\r\n  \r\nMy full code can be found at [this link.][2] \r\n\r\n\r\n  [1]: https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb\r\n  [2]: https://colab.research.google.com/drive/1aY7XlgUClEe5Ldeu8DjSsxuj9sXTtRw4?usp=sharing",
        "accepted_answer_markdown": "Your issue is here:\r\n\r\n```python\r\nself.convs = nn.ModuleList([\r\n                            nn.Conv2d(in_channels = 1, \r\n                                      out_channels = n_filters, \r\n                                      kernel_size = (fs, embedding_dim)) \r\n                            for fs in filter_sizes\r\n                            ])\r\n```\r\n\r\nYou are inputting data of shape `[15, 1, 64, 100]`, which the convolutions are interpreting as batches of size 15, of 1-channel images of HxW 64x100.\r\n\r\nWhat it appears you want is a batch of size 64, so swap those dimensions first:\r\n\r\n```python\r\n...\r\nembedded = embedded.swapdims(0,2)\r\nconved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs] \r\n...\r\n```\r\n\r\n"
    },
    {
        "question_id": "66992585",
        "accepted_answer_id": "68626103",
        "question_title": "How does one use Pytorch (+ cuda) with an A100 GPU?",
        "question_markdown": "I was trying to use my current code with an A100 gpu but I get this error:\r\n```\r\n---&gt; backend=&#39;nccl&#39;\r\n/home/miranda9/miniconda3/envs/metalearningpy1.7.1c10.2/lib/python3.8/site-packages/torch/cuda/__init__.py:104: UserWarning: \r\nA100-SXM4-40GB with CUDA capability sm_80 is not compatible with the current PyTorch installation.\r\nThe current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_61 sm_70 sm_75 compute_37.\r\nIf you want to use the A100-SXM4-40GB GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\r\n```\r\n\r\nwhich is reather confusing because it points to the usual pytorch installation but doesn&#39;t tell me which combination of pytorch version + cuda version to use for my specific hardware (A100). What is the right way to install pytorch for an A100?\r\n\r\n----\r\n\r\nThese are some versions I&#39;ve tried:\r\n```\r\n# conda install -y pytorch==1.8.0 torchvision cudatoolkit=10.2 -c pytorch\r\n# conda install -y pytorch torchvision cudatoolkit=10.2 -c pytorch\r\n#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=10.2 -c pytorch -c conda-forge\r\n# conda install -y pytorch==1.6.0 torchvision cudatoolkit=10.2 -c pytorch\r\n#conda install -y pytorch==1.7.1 torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\r\n\r\n# conda install pytorch torchvision torchaudio cudatoolkit=11.0 -c pytorch\r\n# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\r\n# conda install -y pytorch torchvision cudatoolkit=9.2 -c pytorch # For Nano, CC\r\n# conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\r\n```\r\n\r\n\r\n-----\r\n\r\nnote that this can be subtle because I&#39;ve had this error with this machine + pytorch version in the past:\r\n\r\nhttps://stackoverflow.com/questions/66807131/how-to-solve-the-famous-unhandled-cuda-error-nccl-version-2-7-8-error\r\n\r\n---\r\n\r\n# Bonus 1:\r\n\r\nI still have errors:\r\n```\r\nncclSystemError: System call (socket, malloc, munmap, etc) failed.\r\nTraceback (most recent call last):\r\n  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1423, in &lt;module&gt;\r\n    main()\r\n  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1365, in main\r\n    train(args=args)\r\n  File &quot;/home/miranda9/diversity-for-predictive-success-of-meta-learning/div_src/diversity_src/experiment_mains/main_dist_maml_l2l.py&quot;, line 1385, in train\r\n    args.opt = move_opt_to_cherry_opt_and_sync_params(args) if is_running_parallel(args.rank) else args.opt\r\n  File &quot;/home/miranda9/ultimate-utils/ultimate-utils-proj-src/uutils/torch_uu/distributed.py&quot;, line 456, in move_opt_to_cherry_opt_and_sync_params\r\n    args.opt = cherry.optim.Distributed(args.model.parameters(), opt=args.opt, sync=syn)\r\n  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 62, in __init__\r\n    self.sync_parameters()\r\n  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/cherry/optim.py&quot;, line 78, in sync_parameters\r\n    dist.broadcast(p.data, src=root)\r\n  File &quot;/home/miranda9/miniconda3/envs/meta_learning_a100/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py&quot;, line 1090, in broadcast\r\n    work = default_pg.broadcast([tensor], opts)\r\nRuntimeError: NCCL error in: ../torch/lib/c10d/ProcessGroupNCCL.cpp:911, unhandled system error, NCCL version 2.7.8\r\n```\r\none of the answers suggested to have nvcca &amp; pytorch.version.cuda to match but they do not:\r\n```\r\n(meta_learning_a100) [miranda9@hal-dgx ~]$ python -c &quot;import torch;print(torch.version.cuda)&quot;\r\n\r\n11.1\r\n(meta_learning_a100) [miranda9@hal-dgx ~]$ nvcc -V\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2020 NVIDIA Corporation\r\nBuilt on Wed_Jul_22_19:09:09_PDT_2020\r\nCuda compilation tools, release 11.0, V11.0.221\r\nBuild cuda_11.0_bu.TC445_37.28845127_0\r\n```\r\n\r\nHow do I match them? I this the error? Can someone display their pip, conda and nvcca version to see what set up works?\r\n\r\n\r\nMore error messages:\r\n```\r\nhal-dgx:21797:21797 [0] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\r\nhal-dgx:21797:21797 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nhal-dgx:21797:21797 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\r\nhal-dgx:21797:21797 [0] NCCL INFO Using network IB\r\nNCCL version 2.7.8+cuda11.1\r\nhal-dgx:21805:21805 [2] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\r\nhal-dgx:21799:21799 [1] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\r\nhal-dgx:21805:21805 [2] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nhal-dgx:21799:21799 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nhal-dgx:21811:21811 [3] NCCL INFO Bootstrap : Using [0]enp226s0:141.142.153.83&lt;0&gt; [1]virbr0:192.168.122.1&lt;0&gt;\r\nhal-dgx:21811:21811 [3] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation\r\nhal-dgx:21811:21811 [3] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\r\nhal-dgx:21811:21811 [3] NCCL INFO Using network IB\r\nhal-dgx:21799:21799 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\r\nhal-dgx:21805:21805 [2] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/IB [6]mlx5_6:1/IB [7]mlx5_7:1/IB ; OOB enp226s0:141.142.153.83&lt;0&gt;\r\nhal-dgx:21799:21799 [1] NCCL INFO Using network IB\r\nhal-dgx:21805:21805 [2] NCCL INFO Using network IB\r\n\r\nhal-dgx:21797:27906 [0] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed\r\nhal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:360 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO transport/net_ib.cc:437 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO include/net.h:21 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO include/net.h:51 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:300 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:566 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO init.cc:840 -&gt; 2\r\nhal-dgx:21797:27906 [0] NCCL INFO group.cc:73 -&gt; 2 [Async thread]\r\n\r\nhal-dgx:21811:27929 [3] misc/ibvwrap.cc:280 NCCL WARN Call to ibv_create_qp failed\r\nhal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:360 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO transport/net_ib.cc:437 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO include/net.h:21 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO include/net.h:51 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:300 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:566 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO init.cc:840 -&gt; 2\r\nhal-dgx:21811:27929 [3] NCCL INFO group.cc:73 -&gt; 2 [Async thread]\r\n```\r\n\r\nafter putting \r\n```\r\nimport os\r\nos.environ[&quot;NCCL_DEBUG&quot;] = &quot;INFO&quot;\r\n```\r\n",
        "accepted_answer_markdown": "From the link [pytorch site][1] from @SimonB &#39;s answer, I did:\r\n\r\n    pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\r\n\r\n\r\nThis solved the problem for me.\r\n\r\n  [1]: https://pytorch.org/get-started/locally/"
    },
    {
        "question_id": "56455302",
        "accepted_answer_id": "56456183",
        "question_title": "Use only certain layers of pretrained torchvision network",
        "question_markdown": "I&#39;m trying to use only certain layers in a pretrained torchvision Faster-RCNN network initialized by:\r\n\r\n    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\r\n    model.eval()\r\n\r\nThis works. However, passing `model.modules()` or `model.children()` into an `nn.Sequential` yields an error. Even passing the whole model leads to errors, e.g.\r\n\r\n    model = torch.nn.Sequential(*model.modules())\r\n    model.eval()\r\n    # x is a [C, H, W] image\r\n    y = model(x)\r\n\r\nleads to \r\n\r\n    AttributeError: &#39;dict&#39; object has no attribute &#39;dim&#39;\r\n\r\nand \r\n\r\n    model = torch.nn.Sequential(*model.children())\r\n    model.eval()\r\n    # x is a [C, H, W] image\r\n    y = model(x)\r\n\r\nleads to \r\n\r\n    TypeError: conv2d(): argument &#39;input&#39; (position 1) must be Tensor, not tuple\r\n\r\nThis confuses me because I have modified other PyTorch pretrained models like that in the past. How can I use the FasterRCNN pretrained model to create a new (pretrained) model that uses only certain layers, e.g. all layers but the last one?\r\n\r\n\r\n",
        "accepted_answer_markdown": "Unlike other simple CNN models, it is not trivial to convert an R-CNN based detector to a simple `nn.Sequential` model. If you look at the functionality of R-CNN ([`&#39;generalized_rcnn.py&#39;`][1]) you&#39;ll see that the output features (computed by the FCN backbone) are not just passed to the RPN component, but rather combined with the input image and even with the targets (during training).\r\n\r\nTherefore, I suppose if you want to change the way faster R-CNN behaves, you&#39;ll have to use the base class [`torchvision.models.detection.FasterRCNN()`][2] and provide it with different roi pooling parameters.\r\n\r\n\r\n [1]: https://github.com/pytorch/vision/blob/master/torchvision/models/detection/generalized_rcnn.py\r\n [2]: https://github.com/pytorch/vision/blob/master/torchvision/models/detection/faster_rcnn.py"
    },
    {
        "question_id": "56490278",
        "accepted_answer_id": "56667050",
        "question_title": "Passing tensorDataset or Dataloader to skorch",
        "question_markdown": "I want to apply cross validation in Pytorch using skorch, so I prepared my model and my tensorDataset which returns (image,caption and captions_length) and so it has X and Y, so I&#39;ll not be able to set Y in the method \r\n\r\n    net.fit(dataset)\r\n\r\nbut when I tried that I got an error :\r\n\r\n&gt; ValueError: Stratified CV requires explicitly passing a suitable y\r\n\r\nHere&#39;s part of my code:\r\n````\r\nstart = time.time()\r\nnet = NeuralNetClassifier(\r\n        decoder, criterion= nn.CrossEntropyLoss,\r\n        max_epochs=args.epochs,\r\n        lr=args.lr,\r\n        optimizer=optim.SGD,\r\n        device=&#39;cuda&#39;,  # uncomment this to train with CUDA\r\n       )\r\nnet.fit(dataset, y=None)\r\nend = time.time()\r\n````",
        "accepted_answer_markdown": "&lt;!-- language-all: lang-python --&gt;\r\n\r\nYou are (implicitly) using the internal CV split of skorch which uses a stratified split in case of the `NeuralNetClassifier` which in turn needs information about the labels beforehand. \r\n\r\nWhen passing `X` and `y` to `fit` separately this works fine since `y` is accessible at all times. The problem is that you are using `torch.dataset.Dataset` which is lazy and does not give you access to `y` directly, hence the error.\r\n\r\nYour options are the following.\r\n\r\n### Set `train_split=None` to disable the internal CV split\r\n\r\n```\r\nnet = NeuralNetClassifier(\r\n    train_split=None,\r\n)\r\n```\r\n\r\nYou will lose internal validation and, as such, features like early stopping.\r\n\r\n### Split your data beforehand \r\n\r\nSplit your dataset into two datasets, `dataset_train` and `dataset_valid`, \r\nthen use [`skorch.helper.predefined_split`][1]:\r\n\r\n```\r\nnet = NeuralNetClassifier(\r\n    train_split=predefined_split(dataset_valid),\r\n)\r\n```\r\n\r\nYou lose nothing but depending on your data this might be complicated.\r\n\r\n### Extract your `y` and pass it to fit\r\n\r\n```\r\ny_train = np.array([y for X, y in iter(my_dataset)])\r\nnet.fit(my_dataset, y=y_train)\r\n```\r\n\r\nThis only works if your `y` fits into memory. Since you are using `TensorDataset` you can also do the following to extract your `y`:\r\n\r\n```\r\ny_train = my_dataset.y\r\n```\r\n\r\n[1]: https://skorch.readthedocs.io/en/stable/helper.html#skorch.helper.predefined_split"
    },
    {
        "question_id": "56741087",
        "accepted_answer_id": "56741419",
        "question_title": "How to fix RuntimeError &quot;Expected object of scalar type Float but got scalar type Double for argument&quot;?",
        "question_markdown": "I&#39;m trying to train a classifier via PyTorch. However, I am experiencing problems with training when I feed the model with training data.\r\nI get this error on `y_pred = model(X_trainTensor)`:\r\n\r\n&gt; RuntimeError: Expected object of scalar type Float but got scalar type Double for argument #4 &#39;mat1&#39;\r\n\r\nHere are key parts of my code:\r\n\r\n```python\r\n# Hyper-parameters \r\nD_in = 47  # there are 47 parameters I investigate\r\nH = 33\r\nD_out = 2  # output should be either 1 or 0\r\n```\r\n```python\r\n# Format and load the data\r\ny = np.array( df[&#39;target&#39;] )\r\nX = np.array( df.drop(columns = [&#39;target&#39;], axis = 1) )\r\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)  # split training/test data\r\n\r\nX_trainTensor = torch.from_numpy(X_train) # convert to tensors\r\ny_trainTensor = torch.from_numpy(y_train)\r\nX_testTensor = torch.from_numpy(X_test)\r\ny_testTensor = torch.from_numpy(y_test)\r\n```\r\n```python\r\n# Define the model\r\nmodel = torch.nn.Sequential(\r\n    torch.nn.Linear(D_in, H),\r\n    torch.nn.ReLU(),\r\n    torch.nn.Linear(H, D_out),\r\n    nn.LogSoftmax(dim = 1)\r\n)\r\n```\r\n```python\r\n# Define the loss function\r\nloss_fn = torch.nn.NLLLoss() \r\n```\r\n```python\r\nfor i in range(50):\r\n    y_pred = model(X_trainTensor)\r\n    loss = loss_fn(y_pred, y_trainTensor)\r\n    model.zero_grad()\r\n    loss.backward()\r\n    with torch.no_grad():       \r\n        for param in model.parameters():\r\n            param -= learning_rate * param.grad\r\n```",
        "accepted_answer_markdown": "Reference is from [this github issue](https://github.com/pytorch/pytorch/issues/2138).\r\n\r\nWhen the error is `RuntimeError: Expected object of scalar type Float but got scalar type Double for argument #4 &#39;mat1&#39;`, you would need to use the `.float()` function since it says `Expected object of scalar type Float`.\r\n\r\nTherefore, the solution is changing `y_pred = model(X_trainTensor)` to `y_pred = model(X_trainTensor.float())`.\r\n\r\nLikewise, when you get another error for `loss = loss_fn(y_pred, y_trainTensor)`, you need `y_trainTensor.long()` since the error message says `Expected object of scalar type Long`.\r\n\r\nYou could also do `model.double()`, as suggested by @Paddy\r\n."
    },
    {
        "question_id": "56741108",
        "accepted_answer_id": "56748776",
        "question_title": "PyTorch - Getting the &#39;TypeError: pic should be PIL Image or ndarray. Got &lt;class &#39;numpy.ndarray&#39;&gt;&#39; error",
        "question_markdown": "I am getting the error `TypeError: pic should be PIL Image or ndarray. Got &lt;class &#39;numpy.ndarray&#39;&gt;` when I try to load **a non-image dataset** through the `DataLoader`. The versions of `torch` and `torchvision` are `1.0.1`, and `0.2.2.post3`, respectively. Python&#39;s version is `3.7.1` on a `Windows 10` machine.\r\n\r\nHere is the code:\r\n\r\n    class AndroDataset(Dataset):\r\n        def __init__(self, csv_path):\r\n            self.transform = transforms.Compose([transforms.ToTensor()])\r\n    \r\n            csv_data = pd.read_csv(csv_path)\r\n    \r\n            self.csv_path = csv_path\r\n            self.features = []\r\n            self.classes = []\r\n    \r\n            self.features.append(csv_data.iloc[:, :-1].values)\r\n            self.classes.append(csv_data.iloc[:, -1].values)\r\n    \r\n        def __getitem__(self, index):\r\n            # the error occurs here\r\n            return self.transform(self.features[index]), self.transform(self.classes[index]) \r\n    \r\n        def __len__(self):\r\n            return len(self.features)\r\n\r\n\r\nAnd I set the loader:\r\n\r\n    training_data = AndroDataset(&#39;android.csv&#39;)\r\n    train_loader = DataLoader(dataset=training_data, batch_size=batch_size, shuffle=True)\r\n\r\nHere is the full error stack trace:\r\n\r\n    Traceback (most recent call last):\r\n      File &quot;C:\\Program Files\\JetBrains\\PyCharm 2018.1.2\\helpers\\pydev\\pydevd.py&quot;, line 1758, in &lt;module&gt;\r\n        main()\r\n      File &quot;C:\\Program Files\\JetBrains\\PyCharm 2018.1.2\\helpers\\pydev\\pydevd.py&quot;, line 1752, in main\r\n        globals = debugger.run(setup[&#39;file&#39;], None, None, is_module)\r\n      File &quot;C:\\Program Files\\JetBrains\\PyCharm 2018.1.2\\helpers\\pydev\\pydevd.py&quot;, line 1147, in run\r\n        pydev_imports.execfile(file, globals, locals)  # execute the script\r\n      File &quot;C:\\Program Files\\JetBrains\\PyCharm 2018.1.2\\helpers\\pydev\\_pydev_imps\\_pydev_execfile.py&quot;, line 18, in execfile\r\n        exec(compile(contents+&quot;\\n&quot;, file, &#39;exec&#39;), glob, loc)\r\n      File &quot;C:/Users/talha/Documents/PyCharmProjects/DeepAndroid/deep_test_conv1d.py&quot;, line 231, in &lt;module&gt;\r\n        main()\r\n      File &quot;C:/Users/talha/Documents/PyCharmProjects/DeepAndroid/deep_test_conv1d.py&quot;, line 149, in main\r\n        for i, (images, labels) in enumerate(train_loader):\r\n      File &quot;C:\\Users\\talha\\Documents\\PyCharmProjects\\DeepAndroid\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py&quot;, line 615, in __next__\r\n        batch = self.collate_fn([self.dataset[i] for i in indices])\r\n      File &quot;C:\\Users\\talha\\Documents\\PyCharmProjects\\DeepAndroid\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py&quot;, line 615, in &lt;listcomp&gt;\r\n        batch = self.collate_fn([self.dataset[i] for i in indices])\r\n      File &quot;C:/Users/talha/Documents/PyCharmProjects/DeepAndroid/deep_test_conv1d.py&quot;, line 102, in __getitem__\r\n        return self.transform(self.features[index]), self.transform(self.classes[index])\r\n      File &quot;C:\\Users\\talha\\Documents\\PyCharmProjects\\DeepAndroid\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py&quot;, line 60, in __call__\r\n        img = t(img)\r\n      File &quot;C:\\Users\\talha\\Documents\\PyCharmProjects\\DeepAndroid\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py&quot;, line 91, in __call__\r\n        return F.to_tensor(pic)\r\n      File &quot;C:\\Users\\talha\\Documents\\PyCharmProjects\\DeepAndroid\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py&quot;, line 50, in to_tensor\r\n        raise TypeError(&#39;pic should be PIL Image or ndarray. Got {}&#39;.format(type(pic)))\r\n    TypeError: pic should be PIL Image or ndarray. Got &lt;class &#39;numpy.ndarray&#39;&gt;\r\n\r\n",
        "accepted_answer_markdown": "Expanding on @MiriamFarber&#39;s answer, you cannot use `transforms.ToTensor()` on `numpy.ndarray` objects. You can convert `numpy` arrays to `torch` tensors using [`torch.from_numpy()`](https://pytorch.org/docs/stable/torch.html#torch.from_numpy) and then cast your tensor to the required datatype.\r\n\r\n\r\n----------\r\nEg:\r\n\r\n```\r\n&gt;&gt;&gt; import numpy as np\r\n&gt;&gt;&gt; import torch\r\n&gt;&gt;&gt; np_arr = np.ones((5289, 38))\r\n&gt;&gt;&gt; torch_tensor = torch.from_numpy(np_arr).long()\r\n&gt;&gt;&gt; type(np_arr)\r\n&lt;class &#39;numpy.ndarray&#39;&gt;\r\n&gt;&gt;&gt; type(torch_tensor)\r\n&lt;class &#39;torch.Tensor&#39;&gt;\r\n```"
    },
    {
        "question_id": "56745486",
        "accepted_answer_id": "56748549",
        "question_title": "PyTorch DataLoader - &quot;IndexError: too many indices for tensor of dimension 0&quot;",
        "question_markdown": "I am trying to implement a CNN to identify digits in the MNIST dataset and my code comes up with the error during the data loading process. I don&#39;t understand why this is happening.\r\n\r\n```python\r\nimport torch\r\nimport torchvision\r\nimport torchvision.transforms as transforms\r\n\r\ntransform = transforms.Compose([\r\n    transforms.ToTensor(),\r\n    transforms.Normalize((0.5), (0.5))\r\n])\r\n\r\ntrainset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=True, download=True, transform=transform)\r\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=20, shuffle=True, num_workers=2)\r\n\r\ntestset = torchvision.datasets.MNIST(root=&#39;./data&#39;, train=False, download=True, transform=transform)\r\ntestloader = torch.utils.data.DataLoader(testset, batch_size=20, shuffle=False, num_workers=2)\r\n\r\nfor i, data in enumerate(trainloader, 0):\r\n    inputs, labels = data[0], data[1]\r\n```\r\n\r\nError:\r\n\r\n```text\r\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n&lt;ipython-input-6-b37c638b6114&gt; in &lt;module&gt;\r\n      2 \r\n----&gt; 3     for i, data in enumerate(trainloader, 0):\r\n      4         inputs, labels = data[0], data[1]\r\n\r\n# ...\r\n\r\nIndexError: Traceback (most recent call last):\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py&quot;, line 99, in _worker_loop\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py&quot;, line 99, in &lt;listcomp&gt;\r\n    samples = collate_fn([dataset[i] for i in batch_indices])\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torchvision/datasets/mnist.py&quot;, line 95, in __getitem__\r\n    img = self.transform(img)\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py&quot;, line 61, in __call__\r\n    img = t(img)\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torchvision/transforms/transforms.py&quot;, line 164, in __call__\r\n    return F.normalize(tensor, self.mean, self.std, self.inplace)\r\n  File &quot;/opt/conda/lib/python3.6/site-packages/torchvision/transforms/functional.py&quot;, line 208, in normalize\r\n    tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\r\nIndexError: too many indices for tensor of dimension 0\r\n```",
        "accepted_answer_markdown": "The problem is that the `mean` and `std` have to be sequences (e.g., tuples), therefore you should add a comma after the values:\r\n\r\n```python\r\ntransform = transforms.Compose([\r\n    transforms.ToTensor(),\r\n    transforms.Normalize((0.5,), (0.5,))\r\n])\r\n```\r\n\r\nNote the difference between `(0.5)` and `(0.5,)`. You can check [here](https://github.com/pytorch/vision/blob/8616227c5a4f8210c4fc123b9504972d24397a66/torchvision/transforms/functional.py#L214-L216) how these values are used. If you apply the same process you&#39;ll see that:\r\n\r\n```python\r\nimport torch\r\n\r\nx1 = torch.as_tensor((0.5))\r\nx2 = torch.as_tensor((0.5,))\r\n\r\nprint(x1.shape, x1.ndim)  # output: torch.Size([]) 0\r\nprint(x2.shape, x2.ndim)  # output: torch.Size([1]) 1\r\n```\r\n\r\nMaybe you don&#39;t know, but they are also different in Python:\r\n\r\n```python\r\ntype((0.5))   # &lt;type &#39;float&#39;&gt;\r\ntype((0.5,))  # &lt;type &#39;tuple&#39;&gt;\r\n```"
    },
    {
        "question_id": "56764048",
        "accepted_answer_id": "59283443",
        "question_title": "How to train the original U-Net model with PyTorch?",
        "question_markdown": "I\u2019m trying to implement and train the [original U-Net model](https://arxiv.org/pdf/1505.04597.pdf), but I\u2019m stuck in when I\u2019m trying to train the model using the [ISBI Challenge Dataset](http://brainiac2.mit.edu/isbi_challenge/).\r\n\r\nAccording with the original U-Net model, the network outputs an image with 2 channels and size of 388 x 388. So, my data loader for training generates a tensor with size of *[batch, channels=1, width=572, height=572]* for the input images and *[batch, channels=2, width=388, width=388]* for target/output images.\r\n\r\nMy problem actually is that when I\u2019m trying to use the nn.CrossEntropyLoss() the following error is raised: \r\n\r\n&gt; RuntimeError: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 4 at /opt/conda/conda-bld/pytorch_1556653099582/work/aten/src/THNN/generic/SpatialClassNLLCriterion.c:59\r\n\r\nI\u2019m just starting with PyTorch (newbie here)\u2026 so, I\u2019ll really appreciate if someone could help me to overcome this problem.\r\n\r\nThe sourcecode is available on GitHub:\r\n\r\n[https://github.com/dalifreire/cnn_unet_pytorch](https://github.com/dalifreire/cnn_unet_pytorch)\r\n[https://github.com/dalifreire/cnn_unet_pytorch/blob/master/unet_pytorch.ipynb](https://github.com/dalifreire/cnn_unet_pytorch/blob/master/unet_pytorch.ipynb)\r\n\r\nBest regards!\r\n\r\n\r\n\r\n**UPDATE**\r\n\r\nI just remove the channel dimension from my masks and everything works well\u2026 now I\u2019m generating masks with the shape 1 [width=388, height=388].\r\n\r\nAfter that, I\u2019m working with input images (X), target masks (y) and predicted output masks (y_hat) as follow:\r\n\r\n    X     --&gt; torch.Size([10, 1, 572, 572])\r\n    y     --&gt; torch.Size([10, 388, 388])\r\n    y_hat --&gt; torch.Size([10, 2, 388, 388])\r\n\r\n\r\nBut, I don\u2019t understand why target masks (y) and predicted masks (y_hat) must have different shapes? It\u2019s so weird for me\u2026",
        "accepted_answer_markdown": "From the CrossEntropyLoss docstring of PyTorch:\r\n\r\n    Shape:\r\n\r\n        - Input: :math:`(N, C)` where `C = number of classes`, or\r\n          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\r\n          in the case of `K`-dimensional loss.\r\n\r\n        - Target: :math:`(N)` where each value is :math:`0 \\leq \\text{targets}[i] \\leq C-1`, or\r\n          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case of\r\n          K-dimensional loss.\r\n\r\n        - Output: scalar.\r\n          If :attr:`reduction` is ``&#39;none&#39;``, then the same size as the target:\r\n          :math:`(N)`, or\r\n          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` in the case\r\n          of K-dimensional loss.\r\n\r\nIf your targets contain the class indices already, you should remove the channel dimension.\r\n\r\n[Source][1]\r\n\r\n\r\n  [1]: https://discuss.pytorch.org/t/only-batches-of-spatial-targets-supported-non-empty-3d-tensors-but-got-targets-of-size-1-1-256-256/49134"
    },
    {
        "question_id": "56783182",
        "accepted_answer_id": "56799053",
        "question_title": "RuntimeError: The size of tensor a (133) must match the size of tensor b (10) at non-singleton dimension 1",
        "question_markdown": "I am training a CNN model. I am facing issue while doing the training iteration for my model. The code is as below:\r\n```\r\nclass Net(nn.Module):\r\n    \r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        \r\n        #convo layers\r\n        self.conv1 = nn.Conv2d(3,32,3)\r\n        self.conv2 = nn.Conv2d(32,64,3)\r\n        self.conv3 = nn.Conv2d(64,128,3)\r\n        self.conv4 = nn.Conv2d(128,256,3)\r\n        self.conv5 = nn.Conv2d(256,512,3)\r\n        \r\n        #pooling layer\r\n        self.pool = nn.MaxPool2d(2,2)\r\n        \r\n        #linear layers\r\n        self.fc1 = nn.Linear(512*5*5,2048)\r\n        self.fc2 = nn.Linear(2048,1024)\r\n        self.fc3 = nn.Linear(1024,133)\r\n         \r\n        #dropout layer\r\n        self.dropout = nn.Dropout(0.3)\r\n        def forward(self, x):\r\n        #first layer\r\n        x = self.conv1(x)\r\n        x = F.relu(x)\r\n        x = self.pool(x)\r\n        #x = self.dropout(x)\r\n        #second layer\r\n        x = self.conv2(x)\r\n        x = F.relu(x)\r\n        x = self.pool(x)\r\n        #x = self.dropout(x)\r\n        #third layer\r\n        x = self.conv3(x)\r\n        x = F.relu(x)\r\n        x = self.pool(x)\r\n        #x = self.dropout(x)\r\n        #fourth layer\r\n        x = self.conv4(x)\r\n        x = F.relu(x)\r\n        x = self.pool(x)\r\n        #fifth layer\r\n        x = self.conv5(x)\r\n        x = F.relu(x)\r\n        x = self.pool(x)\r\n        #x = self.dropout(x)\r\n        \r\n        #reshape tensor\r\n        x = x.view(-1,512*5*5)\r\n        #last layer\r\n        x = self.dropout(x)\r\n        x = self.fc1(x)\r\n        x = F.relu(x)\r\n        x = self.dropout(x)\r\n        x = self.fc2(x)\r\n        x = F.relu(x)\r\n        x = self.fc3(x)\r\n\r\n        return x\r\n         \r\n        #loss func\r\n        criterion = nn.MSELoss()\r\n        optimizer = optim.Adam(net.parameters(), lr = 0.0001)\r\n        #criterion = nn.CrossEntropyLoss()\r\n        #optimizer = optim.SGD(net.parameters(), lr = 0.05)\r\n\r\n        def train(n_epochs,model,loader,optimizer,criterion,save_path):    \r\n           for epoch in range(n_epochs):\r\n              train_loss = 0\r\n              valid_loss = 0\r\n              #training \r\n              net.train()\r\n              for batch, (data,target) in enumerate(loaders[&#39;train&#39;]):\r\n                   optimizer.zero_grad()\r\n                   outputs = net(data)\r\n                   #print(outputs.shape)\r\n                   loss = criterion(outputs,target)\r\n                   loss.backward()\r\n                   optimizer.step()\r\n```            \r\n\r\nWhen I use the CrossEntropy Loss function and SGD optimizer, I able able to train the model with no error. \r\nWhen I use MSE loss function and Adam optimizer, I am facing the following error:\r\n\r\n    RuntimeError Traceback (most recent call last) &lt;ipython-input-20-2223dd9058dd&gt; in &lt;module&gt;\r\n          1 #train the model\r\n          2 n_epochs = 2\r\n    ----&gt; 3 train(n_epochs,net,loaders,optimizer,criterion,&#39;saved_model/dog_model.pt&#39;)\r\n    \r\n    &lt;ipython-input-19-a93d145ef9f7&gt; in train(n_epochs, model, loader, optimizer, criterion, save_path)\r\n         22 \r\n         23             #calculate loss\r\n    ---&gt; 24             loss = criterion(outputs,target)\r\n         25 \r\n         26             #backward prop\r\n    \r\n    RuntimeError: The size of tensor a (133) must match the size of tensor b (10) at non-singleton dimension 1.\r\n\r\nDoes the selected loss function and optimizer effect the training of the model? Can anyone please help on this?",
        "accepted_answer_markdown": "The error message clearly suggests that the error occurred at the line \r\n\r\n    loss = criterion(outputs,target)\r\nwhere you are trying to compute the `mean-squared error` between the input and the target.\r\nSee this line: `criterion = nn.MSELoss()`.\r\n\r\nI think you should modify your code where you are estimating loss between (output, target) pair of inputs,i.e., `loss = criterion(outputs,target)` to something like below:\r\n\r\n    loss = criterion(outputs,target.view(1, -1))\r\nHere, you are making `target` shape same as `outputs` from model on line \r\n\r\n    outputs = net(data)\r\nOne more think to notice here is the output of the `net` model, i.e., outputs will be of shape `batch_size X output_channels`, where batch size if the first dimension of input images as during the training you will get batches of images, so your shape in the forward method will get an additional batch dimension at `dim0`: `[batch_size, channels, height, width`], and `ouput_channels` is number of output features/channels from the last linear layer in the `net` model.\r\n\r\nAnd, the the target labels will be of shape `batch_size`, which is `10` in your case, check `batch_size` you passed in `torch.utils.data.DataLoader()`. Therefore, on reshaping it using `view(1, -1)`, it will be of converted into a shape `1 X batch_size`, i.e., `1 X 10`.\r\n\r\nThat&#39;s why, you are getting the error:\r\n\r\n&gt; RuntimeError: input and target shapes do not match: input [10 x 133],\r\n&gt; target [1 x 10]\r\n\r\nSo, a way around is to replace `loss = criterion(outputs,target.view(1, -1))` with `loss = criterion(outputs,target.view(-1, 1))` and change the `output_channels` of last linear layer to `1` instead of `133`. In this way, both of `outputs` and `target` shape will be equal and we can compute `MSE` value then.\r\n\r\nLearn more about pytorch `MSE` loss function from [here][1].\r\n\r\n\r\n  [1]: https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#loss-function"
    },
    {
        "question_id": "56850711",
        "accepted_answer_id": "57673328",
        "question_title": "ImportError: Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training",
        "question_markdown": "cannot install apex for distributed and fp16 training of bert model\r\ni have tried to install by cloning the apex from github and tried to install packages using pip\r\n\r\ni have tried to install apex by cloning from git hub using following command:\r\n\r\ngit clone https://github.com/NVIDIA/apex.git\r\n\r\nand cd apex to goto apex directory and tried to install package using following pip command:\r\n\r\npip install -v --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot;\r\n\r\nfull code is:\r\n\r\n    def main(server_ip,server_port,local_rank,no_cuda,fp16,train_batch_size,gradient_accumulation_steps,seed,do_train,do_eval,output_dir,task_name,data_dir,do_lower_case,bert_model,num_train_epochs,cache_dir,learning_rate,warmup_proportion,loss_scale,max_seq_length):\r\n            if server_ip and server_port:\r\n                # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\r\n                import ptvsd\r\n                print(&quot;Waiting for debugger attach&quot;)\r\n                ptvsd.enable_attach(address=(server_ip, server_port), redirect_output=True)\r\n                ptvsd.wait_for_attach()\r\n            \r\n            processors = {&quot;ner&quot;:NerProcessor}\r\n            print(processors)\r\n        \r\n            if local_rank == -1 or no_cuda:\r\n                device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() and not no_cuda else &quot;cpu&quot;)\r\n                n_gpu = torch.cuda.device_count()\r\n            else:\r\n                torch.cuda.set_device(local_rank)\r\n                device = torch.device(&quot;cuda&quot;, local_rank)\r\n                n_gpu = 1\r\n                # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\r\n                torch.distributed.init_process_group(backend=&#39;nccl&#39;)\r\n            logger.info(&quot;device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}&quot;.format(\r\n                device, n_gpu, bool(local_rank != -1), fp16))\r\n        \r\n            if gradient_accumulation_steps &lt; 1:\r\n                raise ValueError(&quot;Invalid gradient_accumulation_steps parameter: {}, should be &gt;= 1&quot;.format(\r\n                                    args.gradient_accumulation_steps))\r\n            \r\n            train_batch_size = train_batch_size // gradient_accumulation_steps\r\n        \r\n            random.seed(seed)\r\n            np.random.seed(seed)\r\n            torch.manual_seed(seed)\r\n        \r\n            if not do_train and not do_eval:\r\n                raise ValueError(&quot;At least one of `do_train` or `do_eval` must be True.&quot;)\r\n        \r\n            if os.path.exists(output_dir) and os.listdir(output_dir) and do_train:\r\n                raise ValueError(&quot;Output directory ({}) already exists and is not empty.&quot;.format(output_dir))\r\n            if not os.path.exists(output_dir):\r\n                os.makedirs(output_dir)\r\n            \r\n            task_name = task_name.lower()\r\n        \r\n            if task_name not in processors:\r\n                raise ValueError(&quot;Task not found: %s&quot; % (task_name))\r\n        \r\n            processor = processors[task_name]()\r\n            label_list = processor.get_labels()\r\n            num_labels = len(label_list) + 1\r\n        \r\n            tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=do_lower_case)\r\n        \r\n            train_examples = None\r\n            num_train_optimization_steps = None\r\n            if do_train:\r\n                train_examples = processor.get_train_examples(data_dir)\r\n                num_train_optimization_steps = int(\r\n                    len(train_examples) / train_batch_size / gradient_accumulation_steps) * num_train_epochs\r\n                if local_rank != -1:\r\n                    num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\r\n            \r\n        #     # Prepare model\r\n            cache_dir = cache_dir if cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), &#39;distributed_{}&#39;.format(local_rank))\r\n            model = Ner.from_pretrained(bert_model,\r\n                      cache_dir=cache_dir,\r\n                      num_labels = num_labels)\r\n            if fp16:\r\n                model.half()\r\n            # model.cuda()\r\n            model.to(device)\r\n            if local_rank != -1:\r\n                try:\r\n                    from apex.parallel import DistributedDataParallel as DDP\r\n                except ImportError:\r\n                    raise ImportError(&quot;Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.&quot;)\r\n        \r\n                model = DDP(model)\r\n            elif n_gpu &gt; 1:\r\n                model = torch.nn.DataParallel(model)\r\n        \r\n            param_optimizer = list(model.named_parameters())\r\n            no_decay = [&#39;bias&#39;, &#39;LayerNorm.bias&#39;, &#39;LayerNorm.weight&#39;]\r\n            optimizer_grouped_parameters = [\r\n                {&#39;params&#39;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.01},\r\n                {&#39;params&#39;: [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.0}\r\n                 ]\r\n            if fp16:\r\n                try:\r\n                    from apex.optimizers import FP16_Optimizer\r\n                    from apex.optimizers import FusedAdam\r\n                except ImportError:\r\n                    raise ImportError(&quot;Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.&quot;)\r\n        \r\n                optimizer = FusedAdam(optimizer_grouped_parameters,\r\n                                      lr=learning_rate,\r\n                                      bias_correction=False,\r\n                                      max_grad_norm=1.0)\r\n                if loss_scale == 0:\r\n                    optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\r\n                else:\r\n                    optimizer = FP16_Optimizer(optimizer, static_loss_scale=loss_scale)\r\n        \r\n            else:\r\n                optimizer = BertAdam(optimizer_grouped_parameters,\r\n                                     lr=learning_rate,\r\n                                     warmup=warmup_proportion,\r\n                                     t_total=num_train_optimization_steps)\r\n        \r\n            global_step = 0\r\n            nb_tr_steps = 0\r\n            tr_loss = 0\r\n            label_map = {i : label for i, label in enumerate(label_list,1)}\r\n            if do_train:\r\n                train_features = convert_examples_to_features(\r\n                    train_examples, label_list, max_seq_length, tokenizer)\r\n                logger.info(&quot;***** Running training *****&quot;)\r\n                logger.info(&quot;  Num examples = %d&quot;, len(train_examples))\r\n                logger.info(&quot;  Batch size = %d&quot;, train_batch_size)\r\n                logger.info(&quot;  Num steps = %d&quot;, num_train_optimization_steps)\r\n                all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\r\n                all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\r\n                all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\r\n                all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\r\n                all_valid_ids = torch.tensor([f.valid_ids for f in train_features], dtype=torch.long)\r\n                all_lmask_ids = torch.tensor([f.label_mask for f in train_features], dtype=torch.long)\r\n                train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids,all_valid_ids,all_lmask_ids)\r\n                if local_rank == -1:\r\n                    train_sampler = RandomSampler(train_data)\r\n                else:\r\n                    train_sampler = DistributedSampler(train_data)\r\n                train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)\r\n        \r\n                model.train()\r\n                for _ in trange(int(num_train_epochs), desc=&quot;Epoch&quot;):\r\n                    tr_loss = 0\r\n                    nb_tr_examples, nb_tr_steps = 0, 0\r\n                    for step, batch in enumerate(tqdm(train_dataloader, desc=&quot;Iteration&quot;)):\r\n                        batch = tuple(t.to(device) for t in batch)\r\n                        input_ids, input_mask, segment_ids, label_ids, valid_ids,l_mask = batch\r\n                        loss = model(input_ids, segment_ids, input_mask, label_ids,valid_ids,l_mask)\r\n                        del loss\r\n                        if n_gpu &gt; 1:\r\n                            loss = loss.mean() # mean() to average on multi-gpu.\r\n                        if gradient_accumulation_steps &gt; 1:\r\n                            loss = loss / gradient_accumulation_steps\r\n        \r\n                        if fp16:\r\n                            optimizer.backward(loss)\r\n                        else:\r\n                            loss.backward()\r\n        \r\n                        tr_loss += loss.item()\r\n                        nb_tr_examples += input_ids.size(0)\r\n                        nb_tr_steps += 1\r\n                        if (step + 1) % gradient_accumulation_steps == 0:\r\n                            if fp16:\r\n                                # modify learning rate with special warm up BERT uses\r\n                                # if args.fp16 is False, BertAdam is used that handles this automatically\r\n                                lr_this_step = learning_rate * warmup_linear(global_step/num_train_optimization_steps, warmup_proportion)\r\n                                for param_group in optimizer.param_groups:\r\n                                    param_group[&#39;lr&#39;] = lr_this_step\r\n                            optimizer.step()\r\n                            optimizer.zero_grad()\r\n                            global_step += 1\r\n\r\nmain(&#39;&#39;,&#39;&#39;,-1,True,True,8,1,42,True,True,&#39;jpt&#39;,&#39;ner&#39;,&#39;data/&#39;,True,&#39;bert-base-cased&#39;,5,&#39;cache_dir&#39;,5e-5,0.4,0,128)",
        "accepted_answer_markdown": "This worked for me:\r\n\r\n    import os, sys, shutil\r\n    import time\r\n    import gc\r\n    from contextlib import contextmanager\r\n    from pathlib import Path\r\n    import random\r\n    import numpy as np, pandas as pd\r\n    from tqdm import tqdm, tqdm_notebook\r\n    \r\n    @contextmanager\r\n    def timer(name):\r\n        t0 = time.time()\r\n        yield\r\n        print(f&#39;[{name}] done in {time.time() - t0:.0f} s&#39;)\r\n        \r\n    USE_APEX = True\r\n    \r\n    if USE_APEX:\r\n                with timer(&#39;install Nvidia apex&#39;):\r\n                    # Installing Nvidia Apex\r\n                    os.system(&#39;git clone https://github.com/NVIDIA/apex; cd apex; pip install -v --no-cache-dir&#39; + \r\n                              &#39; --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; ./&#39;)\r\n                    os.system(&#39;rm -rf apex/.git&#39;) # too many files, Kaggle fails\r\n                    from apex import amp"
    },
    {
        "question_id": "56971163",
        "accepted_answer_id": "56971788",
        "question_title": "NotImplementedError: when i try to create a DataLoader object in Pytorch",
        "question_markdown": "I created a custom Dataset class that inherits from PyTorch&#39;s Dataset class, in order to handle my custom dataset which i already preprocessed.  \r\n\r\nWhen i try to create a DataLoader object, i get this error:\r\n\r\n```\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py in __init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\r\n    174             if sampler is None:\r\n    175                 if shuffle:\r\n--&gt; 176                     sampler = RandomSampler(dataset)\r\n    177                 else:\r\n    178                     sampler = SequentialSampler(dataset)\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py in __init__(self, data_source, replacement, num_samples)\r\n     62                              &quot;since a random permute will be performed.&quot;)\r\n     63 \r\n---&gt; 64         if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\r\n     65             raise ValueError(&quot;num_samples should be a positive integer &quot;\r\n     66                              &quot;value, but got num_samples={}&quot;.format(self.num_samples))\r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/sampler.py in num_samples(self)\r\n     70         # dataset size might change at runtime\r\n     71         if self._num_samples is None:\r\n---&gt; 72             return len(self.data_source)\r\n     73         return self._num_samples\r\n     74 \r\n\r\n/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py in __len__(self)\r\n     18 \r\n     19     def __len__(self):\r\n---&gt; 20         raise NotImplementedError\r\n     21 \r\n     22     def __add__(self, other):\r\n\r\nNotImplementedError: \r\n\r\n```\r\n\r\nSo, the error message regards to the not implementation of the __len__() function in the dataset.py, right?  But i did implement it and the __getitem__(), __init__() as well .\r\n\r\nHow can i overcome this? \r\nThank you",
        "accepted_answer_markdown": "Make sure the name is correct in your code. It should be `__len__`. "
    },
    {
        "question_id": "57142401",
        "accepted_answer_id": "57217969",
        "question_title": "How to fix &quot;RuntimeError: Function AddBackward0 returned an invalid gradient at index 1 - expected type torch.FloatTensor but got torch.LongTensor&quot;",
        "question_markdown": "I encountered this bug when I am running the machine translation code.\r\n\r\n&gt; RuntimeError                              Traceback (most recent call\r\n&gt; last) &lt;ipython-input-49-b0ce1fe22758&gt; in &lt;module&gt;\r\n&gt;       5 decoder = Decoder(len(out_vocab), embed_size, num_hiddens, num_layers,\r\n&gt;       6                   attention_size, drop_prob)\r\n&gt; ----&gt; 7 train(encoder, decoder, dataset, lr, batch_size, num_epochs)\r\n&gt; \r\n&gt; &lt;ipython-input-48-0faa11e92493&gt; in train(encoder, decoder, dataset,\r\n&gt; lr, batch_size, num_epochs)\r\n&gt;      13             dec_optimizer.zero_grad()\r\n&gt;      14             l = batch_loss(encoder, decoder, X, Y, loss)\r\n&gt; ---&gt; 15             l.backward()\r\n&gt;      16             enc_optimizer.step()\r\n&gt;      17             dec_optimizer.step()\r\n&gt; \r\n&gt; /usr/lib64/python3.6/site-packages/torch/tensor.py in backward(self,\r\n&gt; gradient, retain_graph, create_graph)\r\n&gt;     105                 products. Defaults to ``False``.\r\n&gt;     106         &quot;&quot;&quot;\r\n&gt; --&gt; 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n&gt;     108 \r\n&gt;     109     def register_hook(self, hook):\r\n&gt; \r\n&gt; /usr/lib64/python3.6/site-packages/torch/autograd/__init__.py in\r\n&gt; backward(tensors, grad_tensors, retain_graph, create_graph,\r\n&gt; grad_variables)\r\n&gt;      91     Variable._execution_engine.run_backward(\r\n&gt;      92         tensors, grad_tensors, retain_graph, create_graph,\r\n&gt; ---&gt; 93         allow_unreachable=True)  # allow_unreachable flag\r\n&gt;      94 \r\n&gt;      95 \r\n&gt; \r\n&gt; RuntimeError: Function AddBackward0 returned an invalid gradient at\r\n&gt; index 1 - expected type torch.FloatTensor but got torch.LongTensor\r\n\r\n\r\nI think the bug is in the batch_loss function. But I don&#39;t know why and can&#39;t fix it.\r\n\r\n```python\r\ndef batch_loss(encoder, decoder, X, Y, loss):\r\n    batch_size = X.shape[0]\r\n    enc_state = None\r\n    enc_outputs, enc_state = encoder(X, enc_state)\r\n    # \u521d\u59cb\u5316\u89e3\u7801\u5668\u7684\u9690\u85cf\u72b6\u6001\r\n    dec_state = decoder.begin_state(enc_state)\r\n    # \u89e3\u7801\u5668\u5728\u6700\u521d\u65f6\u95f4\u6b65\u7684\u8f93\u5165\u662fBOS\r\n    dec_input = torch.tensor([out_vocab.stoi[BOS]] * batch_size)\r\n    # \u6211\u4eec\u5c06\u4f7f\u7528\u63a9\u7801\u53d8\u91cfmask\u6765\u5ffd\u7565\u6389\u6807\u7b7e\u4e3a\u586b\u5145\u9879PAD\u7684\u635f\u5931\r\n    mask, num_not_pad_tokens = torch.ones(batch_size), 0\r\n    l = torch.tensor([0])\r\n    for y in Y.t():\r\n        dec_output, dec_state = decoder(dec_input, dec_state, enc_outputs)\r\n        l = l + (mask * loss(dec_output, y)).sum()\r\n        dec_input = y  # \u4f7f\u7528\u5f3a\u5236\u6559\u5b66\r\n        num_not_pad_tokens += mask.sum().item()\r\n        # \u5f53\u9047\u5230EOS\u65f6\uff0c\u5e8f\u5217\u540e\u9762\u7684\u8bcd\u5c06\u5747\u4e3aPAD\uff0c\u76f8\u5e94\u4f4d\u7f6e\u7684\u63a9\u7801\u8bbe\u62100\r\n        mask = mask * (y != out_vocab.stoi[EOS]).float()\r\n    return l / num_not_pad_tokens\r\n\r\ndef train(encoder, decoder, dataset, lr, batch_size, num_epochs):\r\n    d2lt.params_init(encoder, init=nn.init.xavier_uniform_)\r\n    d2lt.params_init(decoder, init=nn.init.xavier_uniform_)\r\n\r\n    enc_optimizer = optim.Adam(encoder.parameters(), lr=lr)\r\n    dec_optimizer = optim.Adam(decoder.parameters(), lr=lr)\r\n    loss = nn.CrossEntropyLoss(reduction=&#39;none&#39;)\r\n    data_iter = tdata.DataLoader(dataset, batch_size, shuffle=True)\r\n    for epoch in range(num_epochs):\r\n        l_sum = 0.0\r\n        for X, Y in data_iter:\r\n            enc_optimizer.zero_grad()\r\n            dec_optimizer.zero_grad()\r\n            l = batch_loss(encoder, decoder, X, Y, loss)\r\n            l.backward()\r\n            enc_optimizer.step()\r\n            dec_optimizer.step()\r\n            l_sum += l.item()\r\n        if (epoch + 1) % 10 == 0:\r\n            print(&quot;epoch %d, loss %.3f&quot; % (epoch + 1, l_sum / len(data_iter)))\r\n```\r\n\r\nLooking forward to a positive reply.",
        "accepted_answer_markdown": "Thank Proyag. Just replace `l = torch.tensor([0])` with `l = torch.tensor([0], dtype=torch.float)` solves my problem."
    },
    {
        "question_id": "57237381",
        "accepted_answer_id": "57238103",
        "question_title": "&quot;RuntimeError: Expected 4-dimensional input for 4-dimensional weight 32 3 3, but got 3-dimensional input of size [3, 224, 224] instead&quot;?",
        "question_markdown": "I am trying to use a pre-trained model. Here&#39;s where the problem occurs\r\n\r\nIsn&#39;t the model supposed to take in a simple colored image? Why is it expecting a 4-dimensional input?\r\n\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-51-d7abe3ef1355&gt; in &lt;module&gt;()\r\n         33 \r\n         34 # Forward pass the data through the model\r\n    ---&gt; 35 output = model(data)\r\n         36 init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\r\n         37 \r\n    \r\n    5 frames\r\n    /usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py in forward(self, input)\r\n        336                             _pair(0), self.dilation, self.groups)\r\n        337         return F.conv2d(input, self.weight, self.bias, self.stride,\r\n    --&gt; 338                         self.padding, self.dilation, self.groups)\r\n        339 \r\n        340 \r\n    \r\n    RuntimeError: Expected 4-dimensional input for 4-dimensional weight 32 3 3, but got 3-dimensional input of size [3, 224, 224] instead\r\n\r\nWhere \r\n\r\n    inception = models.inception_v3()\r\n    model = inception.to(device)\r\n\r\n",
        "accepted_answer_markdown": "As [Usman Ali][1] wrote in his comment, pytorch (and most other DL toolboxes) expects a *batch* of images as an input. Thus you need to call\r\n\r\n```python\r\noutput = model(data[None, ...])  \r\n```\r\n\r\nInserting a singleton &quot;batch&quot; dimension to your input `data`.\r\n\r\nPlease also note that the model you are using might expect a different input size (3x229x229) and not 3x224x224.\r\n\r\n [1]: https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but#comment100977502_57237381"
    },
    {
        "question_id": "57500582",
        "accepted_answer_id": "57509265",
        "question_title": "Unable to figure out inplace operation in the pytorch code?",
        "question_markdown": "I have the following implementation in PyTorch for learning using LSTM:\r\n\r\nhttps://gist.github.com/rahulbhadani/f1d64042cc5a80280755cac262aa48aa\r\n\r\nHowever, the code is experiencing in-place operation error \r\n\r\nwhere my error output is:\r\n\r\n    /home/ivory/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n      # Remove the CWD from sys.path while we load stuff.\r\n    ---------------------------------------------------------------------------\r\n    RuntimeError                              Traceback (most recent call last)\r\n    &lt;ipython-input-86-560ec78f2b64&gt; in &lt;module&gt;\r\n         27 linear = torch.nn.Linear(hidden_nums, output_dim)\r\n         28 \r\n    ---&gt; 29 global_loss_list = global_training(lstm2)\r\n    \r\n    &lt;ipython-input-84-152890a3028c&gt; in global_training(optimizee)\r\n          3     adam_global_optimizer = torch.optim.Adam([{&#39;params&#39;: optimizee.parameters()}, \r\n          4                                      {&#39;params&#39;:linear.parameters()}], lr = 0.0001)\r\n    ----&gt; 5     _, global_loss_1 = learn2(LSTM_Optimizee, training_steps, retain_graph_flag=True, reset_theta=True)\r\n          6 \r\n          7     print(global_loss_1)\r\n    \r\n    &lt;ipython-input-83-0357a528b94d&gt; in learn2(optimizee, unroll_train_steps, retain_graph_flag, reset_theta)\r\n         43             # requires_grad=True. These are accumulated into x.grad for every\r\n         44             # parameter x. In pseudo-code: x.grad += dloss/dx\r\n    ---&gt; 45             loss.backward(retain_graph = retain_graph_flag) #The default is False, when the optimized LSTM is set to True\r\n         46 \r\n         47             print(&#39;x.grad: {}&#39;.format(x.grad))\r\n    \r\n    ~/anaconda3/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\r\n        116                 products. Defaults to ``False``.\r\n        117         &quot;&quot;&quot;\r\n    --&gt; 118         torch.autograd.backward(self, gradient, retain_graph, create_graph)\r\n        119 \r\n        120     def register_hook(self, hook):\r\n    \r\n    ~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\r\n         91     Variable._execution_engine.run_backward(\r\n         92         tensors, grad_tensors, retain_graph, create_graph,\r\n    ---&gt; 93         allow_unreachable=True)  # allow_unreachable flag\r\n         94 \r\n         95 \r\n    \r\n    RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 10]] is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\r\n\r\n\r\nI have tried to trace the error but didn&#39;t succeed. Any help in this regard will be appreciated.\r\n\r\nThanks.",
        "accepted_answer_markdown": "I think the issue is with the following line: \r\n\r\n    global_loss_list.append(global_loss.detach_())\r\n\r\nThe convention in PyTorch for in-place operations is using `_` at the end of the function name (as in `detach_`). I believe you shouldn&#39;t be detaching in-place. In other words, change `detach_` to `detach` "
    },
    {
        "question_id": "57530038",
        "accepted_answer_id": "57534734",
        "question_title": "Concatenation step of U-Net for unequal number of channels",
        "question_markdown": "I am trying to implement the U-NET architecture for image segmentation while implementing the crop and concatenation step in the expansive path, I am unable to understand how the unequal number of channels are concatenated.\r\n\r\n[![Unet][2]][2]\r\n\r\n\r\nAccording to the architecture, the input from the first upsampling step has to be concatenated from the corresponding output from contracting path but the problem is number of channels in contracting path is 512 while after upsampling step they are 1024, how they are supposed to be concatenated.My code for the crop and concatenate is -\r\n\r\n    def crop_and_concat(self, upsampled, bypass, crop=False):\r\n        if crop:\r\n            c = (bypass.size()[2] - upsampled.size()[2]) // 2\r\n            bypass = F.pad(bypass, (-c, -c, -c, -c))\r\n        return torch.cat((upsampled, bypass), 1)\r\n\r\n    \r\n\r\nThe error I am receiving- \r\n`RuntimeError: Given groups=1, weight of size 128 256 5 5, expected input[4, 384, 64, 64] to have 256 channels, but got 384 channels instead`\r\n&lt;br&gt;Where I am doing wrong?\r\n\r\n\r\n  [1]: https://i.sstatic.net/rsLOt.png\r\n  [2]: https://i.sstatic.net/bB2tL.jpg",
        "accepted_answer_markdown": "First of all, you don&#39;t have to be so strict when it comes to U-Net like architectures, there were many derivatives afterwards (see for example [fastai variation with PixelShuffle](https://docs.fast.ai/vision.models.unet.html)).\n\nIn the case of encoder, in the basic version, your channels go (per block):\n\n    1 - 64 - 128 - 256 - 512\n\nStandard convolutional encoder.\nAfter that is a shared layer of  `1024`.\n\nIn decoder, it goes downwards, __but has more channels as you are concatenating encoder states from each block__.\n\nIt would be:\n\n&gt; 1024 -&gt; 512 -&gt; 512 (decoder) + 512 (encoder), 1024 total -&gt; 512\n&gt;\n&gt; 512 -&gt; 256 -&gt; 256 (decoder) + 256 (encoder), 512 total -&gt; 256\n\nand so on.\n\nYou were at the case where `256` from decoder was taken in the account, but `128` added from encoder wasn&#39;t. Just up your channels to 256 + 128 and follow the above scheme for each block of your UNet."
    },
    {
        "question_id": "57534072",
        "accepted_answer_id": "57534659",
        "question_title": "How To Fix: RuntimeError: size mismatch in pyTorch",
        "question_markdown": "I am new to pyTorch and getting following Size Mismatch error: \r\n    \r\n    RuntimeError: size mismatch, m1: [7 x 2092500], m2: [180 x 120] at ..\\aten\\src\\TH/generic/THTensorMath.cpp:961\r\n\r\nModel:\r\n\r\n    class Net(nn.Module):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.conv1 = nn.Conv2d(3, 200, 5)\r\n        self.pool = nn.MaxPool2d(2, 2)\r\n        self.conv2 = nn.Conv2d(200, 180, 5)\r\n        self.fc1 = nn.Linear(180, 120)\r\n        self.fc2 = nn.Linear(120, 84)\r\n        self.fc3 = nn.Linear(84,5)     \r\n\r\n    def forward(self, x):\r\n        x = self.pool(F.relu(self.conv1(x)))\r\n        x = self.pool(F.relu(self.conv2(x)))\r\n        x = x.view(x.shape[0], -1)\r\n        x = F.relu(self.fc1(x))\r\n        x = F.relu(self.fc2(x))\r\n        x = self.fc3(x)\r\n        return x\r\n\r\n\r\nHow ever I tried changing `x = x.view(x.shape[0], -1)` to `x = x.view(x.size(0), -1)` but that also did&#39;nt work. Dimension of images is 512x384. and have used following transformation:\r\n\r\n    def load_dataset():\r\n    data_path = &#39;./dataset/training&#39;\r\n    \r\n    transform = transforms.Compose(\r\n                   [transforms.Resize((512,384)),\r\n                    transforms.ToTensor(),\r\n                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\r\n    \r\n   \r\n    train_dataset = torchvision.datasets.ImageFolder(root=data_path,transform=transform)\r\n    train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=7,num_workers=0,shuffle=True)\r\n    \r\n    return train_loader",
        "accepted_answer_markdown": "The problem is that the dimensions of the output of your last max pooling layer don&#39;t match the input of the first fully connected layer. This is the network structure until the last max pool layer for input shape `(3, 512, 384)`:\r\n\r\n    ----------------------------------------------------------------\r\n            Layer (type)               Output Shape         Param #\r\n    ================================================================\r\n                Conv2d-1        [-1, 200, 508, 380]          15,200\r\n             MaxPool2d-2        [-1, 200, 254, 190]               0\r\n                Conv2d-3        [-1, 180, 250, 186]         900,180\r\n             MaxPool2d-4         [-1, 180, 125, 93]               0\r\n    ================================================================\r\n\r\nThe last row of the table means that `MaxPool2d-4` outputs 180 channels (filter outputs) of 125 width and 93 height. So you need your first fully connected layer to have `180 * 125 * 93 = 2092500` input size. This is a lot, so I&#39;d advise you to refine your architecture. In any case, if you change the input size of the first fully connected layer to `2092500`, it works:\r\n\r\n    class Net(nn.Module):\r\n        def __init__(self):\r\n            super(Net, self).__init__()\r\n            self.conv1 = nn.Conv2d(3, 200, 5)\r\n            self.pool = nn.MaxPool2d(2, 2)\r\n            self.conv2 = nn.Conv2d(200, 180, 5)\r\n            #self.fc1 = nn.Linear(180, 120)\r\n            self.fc1 = nn.Linear(2092500, 120)\r\n            self.fc2 = nn.Linear(120, 84)\r\n            self.fc3 = nn.Linear(84,5)\r\n    \r\n        def forward(self, x):\r\n            x = self.pool(F.relu(self.conv1(x)))\r\n            x = self.pool(F.relu(self.conv2(x)))\r\n            x = x.view(x.shape[0], -1)\r\n            x = F.relu(self.fc1(x))\r\n            x = F.relu(self.fc2(x))\r\n            x = self.fc3(x)\r\n            return x\r\n\r\nGiving the following architecture:\r\n\r\n    ----------------------------------------------------------------\r\n            Layer (type)               Output Shape         Param #\r\n    ================================================================\r\n                Conv2d-1        [-1, 200, 508, 380]          15,200\r\n             MaxPool2d-2        [-1, 200, 254, 190]               0\r\n                Conv2d-3        [-1, 180, 250, 186]         900,180\r\n             MaxPool2d-4         [-1, 180, 125, 93]               0\r\n                Linear-5                  [-1, 120]     251,100,120\r\n                Linear-6                   [-1, 84]          10,164\r\n                Linear-7                    [-1, 5]             425\r\n    ================================================================\r\n    Total params: 252,026,089\r\n    Trainable params: 252,026,089\r\n    Non-trainable params: 0\r\n\r\n(You can use the [torchsummary](https://github.com/sksq96/pytorch-summary) package to generate these tables.)"
    },
    {
        "question_id": "57618507",
        "accepted_answer_id": "57876684",
        "question_title": "fastai error predicting with exported/reloaded model: &quot;Input type and weight type should be the same&quot;",
        "question_markdown": "Whenever I export a fastai model and reload it, I get this error (or a very similar one) when I try and use the reloaded model to generate predictions on a new test set:\r\n\r\n```\r\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same\r\n```\r\n\r\nMinimal reprodudeable code example below, you just need to update your `FILES_DIR` variable to where the MNIST data gets deposited on your system:\r\n\r\n```\r\nfrom fastai import *\r\nfrom fastai.vision import *\r\n\r\n# download data for reproduceable example\r\nuntar_data(URLs.MNIST_SAMPLE)\r\nFILES_DIR = &#39;/home/mepstein/.fastai/data/mnist_sample&#39;  # this is where command above deposits the MNIST data for me\r\n\r\n\r\n# Create FastAI databunch for model training\r\ntfms = get_transforms()\r\ntr_val_databunch = ImageDataBunch.from_folder(path=FILES_DIR,  # location of downloaded data shown in log of prev command\r\n                                train = &#39;train&#39;,\r\n                                valid_pct = 0.2,\r\n                                ds_tfms = tfms).normalize()\r\n\r\n# Create Model\r\nconv_learner = cnn_learner(tr_val_databunch, \r\n                           models.resnet34, \r\n                           metrics=[error_rate]).to_fp16()\r\n\r\n# Train Model\r\nconv_learner.fit_one_cycle(4)\r\n\r\n# Export Model\r\nconv_learner.export()  # saves model as &#39;export.pkl&#39; in path associated with the learner\r\n\r\n# Reload Model and use it for inference on new hold-out set\r\nreloaded_model = load_learner(path = FILES_DIR,\r\n                              test = ImageList.from_folder(path = f&#39;{FILES_DIR}/valid&#39;))\r\n\r\npreds = reloaded_model.get_preds(ds_type=DatasetType.Test)\r\n```\r\nOutput:\r\n\r\n&gt; &quot;RuntimeError: Input type (torch.cuda.FloatTensor) and weight type\r\n&gt; (torch.cuda.HalfTensor) should be the same&quot;\r\n\r\nStepping through the code statement by statement, everything works fine until the last line `pred = ...` which is where the torch error above pops up.\r\n\r\nRelevant software versions:\r\n\r\nPython 3.7.3\r\nfastai           1.0.57   \r\ntorch            1.2.0    \r\ntorchvision      0.4.0 ",
        "accepted_answer_markdown": "So the answer to this ended up being relatively simple:\r\n\r\n1) As noted in my comment, training in mixed precision mode (setting `conv_learner` `to_fp16()`) caused the error with the exported/reloaded model\r\n\r\n2) To train in mixed precision mode (which is faster than regular training) and enable export/reload of the model without errors, simply set the model back to default precision before exporting.\r\n\r\n...In code, simply changing the example above:\r\n```\r\n# Export Model\r\nconv_learner.export()\r\n```\r\n\r\nto:\r\n```\r\n# Export Model (after converting back to default precision for safe export/reload\r\nconv_learner = conv_learner.to_fp32()\r\nconv_learner.export()\r\n```\r\n\r\n...and now the full (reproduceable) code example above runs without errors, including the prediction after model reload."
    },
    {
        "question_id": "57798033",
        "accepted_answer_id": "57802321",
        "question_title": "ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 1]))",
        "question_markdown": "    ValueError                                Traceback (most recent call last)\r\n    &lt;ipython-input-30-33821ccddf5f&gt; in &lt;module&gt;\r\n         23         output = model(data)\r\n         24         # calculate the batch loss\r\n    ---&gt; 25         loss = criterion(output, target)\r\n         26         # backward pass: compute gradient of the loss with respect to model parameters\r\n         27         loss.backward()\r\n    \r\n    C:\\Users\\mnauf\\Anaconda3\\envs\\federated_learning\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\r\n        487             result = self._slow_forward(*input, **kwargs)\r\n        488         else:\r\n    --&gt; 489             result = self.forward(*input, **kwargs)\r\n        490         for hook in self._forward_hooks.values():\r\n        491             hook_result = hook(self, input, result)\r\n    \r\n    C:\\Users\\mnauf\\Anaconda3\\envs\\federated_learning\\lib\\site-packages\\torch\\nn\\modules\\loss.py in forward(self, input, target)\r\n        593                                                   self.weight,\r\n        594                                                   pos_weight=self.pos_weight,\r\n    --&gt; 595                                                   reduction=self.reduction)\r\n        596 \r\n        597 \r\n    \r\n    C:\\Users\\mnauf\\Anaconda3\\envs\\federated_learning\\lib\\site-packages\\torch\\nn\\functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)\r\n       2073 \r\n       2074     if not (target.size() == input.size()):\r\n    -&gt; 2075         raise ValueError(&quot;Target size ({}) must be the same as input size ({})&quot;.format(target.size(), input.size()))\r\n       2076 \r\n       2077     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)\r\n    \r\n    ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 1]))\r\n\r\nI am training a CNN. Working on the Horses vs humans dataset. [This is my code](https://github.com/mnauf/horses_humans/blob/master/Untitled.ipynb). I am using `criterion = nn.BCEWithLogitsLoss()` and `optimizer = optim.RMSprop(model.parameters(), lr=0.01`). My final layer is `self.fc2 = nn.Linear(512, 1)`. Out last neuron, will output 1 for horse and 0 for human, right? or should I choose 2 neurons for output?\r\n\r\n`16` is the batch size. Since the error says `ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 1]))`. I don&#39;t understand, where do I need to make change, to rectify the error.",
        "accepted_answer_markdown": "`target = target.unsqueeze(1)`, before passing target to criterion, changed the target tensor size from `[16]` to `[16,1]`. Doing it solved the issue. Furthermore, I also needed to do `target = target.float()` before passing it to criterion, because our outputs are in float. Besides, there was another error in the code. I was using sigmoid activation function in the last layer, but I shouldn\u2019t because the criterion I am using already comes with sigmoid builtin."
    },
    {
        "question_id": "58059221",
        "accepted_answer_id": "58059613",
        "question_title": "Pytorch: ValueError: Expected input batch_size (32) to match target batch_size (64)",
        "question_markdown": "Tried to run the CNN examples on MNIST dataset, batch size=64, channel =1, n_h=28, n_w=28, n_iters = 1000. The program runs for first 500 interation and then gives the above mentioned error.\r\nThere are same topics already being discussed on the forum such as : [topic 1][1]\r\nand [topic 2][2], but none of them could help me identify the mistake in the following code:\r\n\r\n    class CNN_MNIST(nn.Module):\r\n    def __init__(self):\r\n        super(CNN_MNIST,self).__init__()\r\n        \r\n        # convolution layer 1\r\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels= 32, kernel_size=5,\r\n                              stride=1,padding=2)\r\n        \r\n        # ReLU activation \r\n        self.relu1 = nn.ReLU()\r\n        \r\n        # maxpool 1\r\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2)\r\n        \r\n        # convolution 2\r\n        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5,\r\n                              stride=1,padding=2)\r\n        \r\n        # ReLU activation \r\n        self.relu2 = nn.ReLU()\r\n        \r\n        # maxpool 2\r\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)\r\n        \r\n        # fully connected 1\r\n        self.fc1 = nn.Linear(7*7*64,1000)\r\n        # fully connected 2\r\n        self.fc2 = nn.Linear(1000,10)\r\n        \r\n    def forward(self,x):\r\n        \r\n        # convolution 1\r\n        out = self.cnn1(x)\r\n        # activation function\r\n        out = self.relu1(out)\r\n        # maxpool 1\r\n        out = self.maxpool1(out)\r\n        \r\n        # convolution 2\r\n        out = self.cnn2(out)\r\n        # activation function\r\n        out = self.relu2(out)\r\n        # maxpool 2\r\n        out = self.maxpool2(out)\r\n        \r\n        # flatten the output\r\n        out = out.view(out.size(0),-1)\r\n        \r\n        # fully connected layers\r\n        out = self.fc1(out)\r\n        out = self.fc2(out)\r\n        \r\n        return out\r\n    # model trainning\r\n    count = 0\r\n    loss_list = []\r\n    iteration_list = []\r\n    accuracy_list = []\r\n\r\n    for epoch in range(int(n_epochs)):\r\n        for i, (image,labels) in enumerate(train_loader):\r\n        \r\n        train = Variable(image)\r\n        labels = Variable(labels)\r\n        \r\n        # clear gradient\r\n        optimizer.zero_grad()\r\n        \r\n        # forward propagation\r\n        output = cnn_model(train)\r\n        \r\n        # calculate softmax and cross entropy loss\r\n        loss = error(output,label)\r\n        \r\n        # calculate gradients\r\n        loss.backward()\r\n        \r\n        # update the optimizer\r\n        optimizer.step()\r\n        \r\n        count += 1\r\n        \r\n        if count % 50 ==0:\r\n            # calculate the accuracy\r\n            correct = 0\r\n            total = 0\r\n            \r\n            # iterate through the test data\r\n            for image, labels in test_loader:\r\n                \r\n                test = Variable(image)\r\n                \r\n                # forward propagation\r\n                output = cnn_model(test)\r\n                \r\n                # get prediction\r\n                predict = torch.max(output.data,1)[1]\r\n                \r\n                # total number of labels\r\n                total += len(labels)\r\n                \r\n                # correct prediction\r\n                correct += (predict==labels).sum()\r\n                \r\n            # accuracy\r\n            accuracy = 100*correct/float(total)\r\n            \r\n            # store loss, number of iteration, and accuracy\r\n            loss_list.append(loss.data)\r\n            iteration_list.append(count)\r\n            accuracy_list.append(accuracy)\r\n            \r\n            # print loss and accurcay as the algorithm progresses\r\n            if count % 500 ==0:\r\n                print(&#39;Iteration :{}    Loss :{}    Accuracy :\r\n\r\n    {}&#39;.format(count,loss.item(),accuracy))\r\n\r\n  [1]: https://stackoverflow.com/questions/56719867/pytorch-expected-input-batch-size-12-to-match-target-batch-size-64\r\n  [2]: https://stackoverflow.com/questions/54928638/pytorch-cnn-error-expected-input-batch-size-4-to-match-target-batch-size-64\r\n\r\nThe error is as follows:\r\n\r\n        ---------------------------------------------------------------------------\r\n    ValueError                                Traceback (most recent call last)\r\n    &lt;ipython-input-19-9e93a242961b&gt; in &lt;module&gt;\r\n         18 \r\n         19         # calculate softmax and cross entropy loss\r\n    ---&gt; 20         loss = error(output,label)\r\n         21 \r\n         22         # calculate gradients\r\n    \r\n    ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\r\n        545             result = self._slow_forward(*input, **kwargs)\r\n        546         else:\r\n    --&gt; 547             result = self.forward(*input, **kwargs)\r\n        548         for hook in self._forward_hooks.values():\r\n        549             hook_result = hook(self, input, result)\r\n    \r\n    ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py in forward(self, input, target)\r\n        914     def forward(self, input, target):\r\n        915         return F.cross_entropy(input, target, weight=self.weight,\r\n    --&gt; 916                                ignore_index=self.ignore_index, reduction=self.reduction)\r\n        917 \r\n        918 \r\n    \r\n    ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)\r\n       1993     if size_average is not None or reduce is not None:\r\n       1994         reduction = _Reduction.legacy_get_string(size_average, reduce)\r\n    -&gt; 1995     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\r\n       1996 \r\n       1997 \r\n    \r\n    ~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\r\n       1820     if input.size(0) != target.size(0):\r\n       1821         raise ValueError(&#39;Expected input batch_size ({}) to match target batch_size ({}).&#39;\r\n    -&gt; 1822                          .format(input.size(0), target.size(0)))\r\n       1823     if dim == 2:\r\n       1824         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\r\n    \r\n    ValueError: Expected input batch_size (32) to match target batch_size (64).\r\n\r\n\r\n",
        "accepted_answer_markdown": "You are providing the wrong target to your loss:\r\n```python\r\nloss = error(output, label)\r\n```\r\nWhile your loader gives you\r\n```python\r\n    for i, (image,labels) in enumerate(train_loader):\r\n\r\n      train = Variable(image)\r\n      labels = Variable(labels)\r\n```\r\nSo you have a variable name `labels` (with `s`)  from the loader, yet you feed `label` (no `s`) to your loss. \r\n\r\nBatch size is the least of your worries."
    },
    {
        "question_id": "58157523",
        "accepted_answer_id": "58158572",
        "question_title": "RuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 &#39;mat2&#39; how to fix it?",
        "question_markdown": "```\r\n\r\nimport torch.nn as nn \r\nimport torch \r\nimport torch.optim as optim\r\nimport itertools\r\n\r\nclass net1(nn.Module):\r\n    def __init__(self):\r\n        super(net1,self).__init__()\r\n\r\n        self.pipe = nn.Sequential(\r\n            nn.Linear(10,10),\r\n            nn.ReLU()\r\n        )\r\n\r\n    def forward(self,x):\r\n        return self.pipe(x.long())\r\n\r\nclass net2(nn.Module):\r\n    def __init__(self):\r\n        super(net2,self).__init__()\r\n\r\n        self.pipe = nn.Sequential(\r\n            nn.Linear(10,20),\r\n            nn.ReLU(),\r\n            nn.Linear(20,10)\r\n        )\r\n\r\n    def forward(self,x):\r\n        return self.pipe(x.long())\r\n\r\n\r\n\r\nnetFIRST = net1()\r\nnetSECOND = net2()\r\n\r\nlearning_rate = 0.001\r\n\r\nopt = optim.Adam(itertools.chain(netFIRST.parameters(),netSECOND.parameters()), lr=learning_rate)\r\n\r\nepochs = 1000\r\n\r\nx = torch.tensor([1,2,3,4,5,6,7,8,9,10],dtype=torch.long)\r\ny = torch.tensor([10,9,8,7,6,5,4,3,2,1],dtype=torch.long)\r\n\r\n\r\nfor epoch in range(epochs):\r\n    opt.zero_grad()\r\n\r\n    prediction = netSECOND(netFIRST(x))\r\n    loss = (y.long() - prediction)**2\r\n    loss.backward()\r\n\r\n    print(loss)\r\n    print(prediction)\r\n    opt.step()\r\n```\r\n\r\nerror:\r\n\r\n&gt; line 49, in &lt;module&gt; prediction = netSECOND(netFIRST(x))\r\n&gt; \r\n&gt; line 1371, in linear; output = input.matmul(weight.t())\r\n&gt; \r\n&gt; RuntimeError: Expected object of scalar type Long but got scalar type\r\n&gt; Float for argument #2 &#39;mat2&#39;\r\n\r\n\r\nI don&#39;t really see what I&#39;m doing wrong. I have tried to turn everything in a `Long` in every possible way. I don&#39;t really get the way typing works for pytorch. Last time I tried something with just one layer and it forced me to use type `int`.\r\nCould someone explain how the typing is established in pytorch and how to prevent and fix errors like this??\r\nA lot I mean an awful lot of thanks in advance, this problem really bothers me and I can&#39;t seem to fix it no matter what I try.",
        "accepted_answer_markdown": "The weights are Floats, the inputs are Longs. This is not allowed. In fact, I don&#39;t think torch supports anything else than Floats in neural networks.\r\n\r\nIf you remove *all* calls to long, and define your input as floats, it will work (it does, I tried).\r\n\r\n(You will then get another unrelated error: you need to sum your loss)"
    },
    {
        "question_id": "58510249",
        "accepted_answer_id": "58512759",
        "question_title": "Pytorch Autograd: what does runtime error &quot;grad can be implicitly created only for scalar outputs&quot; mean",
        "question_markdown": "I am trying to understand Pytorch autograd in depth; I would like to observe the gradient of a simple tensor after going through a sigmoid function as below:\r\n\r\n    import torch\r\n    from torch import autograd \r\n    \r\n    D = torch.arange(-8, 8, 0.1, requires_grad=True)\r\n    \r\n    with autograd.set_grad_enabled(True):\r\n        S = D.sigmoid()\r\n    S.backward()\r\n\r\nMy goal is to get D.grad() but even before calling it I get the runtime error:\r\n\r\n    RuntimeError: grad can be implicitly created only for scalar outputs\r\n\r\nI see another [post][1] with similar question but the answer over there is not applied to my question.  Thanks \r\n\r\n\r\n  [1]: https://stackoverflow.com/questions/52317407/pytorch-autograd-grad-can-be-implicitly-created-only-for-scalar-outputs",
        "accepted_answer_markdown": "The error means you can only run `.backward` (with no arguments) on a unitary/scalar tensor. I.e. a tensor with a single element.\r\n\r\nFor example, you could do\r\n\r\n    T = torch.sum(S)\r\n    T.backward()\r\n\r\nsince `T` would be a scalar output.\r\n\r\nI posted some more information on using pytorch to compute derivatives of tensors in [this answer](https://stackoverflow.com/questions/57261254/using-automatic-differentiation-libraries-to-compute-partial-derivatives-of-an-a/57569121#57569121).\r\n"
    }
]