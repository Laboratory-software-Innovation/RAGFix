[
    {
        "question_id": "59013109",
        "accepted_answer_id": "59013131",
        "question_title": "RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same",
        "question_markdown": "This:\r\n\r\n```\r\ndevice = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\r\nmodel.to(device)\r\n\r\nfor data in dataloader:\r\n    inputs, labels = data\r\n    outputs = model(inputs)\r\n```\r\nGives the error: \r\n&gt;RuntimeError: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same\r\n",
        "accepted_answer_markdown": "You get this error because your model is on the GPU, but your data is on the CPU. So, you need to send your input tensors to the GPU.\r\n```python\r\ninputs, labels = data                         # this is what you had\r\ninputs, labels = inputs.cuda(), labels.cuda() # add this line\r\n```\r\nOr like this, to stay consistent with the rest of your code:\r\n```python\r\ndevice = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)\r\n\r\ninputs, labels = inputs.to(device), labels.to(device)\r\n``` \r\nThe __same error__ will be raised if your input tensors are on the GPU but your model weights aren&#39;t. In this case, you need to send your model weights to the GPU.\r\n```\r\nmodel = MyModel()\r\n\r\nif torch.cuda.is_available():\r\n    model.cuda()\r\n```\r\nSee the documentation for [`cuda()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cuda), and its opposite, [`cpu()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.cpu)."
    },
    {
        "question_id": "59115888",
        "accepted_answer_id": "59115958",
        "question_title": "transforms.Normalize() between 0 and 1 when using Lab",
        "question_markdown": "which mean, std should I use when I want to normalize a tensor to a range of 0 to 1? But I work with images with 2 channels (a, b channel -&gt; -128 to 127) only instead of 3 channels. Thus, the usual `mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]` will not do the job. \n\n    transform = transforms.Compose([\n       transforms.ToTensor(),\n       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n    ])\n\nThis leads to this error message:\n\n&gt; tensor.sub_(mean[:, None, None]).div_(std[:, None, None])\n&gt;\n&gt; RuntimeError: The size of tensor a (2) must match the size of tensor b\n&gt; (3) at non-singleton dimension 0",
        "accepted_answer_markdown": "As you can see, PyTorch complains about the Tensor size, since you lack a channel.\r\n\r\nAdditionally, the &quot;usual&quot; mean and std values are computed on ImageNet dataset, and are useful if the statistics of your data match the ones of that dataset.\r\n\r\nAs you work with two channels only, I assume that your domain might be fairly different from 3-channels natural images. In that case I would simply use `0.5` for both `mean` and `std`, such that the minimum value 0 will be converted to `(0 - 0.5) / 0.5 = -1` and the maximum value of 1 to `(1 - 0.5) / 0.5 = 1`.\r\n\r\n```python\r\ntransform = transforms.Compose([\r\n   transforms.ToTensor(),\r\n   transforms.Normalize(mean=[0.5, 0.5],\r\n                        std=[0.5, 0.5])\r\n])\r\n```\r\n\r\n---\r\n\r\nEdit: I would recommend zero-centering of the input. \r\n\r\nHowever, if for some reason you must have it in range [0, 1], calling only `ToTensor()` would suffice.\r\n\r\nIn this case, a word of caution. I think `ToTensor()` [assumes your input to lie in range [0, 255]][1] prior to the transform, so it basically divides it by 255. If that is not the case in your domain (e.g. your input is always in range [1, 50] for some reason) I would simply create a [custom transform][2] to divide for the actual upper bound for your data.\r\n\r\n\r\n  [1]: https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.ToTensor\r\n  [2]: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#transforms"
    },
    {
        "question_id": "59125208",
        "accepted_answer_id": "59125406",
        "question_title": "How to properly implement 1D CNN for numerical data in PyTorch?",
        "question_markdown": "I have a 500x2000 matrix, where each row represents an individual and each column is a measurement of some particular quality about that individual. I&#39;m using a batch size of 64, so the input for each cycle of the network is actually a 64x2000 matrix. I&#39;m trying to build a CNN in PyTorch to classify individuals given a set of these measurements. However, I&#39;ve stumbled on the parameters for the convolutional layer.\r\n\r\nBelow is my current definition for a simple convolutional neural network.\r\n```\r\nclass CNNnet(nn.Module)\r\n    def __init__(self):\r\n        self.conv1 = nn.Conv1d(2000, 200, (1,2), stride=10)\r\n        self.pool = nn.MaxPool1d(kernel_size = (1, 2), stride = 2)\r\n\r\n        self.fc1 = nn.Linear(64, 30)\r\n        self.fc2 = nn.Linear(30, 7)\r\n\r\n    def forward(self, x):\r\n        x = x.view(64, 2000, 1)\r\n        x = F.relu(self.conv1(x))\r\n        x = self.pool(x)\r\n        x = F.relu(self.fc1(x))\r\n        x = self.fc2(x)\r\n        return x\r\n```\r\n\r\nAttempting to train this model produces the following error:\r\n\r\n&gt; &quot;RuntimeError: Expected 4-dimensional input for 4-dimensional weight\r\n&gt; 200 2000 1 2, but got 3-dimensional input of size [64, 2000, 1]\r\n&gt; instead&quot;.\r\n\r\n \r\n\r\nI&#39;m confused on why it&#39;s expecting a 4D *200x2000x1x2* matrix (shouldn&#39;t the number of output channels be irrelevant to the input? And why is there a 2 at the end?).\r\n\r\nMy question is what would be the proper syntax or approach for writing a CNN (specifically the convolutional layer) when dealing with 1D data. Any help is greatly appreciated.",
        "accepted_answer_markdown": "So the kernel size in the 1 dimensional case is simply a vector. So if you\u2019ll want a kernel of size \u20181X2\u2019 you need to specify the \u20182\u2019 \r\nIn the 2 dimensional case 2 will mean a \u20182X2\u2019 kernel size.\r\n\r\nYou gave a tuple of 2 values so you use 2 kernel types each will create its own channel"
    }
]